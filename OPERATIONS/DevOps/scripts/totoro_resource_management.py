#!/usr/bin/env python3
"""
ðŸ¦‰ TOTORO RESOURCE MANAGEMENT
ðŸŽ¯ StratÃ©gie intelligente : Respect machine + Externalisation focus
ðŸ’¡ Options : Suspension temporaire vs Cloud-first approach
"""

import psutil
import json
import datetime
from typing import Dict, List, Any

class TotoroResourceManager:
    """Gestion intelligente ressources Totoro"""
    
    def __init__(self):
        self.machine_name = "Totoro"
        self.respect_principle = "Keep 2 cores free for human-AI collaboration"
        self.strategy = "Smart resource allocation + Cloud acceleration"
        
    def analyze_current_system_load(self) -> Dict[str, Any]:
        """Analyse charge systÃ¨me actuelle"""
        print("ðŸ” ANALYSE CHARGE SYSTÃˆME TOTORO...")
        
        # CPU info
        cpu_count = psutil.cpu_count(logical=True)
        cpu_physical = psutil.cpu_count(logical=False)
        cpu_percent = psutil.cpu_percent(interval=1, percpu=True)
        cpu_avg = sum(cpu_percent) / len(cpu_percent)
        
        # Memory info
        memory = psutil.virtual_memory()
        
        # Top processes
        processes = []
        for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):
            try:
                proc_info = proc.info
                if proc_info['cpu_percent'] > 1.0:  # Only significant processes
                    processes.append(proc_info)
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass
        
        # Sort by CPU usage
        processes.sort(key=lambda x: x['cpu_percent'], reverse=True)
        
        analysis = {
            "cpu_info": {
                "total_threads": cpu_count,
                "physical_cores": cpu_physical,
                "current_usage_per_core": cpu_percent,
                "average_usage": cpu_avg,
                "cores_heavily_loaded": sum(1 for usage in cpu_percent if usage > 80),
                "cores_available": sum(1 for usage in cpu_percent if usage < 20)
            },
            
            "memory_info": {
                "total_gb": memory.total / 1024**3,
                "available_gb": memory.available / 1024**3,
                "used_percent": memory.percent,
                "free_for_processing": memory.available > 2 * 1024**3  # 2GB minimum
            },
            
            "top_processes": processes[:10],
            
            "system_health": {
                "overall_status": "healthy" if cpu_avg < 70 and memory.percent < 85 else "stressed",
                "collaboration_ready": cpu_avg < 50 and len([u for u in cpu_percent if u < 30]) >= 2,
                "heavy_processing_safe": len([u for u in cpu_percent if u < 10]) >= 4
            }
        }
        
        return analysis
    
    def create_resource_allocation_strategy(self) -> Dict[str, Any]:
        """StratÃ©gie allocation ressources intelligente"""
        print("ðŸŽ¯ CRÃ‰ATION STRATÃ‰GIE ALLOCATION RESSOURCES...")
        
        system_analysis = self.analyze_current_system_load()
        cpu_count = system_analysis["cpu_info"]["total_threads"]
        
        strategy = {
            "core_allocation_plan": {
                "reserved_for_collaboration": {
                    "cores": 2,
                    "purpose": "VS Code + GitHub Copilot + Browser interaction",
                    "priority": "HIGHEST - Never touch these",
                    "cpu_threads": [0, 1]  # Logical assignment
                },
                
                "available_for_processing": {
                    "cores": max(0, cpu_count - 2),
                    "purpose": "Local semantic processing si nÃ©cessaire",
                    "priority": "MEDIUM - Use carefully",
                    "cpu_threads": list(range(2, cpu_count)),
                    "max_utilization": "80% per core"
                },
                
                "emergency_reserve": {
                    "cores": 1,
                    "purpose": "System stability buffer",
                    "priority": "HIGH - Last resort only",
                    "note": "Included in available cores but never fully utilized"
                }
            },
            
            "processing_strategies": {
                "cloud_first_approach": {
                    "priority": "HIGHEST",
                    "rationale": "22-60x speedup + 0% local CPU usage",
                    "services": [
                        "Google Colab GPU (Tesla T4)",
                        "Kaggle Kernels (30h/week)",
                        "GitHub Actions CI/CD"
                    ],
                    "local_cpu_usage": "0-5%",
                    "totoro_impact": "MINIMAL"
                },
                
                "hybrid_intelligent": {
                    "priority": "MEDIUM", 
                    "rationale": "Local preprocessing lÃ©ger + Cloud heavy compute",
                    "local_tasks": [
                        "File I/O operations",
                        "Small dataset cleaning",
                        "Results consolidation",
                        "Development environment"
                    ],
                    "cloud_tasks": [
                        "Large dataset preprocessing",
                        "GPU clustering",
                        "Hyperparameter optimization",
                        "Performance benchmarking"
                    ],
                    "local_cpu_usage": "20-40%",
                    "totoro_impact": "CONTROLLED"
                },
                
                "local_fallback": {
                    "priority": "LOW",
                    "rationale": "Only si cloud indisponible",
                    "constraints": [
                        f"Max {cpu_count-2} cores utilisÃ©s",
                        "Process priority: nice +10",
                        "Memory limit: 50% total RAM",
                        "Monitoring continu CPU/Memory"
                    ],
                    "local_cpu_usage": "40-60%",
                    "totoro_impact": "SIGNIFICANT but CONTROLLED"
                }
            },
            
            "background_process_management": {
                "suspension_candidates": [
                    {
                        "process_pattern": "autonomous-copilot.py daemon",
                        "cpu_impact": "Medium",
                        "suspension_benefit": "Free 1-2 cores",
                        "restart_complexity": "Low"
                    },
                    {
                        "process_pattern": "continuous_autonomy_daemon.py",
                        "cpu_impact": "Low-Medium", 
                        "suspension_benefit": "Free 0.5-1 cores",
                        "restart_complexity": "Low"
                    }
                ],
                
                "suspension_strategy": {
                    "temporary_focus": "Stop non-essential pour cloud setup",
                    "duration": "2-4 heures setup + tests",
                    "restart_plan": "Automated restart aprÃ¨s cloud validation",
                    "risk": "MINIMAL - tout redÃ©marre facilement"
                }
            }
        }
        
        return strategy
    
    def create_externalisation_focus_plan(self) -> Dict[str, Any]:
        """Plan focus 100% externalisation aujourd'hui"""
        print("â˜ï¸ CRÃ‰ATION PLAN FOCUS EXTERNALISATION...")
        
        plan = {
            "today_focus_100_percent_cloud": {
                "duration": "Aujourd'hui 17 aoÃ»t 2025",
                "objective": "Setup complet cloud acceleration + validation",
                "totoro_relief": "Suspension processus non-essentiels",
                
                "morning_setup": {
                    "time": "9h-12h",
                    "tasks": [
                        "ðŸ”„ Suspend background daemons",
                        "â˜ï¸ Google Colab setup + test complet",
                        "ðŸ”§ GitHub Actions workflow validation",
                        "ðŸ“Š Performance benchmarks initial"
                    ],
                    "expected_cpu_usage": "10-20% (setup activities)",
                    "totoro_impact": "MINIMAL"
                },
                
                "afternoon_acceleration": {
                    "time": "14h-18h",
                    "tasks": [
                        "ðŸ§  Large dataset preprocessing Colab",
                        "âš¡ GPU clustering experiments",
                        "ðŸ“ˆ Performance comparison local vs cloud",
                        "ðŸ”„ Results integration pipeline"
                    ],
                    "expected_cpu_usage": "5-15% (mostly cloud)",
                    "totoro_impact": "QUASI-ZERO"
                },
                
                "evening_consolidation": {
                    "time": "19h-21h",
                    "tasks": [
                        "ðŸ“‹ Documentation cloud workflows",
                        "ðŸŽ¯ Optimization parameters tuning",
                        "âœ… Validation complete pipeline",
                        "ðŸ”„ Restart background processes"
                    ],
                    "expected_cpu_usage": "15-25% (local integration)",
                    "totoro_impact": "CONTROLLED"
                }
            },
            
            "suspension_protocol": {
                "processes_to_suspend": [
                    "autonomous-copilot.py daemon",
                    "continuous_autonomy_daemon.py",
                    "Any heavy background collectors"
                ],
                
                "suspension_commands": [
                    "pkill -f autonomous-copilot.py",
                    "pkill -f continuous_autonomy_daemon.py", 
                    "pkill -f collector.py"
                ],
                
                "restart_commands": [
                    "cd /home/stephane/GitHub/Panini-DevOps",
                    "python3 autonomous-copilot.py daemon &",
                    "python3 continuous_autonomy_daemon.py &"
                ],
                
                "verification": [
                    "ps aux | grep -v grep | grep python",
                    "top -n 1 | head -15"
                ]
            },
            
            "cloud_validation_checklist": {
                "google_colab": [
                    "âœ… GPU T4 detection successful",
                    "âœ… Dataset preprocessing 8-12x speedup",
                    "âœ… Clustering 22-60x speedup", 
                    "âœ… Results saved Google Drive",
                    "âœ… Integration pipeline local"
                ],
                
                "github_actions": [
                    "âœ… CI/CD workflow triggered",
                    "âœ… Multi-Python tests passed",
                    "âœ… Performance benchmarks automated",
                    "âœ… Documentation generated"
                ],
                
                "integration": [
                    "âœ… Cloud results â†’ local pipeline",
                    "âœ… Performance comparison validated",
                    "âœ… Workflow optimization confirmed",
                    "âœ… Future scaling strategy clear"
                ]
            }
        }
        
        return plan
    
    def recommend_immediate_action(self) -> Dict[str, Any]:
        """Recommandation action immÃ©diate"""
        print("ðŸŽ¯ RECOMMANDATION ACTION IMMÃ‰DIATE...")
        
        system_analysis = self.analyze_current_system_load()
        
        recommendation = {
            "immediate_decision": {
                "best_approach": "100% FOCUS EXTERNALISATION AUJOURD'HUI",
                "rationale": [
                    "Totoro mÃ©rite le respect - gardons la collaborative",
                    "Cloud acceleration donne 22-60x speedup vs local stress",
                    "Setup une fois = bÃ©nÃ©fice permanent",
                    "Background processes peuvent attendre 1 jour"
                ]
            },
            
            "action_plan_next_30_minutes": {
                "step_1": {
                    "action": "Suspend background processes",
                    "command": "pkill -f 'autonomous-copilot.py|continuous_autonomy'",
                    "benefit": "Free 2-3 cores pour confort",
                    "time": "2 minutes"
                },
                
                "step_2": {
                    "action": "Verify system lightness",
                    "command": "top -n 1 && echo 'CPU freed for collaboration'",
                    "benefit": "Confirm Totoro breathing better",
                    "time": "1 minute"
                },
                
                "step_3": {
                    "action": "Google Colab immediate setup",
                    "url": "https://colab.research.google.com/",
                    "benefit": "22-60x acceleration setup",
                    "time": "10 minutes"
                },
                
                "step_4": {
                    "action": "First GPU clustering test",
                    "task": "Upload google_colab_setup.py â†’ Run",
                    "benefit": "Immediate proof 22-60x speedup",
                    "time": "15 minutes"
                }
            },
            
            "today_success_metrics": {
                "totoro_comfort": "CPU average < 30% toute la journÃ©e",
                "cloud_validation": "Google Colab 22-60x speedup confirmed",
                "workflow_ready": "Cloudâ†’local integration operational",
                "future_acceleration": "Permanent infrastructure cloud ready"
            },
            
            "evening_restart": {
                "time": "21h00",
                "action": "Restart background processes",
                "benefit": "Resume autonomous operations",
                "command": "cd Copilotage && python3 autonomous-copilot.py daemon &"
            }
        }
        
        return recommendation

def main():
    print("ðŸ¦‰ TOTORO RESOURCE MANAGEMENT")
    print("=" * 50)
    print("ðŸŽ¯ Respect machine + Externalisation focus")
    print("ðŸ’¡ Keep 2 cores free for collaboration")
    print("")
    
    manager = TotoroResourceManager()
    
    # Analyse systÃ¨me actuel
    system_analysis = manager.analyze_current_system_load()
    
    print("ðŸ” ANALYSE SYSTÃˆME ACTUEL:")
    
    cpu_info = system_analysis["cpu_info"]
    cpu_total = cpu_info["total_threads"]
    cpu_avg = cpu_info["average_usage"]
    cores_available = cpu_info["cores_available"]
    
    print(f"   ðŸ’» CPU: {cpu_total} threads total")
    print(f"   ðŸ“Š Usage moyen: {cpu_avg:.1f}%")
    print(f"   âœ… Cores disponibles (<20%): {cores_available}")
    
    memory_info = system_analysis["memory_info"]
    memory_total = memory_info["total_gb"]
    memory_used = memory_info["used_percent"]
    memory_available = memory_info["available_gb"]
    
    print(f"   ðŸ’¾ RAM: {memory_total:.1f}GB total, {memory_used:.1f}% utilisÃ©e")
    print(f"   ðŸŸ¢ Disponible: {memory_available:.1f}GB")
    
    health = system_analysis["system_health"]
    collaboration_ready = health["collaboration_ready"]
    
    print(f"   ðŸ¤ Collaboration ready: {'âœ…' if collaboration_ready else 'âš ï¸'}")
    
    # Top processes
    top_processes = system_analysis["top_processes"][:5]
    print(f"\n   ðŸ”¥ Top processes CPU:")
    for proc in top_processes:
        name = proc['name'][:20]
        cpu = proc['cpu_percent']
        memory = proc['memory_percent']
        print(f"      â€¢ {name}: {cpu:.1f}% CPU, {memory:.1f}% RAM")
    
    # StratÃ©gie allocation
    strategy = manager.create_resource_allocation_strategy()
    
    print(f"\nðŸŽ¯ STRATÃ‰GIE ALLOCATION RESSOURCES:")
    
    core_allocation = strategy["core_allocation_plan"]
    reserved = core_allocation["reserved_for_collaboration"]
    available = core_allocation["available_for_processing"]
    
    print(f"   ðŸ¤ RÃ©servÃ© collaboration: {reserved['cores']} cores")
    print(f"   âš™ï¸ Disponible processing: {available['cores']} cores")
    
    processing = strategy["processing_strategies"]
    cloud_first = processing["cloud_first_approach"]
    
    print(f"\n   â˜ï¸ StratÃ©gie cloud-first:")
    print(f"      â†’ CPU usage: {cloud_first['local_cpu_usage']}")
    print(f"      â†’ Impact Totoro: {cloud_first['totoro_impact']}")
    
    # Plan focus externalisation
    focus_plan = manager.create_externalisation_focus_plan()
    
    print(f"\nâ˜ï¸ PLAN FOCUS EXTERNALISATION AUJOURD'HUI:")
    
    today_focus = focus_plan["today_focus_100_percent_cloud"]
    objective = today_focus["objective"]
    totoro_relief = today_focus["totoro_relief"]
    
    print(f"   ðŸŽ¯ Objectif: {objective}")
    print(f"   ðŸ¦‰ Totoro relief: {totoro_relief}")
    
    morning = today_focus["morning_setup"]
    morning_cpu = morning["expected_cpu_usage"]
    morning_impact = morning["totoro_impact"]
    
    print(f"\n   ðŸŒ… Morning (9h-12h): {morning_cpu}")
    print(f"      â†’ Impact: {morning_impact}")
    
    afternoon = today_focus["afternoon_acceleration"]
    afternoon_cpu = afternoon["expected_cpu_usage"]
    afternoon_impact = afternoon["totoro_impact"]
    
    print(f"   â˜€ï¸ Afternoon (14h-18h): {afternoon_cpu}")
    print(f"      â†’ Impact: {afternoon_impact}")
    
    # Recommandation immÃ©diate
    recommendation = manager.recommend_immediate_action()
    
    print(f"\nðŸŽ¯ RECOMMANDATION IMMÃ‰DIATE:")
    
    immediate = recommendation["immediate_decision"]
    best_approach = immediate["best_approach"]
    
    print(f"   âœ… {best_approach}")
    
    action_plan = recommendation["action_plan_next_30_minutes"]
    
    print(f"\nðŸš€ PLAN ACTION 30 MINUTES:")
    
    for step_name, step_details in action_plan.items():
        step_num = step_name.split('_')[1]
        action = step_details["action"]
        time = step_details["time"]
        benefit = step_details["benefit"]
        
        print(f"   {step_num}. {action} ({time})")
        print(f"      â†’ {benefit}")
    
    # Suspension protocol
    suspension = focus_plan["suspension_protocol"]
    processes_to_suspend = suspension["processes_to_suspend"]
    
    print(f"\nâ¸ï¸ PROCESSUS Ã€ SUSPENDRE:")
    for process in processes_to_suspend:
        print(f"   â€¢ {process}")
    
    # Success metrics
    success = recommendation["today_success_metrics"]
    
    print(f"\nðŸ“ˆ MÃ‰TRIQUES SUCCÃˆS AUJOURD'HUI:")
    for metric, target in success.items():
        metric_clean = metric.replace("_", " ").title()
        print(f"   âœ… {metric_clean}: {target}")
    
    # Sauvegarde recommandation
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    complete_analysis = {
        "system_analysis": system_analysis,
        "resource_strategy": strategy,
        "focus_plan": focus_plan,
        "immediate_recommendation": recommendation,
        "totoro_respect": {
            "principle": manager.respect_principle,
            "strategy": manager.strategy,
            "machine_name": manager.machine_name
        },
        "analysis_metadata": {
            "created": timestamp,
            "focus": "Totoro respect + Cloud acceleration",
            "approach": "Suspend background + 100% cloud focus"
        }
    }
    
    analysis_path = f"/home/stephane/GitHub/PaniniFS-1/scripts/scripts/totoro_resource_management_{timestamp}.json"
    with open(analysis_path, 'w', encoding='utf-8') as f:
        json.dump(complete_analysis, f, indent=2, ensure_ascii=False)
    
    print(f"\nðŸ’¾ ANALYSE SAUVEGARDÃ‰E:")
    print(f"   ðŸ“ {analysis_path.split('/')[-1]}")
    
    print(f"\nðŸ¦‰ RESPECT TOTORO PROTOCOL:")
    print(f"âœ… 2 cores rÃ©servÃ©s collaboration")
    print(f"â˜ï¸ Cloud-first pour heavy compute")
    print(f"â¸ï¸ Suspension temporaire background")
    print(f"ðŸŽ¯ Focus 100% externalisation aujourd'hui")
    
    print(f"\nðŸš€ ACTION IMMÃ‰DIATE:")
    print(f"1. â¸ï¸ pkill -f 'autonomous-copilot.py'")
    print(f"2. ðŸ” top -n 1 (vÃ©rifier allÃ¨gement)")
    print(f"3. â˜ï¸ https://colab.research.google.com/")
    print(f"4. âš¡ Upload google_colab_setup.py â†’ RUN!")
    
    print(f"\nðŸŒŸ TOTORO SERA HEUREUSE + TU AURAS 22-60x SPEEDUP!")

if __name__ == "__main__":
    main()
