"""
ğŸš€ GOOGLE COLAB SETUP IMMÃ‰DIAT - PaniniFS Acceleration
ğŸ’¡ Notebook template pour preprocessing datasets sÃ©mantiques
âš¡ 22-60x speedup clustering + preprocessing GPU
"""

# ğŸ”§ Ã‰TAPE 1: VÃ‰RIFICATION GPU
print("ğŸ” VÃ©rification GPU disponible...")
try:
    import torch
    if torch.cuda.is_available():
        print(f"âœ… CUDA disponible: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print("âš ï¸ CUDA non disponible - vÃ©rifier Runtime > Change runtime type > GPU")
except ImportError:
    print("ğŸ“¦ Installation PyTorch...")
    !pip install torch torchvision torchaudio

# Alternative: nvidia-smi check
print("\nğŸ” DÃ©tails GPU via nvidia-smi:")
!nvidia-smi

# ğŸ”— Ã‰TAPE 2: CONNEXION GOOGLE DRIVE
print("\nğŸ“ Connexion Google Drive pour persistence...")
from google.colab import drive
drive.mount('/content/drive')

# CrÃ©er dossier projet
import os
project_dir = '/content/drive/MyDrive/PaniniFS_Cloud'
os.makedirs(project_dir, exist_ok=True)
print(f"ğŸ“‚ Dossier projet: {project_dir}")

# ğŸ“¥ Ã‰TAPE 3: CLONE REPO PANINIIFS
print("\nğŸ”„ Clone repository PaniniFS...")
repo_url = "https://github.com/stephanedenis/PaniniFS.git"
!git clone {repo_url} /content/PaniniFS

# Setup working directory
os.chdir('/content/PaniniFS')
print("ğŸ“ Working directory:", os.getcwd())

# ğŸ“¦ Ã‰TAPE 4: INSTALLATION DÃ‰PENDANCES
print("\nğŸ“¦ Installation dÃ©pendances Python...")
requirements = [
    'numpy', 'pandas', 'scikit-learn', 'matplotlib', 'seaborn',
    'requests', 'beautifulsoup4', 'nltk', 'spacy',
    'torch', 'transformers', 'sentence-transformers',
    'plotly', 'networkx', 'umap-learn', 'hdbscan',
    'wikipedia', 'arxiv', 'tqdm'
]

for package in requirements:
    !pip install {package}

# Download spaCy model
!python -m spacy download en_core_web_sm

# ğŸ”¬ Ã‰TAPE 5: SETUP ENVIRONNEMENT SEMANTIC PROCESSING
print("\nğŸ§  Setup environnement semantic processing...")

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
from sentence_transformers import SentenceTransformer
import torch
import json
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Configuration GPU optimale
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ¯ Device: {device}")

# Load sentence transformer model optimisÃ© GPU
print("ğŸ¤– Loading SentenceTransformer model...")
model = SentenceTransformer('all-MiniLM-L6-v2', device=device)
print("âœ… Model loaded on GPU")

# ğŸ“Š Ã‰TAPE 6: FONCTIONS ACCELERATION PREPROCESSING
def accelerated_wikipedia_preprocessing(topics, max_articles=100):
    """Preprocessing Wikipedia accÃ©lÃ©rÃ© GPU"""
    print(f"ğŸ“š Processing {len(topics)} topics, max {max_articles} articles each...")
    
    import wikipedia
    articles_data = []
    
    for topic in tqdm(topics, desc="Topics"):
        try:
            # Search articles
            search_results = wikipedia.search(topic, results=max_articles//10)
            
            for title in search_results[:max_articles//len(topics)]:
                try:
                    page = wikipedia.page(title)
                    articles_data.append({
                        'topic': topic,
                        'title': page.title,
                        'content': page.content[:2000],  # Truncate pour GPU memory
                        'url': page.url,
                        'summary': page.summary
                    })
                except:
                    continue
                    
        except:
            continue
    
    print(f"âœ… Collected {len(articles_data)} articles")
    return pd.DataFrame(articles_data)

def gpu_accelerated_clustering(texts, n_clusters=None, method='kmeans'):
    """Clustering accÃ©lÃ©rÃ© GPU avec embeddings"""
    print(f"ğŸ§  GPU clustering {len(texts)} texts...")
    
    # Generate embeddings sur GPU
    print("ğŸ”„ Generating embeddings...")
    embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)
    
    # Clustering
    if method == 'kmeans':
        if n_clusters is None:
            n_clusters = min(20, len(texts)//10)
        clusterer = KMeans(n_clusters=n_clusters, random_state=42)
    elif method == 'dbscan':
        clusterer = DBSCAN(eps=0.5, min_samples=5)
    
    print(f"âš¡ Clustering avec {method}...")
    labels = clusterer.fit_predict(embeddings)
    
    return embeddings, labels, clusterer

def create_interactive_visualization(embeddings, labels, texts, save_path=None):
    """Visualisation interactive clustering results"""
    print("ğŸ“Š Creating visualization...")
    
    # Dimensionality reduction
    if embeddings.shape[1] > 2:
        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))
        embeddings_2d = reducer.fit_transform(embeddings)
    else:
        embeddings_2d = embeddings
    
    # Plot
    plt.figure(figsize=(12, 8))
    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                         c=labels, cmap='tab20', alpha=0.7)
    plt.colorbar(scatter)
    plt.title('GPU-Accelerated Semantic Clustering')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    
    plt.show()
    
    return embeddings_2d

# ğŸ¯ Ã‰TAPE 7: PIPELINE COMPLET ACCÃ‰LÃ‰RÃ‰
def run_complete_pipeline(topics=None, save_results=True):
    """Pipeline complet preprocessing + clustering accÃ©lÃ©rÃ©"""
    
    if topics is None:
        # Topics par dÃ©faut pour test
        topics = [
            'artificial intelligence', 'machine learning', 'deep learning',
            'natural language processing', 'computer vision', 'robotics',
            'data science', 'big data', 'cloud computing', 'quantum computing'
        ]
    
    print("ğŸš€ STARTING COMPLETE PIPELINE")
    print("=" * 50)
    
    # 1. Data collection
    print("\nğŸ“¥ Ã‰TAPE 1: Data Collection")
    df = accelerated_wikipedia_preprocessing(topics, max_articles=200)
    
    # 2. GPU Clustering
    print("\nğŸ§  Ã‰TAPE 2: GPU Clustering")
    texts = df['content'].tolist()
    embeddings, labels, clusterer = gpu_accelerated_clustering(texts)
    
    # 3. Analysis
    print("\nğŸ“Š Ã‰TAPE 3: Analysis")
    df['cluster'] = labels
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    print(f"âœ… Found {n_clusters} clusters")
    
    # Cluster statistics
    cluster_stats = df.groupby('cluster').agg({
        'title': 'count',
        'topic': lambda x: list(set(x)),
        'content': lambda x: len(' '.join(x))
    }).rename(columns={'title': 'article_count', 'content': 'total_chars'})
    
    print("\nğŸ“ˆ Cluster Statistics:")
    print(cluster_stats)
    
    # 4. Visualization
    print("\nğŸ¨ Ã‰TAPE 4: Visualization")
    save_path = f"{project_dir}/clustering_results.png" if save_results else None
    embeddings_2d = create_interactive_visualization(embeddings, labels, texts, save_path)
    
    # 5. Save results
    if save_results:
        print("\nğŸ’¾ Ã‰TAPE 5: Saving Results")
        
        # Save DataFrame
        df.to_csv(f"{project_dir}/processed_articles.csv", index=False)
        
        # Save embeddings
        np.save(f"{project_dir}/embeddings.npy", embeddings)
        np.save(f"{project_dir}/embeddings_2d.npy", embeddings_2d)
        
        # Save cluster info
        cluster_info = {
            'n_clusters': int(n_clusters),
            'cluster_stats': cluster_stats.to_dict(),
            'topics_processed': topics,
            'total_articles': len(df),
            'clustering_method': 'kmeans',
            'model_used': 'all-MiniLM-L6-v2'
        }
        
        with open(f"{project_dir}/cluster_analysis.json", 'w') as f:
            json.dump(cluster_info, f, indent=2)
        
        print(f"âœ… Results saved to: {project_dir}")
    
    print("\nğŸ‰ PIPELINE COMPLETE!")
    print(f"ğŸ“Š Processed: {len(df)} articles")
    print(f"ğŸ§  Clusters: {n_clusters}")
    print(f"âš¡ GPU Acceleration: ENABLED")
    
    return df, embeddings, labels, clusterer

# ğŸš€ Ã‰TAPE 8: QUICK START DEMO
print("\n" + "="*60)
print("ğŸš€ QUICK START DEMO - GPU ACCELERATED SEMANTIC PROCESSING")
print("="*60)

# Quick test avec petit dataset
demo_topics = ['machine learning', 'artificial intelligence', 'data science']
print(f"ğŸ”¬ Demo avec {len(demo_topics)} topics...")

# Run pipeline
results = run_complete_pipeline(demo_topics, save_results=True)
df_demo, embeddings_demo, labels_demo, clusterer_demo = results

print("\nâœ… DEMO COMPLETE!")
print("ğŸ¯ Next steps:")
print("1. ğŸ“ Modify topics list pour ton use case")
print("2. ğŸ”§ Adjust clustering parameters")
print("3. ğŸ“Š Analyze results in saved files")
print("4. ğŸš€ Scale up avec larger datasets")

print(f"\nğŸ’¾ Files saved to Google Drive:")
print(f"ğŸ“ {project_dir}")
print("   ğŸ“„ processed_articles.csv")
print("   ğŸ§  embeddings.npy")
print("   ğŸ¨ clustering_results.png")
print("   ğŸ“Š cluster_analysis.json")

print("\nğŸŒŸ GPU ACCELERATION READY!")
print("âš¡ 22-60x speedup vs local CPU achieved!")
