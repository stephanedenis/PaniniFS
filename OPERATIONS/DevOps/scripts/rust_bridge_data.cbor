£eatoms™R¨bidh36f5017cgconceptxintelligence artificiellejdefinitiony3Scor match paris Nantes L'intelligence artificielle (IA) est l'ensemble des programmes ou algorithmes permettant aux machines d'effectuer des tÃ¢ches typiquement associÃ©es Ã  l'intelligence humaine, comme l'apprentissage, le raisonnement, la rÃ©solution de problÃ¨me, la perception ou la prise de dÃ©cisionlsource_agentuautonomous_copilot_v1ksource_typeiwikipediaitimestampx2025-08-15T21:38:32.549669jconfidenceû?ë333333nparent_sourcesx7https://fr.wikipedia.org/wiki/Intelligence_artificielle¨bidh9a0d07a4gconceptpmachine learningjdefinitionx@L'apprentissage automatique (en anglais : machine learning, littlsource_agentuautonomous_copilot_v1ksource_typeiwikipediaitimestampx2025-08-15T21:38:33.856007jconfidenceû?ë333333nparent_sourcesx7https://fr.wikipedia.org/wiki/Apprentissage_automatique¨bidh572f6068gconceptsrÃ©seau de neuronesjdefinitionxñUn rÃ©seau de neurones artificiels, ou rÃ©seau neuronal artificiel, est un systÃ¨me informatique dont la conception est inspirÃ©e Ã  l'origine du fonctionnement des neurones mais qui, par la suite, s'est rapprochÃ© des mÃ©thodes statistiqueslsource_agentuautonomous_copilot_v1ksource_typeiwikipediaitimestampx2025-08-15T21:38:36.491837jconfidenceû?ë333333nparent_sourcesxAhttps://fr.wikipedia.org/wiki/R%C3%A9seau_de_neurones_artificiels¨bidh5e32af2egconceptuapprentissage profondjdefinitionxúL'apprentissage profond ou apprentissage en profondeur (en anglais : deep learning) est un sous-domaine de lâ€™intelligence artificielle qui utilise des rÃ©seaux neuronaux artificiels formant de nombreuses couches pour rÃ©soudre des tÃ¢ches complexeslsource_agentuautonomous_copilot_v1ksource_typeiwikipediaitimestampx2025-08-15T21:38:37.704271jconfidenceû?ë333333nparent_sourcesx3https://fr.wikipedia.org/wiki/Apprentissage_profond¨bidh5d0f7310gconceptltransformersjdefinitionx·Transformers (ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼, ToransufÅmÄ) est une franchise crÃ©Ã©e par les entreprises japonaise Takara Tomy et amÃ©ricaine Hasbro, toutes deux productrices de jouetslsource_agentuautonomous_copilot_v1ksource_typeiwikipediaitimestampx2025-08-15T21:38:38.884966jconfidenceû?ë333333nparent_sourcesx*https://fr.wikipedia.org/wiki/Transformers¨bidh70d5fb14gconceptjalgorithmejdefinitionx„Un algorithme est une suite finie et non ambiguÃ« d'instructions et dâ€™opÃ©rations permettant de rÃ©soudre une classe de problÃ¨meslsource_agentuautonomous_copilot_v1ksource_typeiwikipediaitimestampx2025-08-15T21:38:40.952643jconfidenceû?ë333333nparent_sourcesx(https://fr.wikipedia.org/wiki/Algorithme¨bidhe528b903gconceptfbuenosjdefinitionx³Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeleylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.422687jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.03550v1¨bidh20d0d6ccgconceptdwelljdefinitionx³Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeleylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.425727jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.03550v1¨bidhac2db510gconceptegivenjdefinitionx³Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeleylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.425818jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.03550v1¨bidh570c3fc2gconceptgmachinejdefinitionx³Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeleylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.425853jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1909.03550v1¨bidh47bf5016gconceptiprincetonjdefinitionx³Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeleylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.425883jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.03550v1¨bidh9102595fgconcepticourse atjdefinitionx³Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeleylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.425925jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.03550v1¨bidhff0583d9gconceptglecturejdefinitionx³Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeleylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.426026jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1909.03550v1¨bidhe0004a75gconceptgderivedjdefinitionx³Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeleylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.426137jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.03550v1¨bidha2eb1487gconcepthlearningjdefinitionx³Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeleylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.426268jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1909.03550v1¨bidhef25baacgconceptlbuenos airesjdefinitionx³Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeleylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.426407jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.03550v1¨bidh2d15e853gconceptsadversarial machinejdefinitionxõI describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detectlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.529530jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1811.04422v1¨bidh11918eb5gconceptnincluding testjdefinitionx–This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shapinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.529675jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1811.04422v1¨bidh84c99158gconceptgmachinejdefinitionxõI describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detectlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.529732jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1811.04422v1¨bidhe4408f88gconceptgdo harmjdefinitionxõI describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detectlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.529776jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1811.04422v1¨bidhd38c7c8egconceptomachine learnerjdefinitionxõI describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detectlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.529815jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1811.04422v1¨bidh0e1334f3gconcepththe viewjdefinitionx|The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.529857jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1811.04422v1¨bidha76329a2gconceptditemjdefinitionx–This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shapinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.529895jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1811.04422v1¨bidh21284da8gconceptjencouragesjdefinitionx|The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.529942jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1811.04422v1¨bidh68cfa4cegconcepthtypes ofjdefinitionx–This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shapinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.529987jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1811.04422v1¨bidhb97130c8gconceptfsystemjdefinitionxõI describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detectlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.530028jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1811.04422v1¨bidha5dcfc32gconceptfto thejdefinitionxSThe article is devoted to the problem of small learning samples in machine learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.633273jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.04849v1¨bidh7a9ba002gconceptoand recognitionjdefinitionxSThe article is devoted to the problem of small learning samples in machine learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.633460jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1707.04849v1¨bidh872244f6gconceptklearning isjdefinitionx¥The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flawslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.633524jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.04849v1¨bidh5c8955e3gconceptgdevotedjdefinitionxSThe article is devoted to the problem of small learning samples in machine learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.633568jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.04849v1¨bidhd362dcfcgconceptkand minimaxjdefinitionx¥The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flawslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.633626jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.04849v1¨bidh08f7415dgconceptnsmall learningjdefinitionxSThe article is devoted to the problem of small learning samples in machine learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.633675jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.04849v1¨bidh35cc20aagconceptkrecognitionjdefinitionxSThe article is devoted to the problem of small learning samples in machine learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.633727jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1707.04849v1¨bidha6369815gconceptgmachinejdefinitionxSThe article is devoted to the problem of small learning samples in machine learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.633779jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1707.04849v1¨bidhceb03d78gconceptflookedjdefinitionx¥The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flawslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.633824jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.04849v1¨bidhd0737d10gconceptjis devotedjdefinitionxSThe article is devoted to the problem of small learning samples in machine learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.633866jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.04849v1¨bidh30410f6fgconceptravailable datasetsjdefinitiony¸The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.736800jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.09246v1¨bidh9eef4a92gconceptgmachinejdefinitionxrIn this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.736919jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1909.09246v1¨bidh6ce47fe9gconcepthoverviewjdefinitionxrIn this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.736977jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.09246v1¨bidh0228d0d4gconceptdtheyjdefinitiony¸The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.737030jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.09246v1¨bidh0b49794agconceptgprovidejdefinitionxrIn this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.737076jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.09246v1¨bidhb811269bgconceptdsomejdefinitionx†We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.737123jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.09246v1¨bidh5804ed0egconceptetasksjdefinitionxrIn this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.737170jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.09246v1¨bidhb1eecbe8gconceptfvectorjdefinitiony¸The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.737216jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.09246v1¨bidh6cb689a8gconcepthto applyjdefinitionxNext, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.737255jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.09246v1¨bidh0608e8ccgconceptfhow tojdefinitionxNext, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.737295jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.09246v1¨bidha818d166gconceptlbenefits andjdefinitionxWMachine learning technologies have demonstrated immense capabilities in various domainslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.839946jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2301.09753v1¨bidh1d948ac3gconceptganalyzejdefinitionx We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image basedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.840107jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2301.09753v1¨bidh53defd32gconcepthadoptionjdefinitionxSHowever, adoption of machine learning technologies has a lot of untouched potentiallsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.840176jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2301.09753v1¨bidh9cd5598agconceptgmachinejdefinitionxWMachine learning technologies have demonstrated immense capabilities in various domainslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.840230jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2301.09753v1¨bidhbec78dc8gconceptomodular machinejdefinitionxæIn this work we explore the benefits of modular machine learning solutions and discuss how modular machine learning solutions can overcome some of the major solution engineering limitations of monolithic machine learning solutionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.840293jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2301.09753v1¨bidh53a48f2dgconceptibased andjdefinitionx We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image basedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.840384jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2301.09753v1¨bidhef93d97agconceptkimage basedjdefinitionx We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image basedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.840450jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2301.09753v1¨bidh6af99423gconceptdtheyjdefinitionx8They play a key role in the success of modern businesseslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.840506jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2301.09753v1¨bidhed828c86gconceptetodayjdefinitionx½We recognize that the monolithic nature prevalent in today's machine learning applications stands in the way of efficient and cost effective customized machine learning solution developmentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.840560jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2301.09753v1¨bidh70efa5eegconcepttsolution developmentjdefinitionx½We recognize that the monolithic nature prevalent in today's machine learning applications stands in the way of efficient and cost effective customized machine learning solution developmentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.840614jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2301.09753v1¨bidh429081f3gconceptfformaljdefinitionxæIntroduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.943640jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/0904.3664v1¨bidh8e64aec7gconceptnmaxent dualityjdefinitionxæIntroduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.943756jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/0904.3664v1¨bidh18754c90gconceptidimensionjdefinitionxæIntroduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.943807jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/0904.3664v1¨bidh094ee57agconceptgand pacjdefinitionxæIntroduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.943867jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/0904.3664v1¨bidh7ee52082gconceptgmachinejdefinitionxæIntroduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.943909jconfidenceû?îffffffnparent_sourcesx!https://arxiv.org/abs/0904.3664v1¨bidhaaec0264gconceptjclusteringjdefinitionxæIntroduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.943950jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/0904.3664v1¨bidh4fe6fb2cgconceptlvc dimensionjdefinitionxæIntroduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.943993jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/0904.3664v1¨bidhace5f85dgconceptfmaxentjdefinitionxæIntroduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.944030jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/0904.3664v1¨bidh3ae67ca5gconceptiinferencejdefinitionxæIntroduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.944066jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/0904.3664v1¨bidhff679b88gconcepthlearningjdefinitionxæIntroduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:32.944101jconfidenceû?îffffffnparent_sourcesx!https://arxiv.org/abs/0904.3664v1¨bidh937c762agconceptjinfluencedjdefinitionxeMachine learning techniques have influenced the field of computer architecture like many other fieldslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.048070jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2012.04105v1¨bidha2945ff8gconcepthresearchjdefinitionxsWe also provide a detailed survey of computer architecture research that employs different machine learning methodslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.048136jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2012.04105v1¨bidhc36a700fgconceptothe outstandingjdefinitionx­Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecturelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.048175jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2012.04105v1¨bidh572eee85gconceptllearning andjdefinitionxeMachine learning techniques have influenced the field of computer architecture like many other fieldslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.048217jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2012.04105v1¨bidh4503a937gconcepthfield ofjdefinitionxeMachine learning techniques have influenced the field of computer architecture like many other fieldslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.048268jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2012.04105v1¨bidhc3d64e60gconceptjbe appliedjdefinitionxxThis paper studies how the fundamental machine learning techniques can be applied towards computer architecture problemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.048386jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2012.04105v1¨bidh953a478bgconceptjto exploitjdefinitionx­Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecturelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.048497jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2012.04105v1¨bidhd464b970gconcepthcomputerjdefinitionxeMachine learning techniques have influenced the field of computer architecture like many other fieldslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.048564jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2012.04105v1¨bidh551dbba9gconceptkoutstandingjdefinitionx­Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecturelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.048625jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2012.04105v1¨bidh547f0bc2gconceptjtechniquesjdefinitionxeMachine learning techniques have influenced the field of computer architecture like many other fieldslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.051408jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2012.04105v1¨bidh811e7cc0gconcepthrequiredjdefinitionx¶While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologistlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.154239jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.07492v2¨bidh846c0e19gconceptkfurthermorejdefinitionxÑFurthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorologylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.154454jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.07492v2¨bidh0dca6eb0gconceptgmachinejdefinitionxJRecently, the use of machine learning in meteorology has increased greatlylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.154528jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2204.07492v2¨bidhf46debcfgconcepthprovidedjdefinitionxÑFurthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorologylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.154583jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.07492v2¨bidh2dde2f9bgconceptplearning methodsjdefinitionx¶While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologistlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.154647jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.07492v2¨bidh75ec0eefgconceptjthis paperjdefinitionxÂTo reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methodslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.154697jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.07492v2¨bidh8d972cdfgconcepthlanguagejdefinitionxšA familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.154749jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.07492v2¨bidheb3655fdgconceptiand lowerjdefinitionxÂTo reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methodslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.154800jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.07492v2¨bidh6135cf38gconceptdsomejdefinitionxÂTo reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methodslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.154852jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.07492v2¨bidhf64cde61gconceptldemonstratedjdefinitionxÉThe following machine learning methods are demonstrated: linear regression; logistic regression; decision trees; random forest; gradient boosted decision trees; naive Bayes; and support vector machineslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.154894jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.07492v2¨bidhb7ab5ec0gconceptfcan bejdefinitionxbAt the same time, machine learning experts warn that machine learning models can be biased as welllsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.260700jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.01866v1¨bidh76a052f5gconceptereachjdefinitionxfTo reach such a goal, we develop interactive plots to visualizing the bias learned from synthetic datalsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.260858jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.01866v1¨bidh84607bbegconceptdfairjdefinitionxBias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etclsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.260915jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.01866v1¨bidh83561e8fgconceptgmachinejdefinitionxRecently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.260955jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1909.01866v1¨bidh311eae89gconceptdevenjdefinitionxRecently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.261001jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.01866v1¨bidha6c2b9f3gconceptmimpediment tojdefinitionxBias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etclsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.261040jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.01866v1¨bidh242dc6f3gconceptnfair decisionsjdefinitionxBias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etclsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.261083jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.01866v1¨bidh941740fbgconceptkmethods forjdefinitionxRecently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.261122jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.01866v1¨bidh2f5d9f15gconcepthhope hasjdefinitionxRecently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.261165jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.01866v1¨bidh69c22d60gconceptfat thejdefinitionxRecently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.261202jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.01866v1¨bidh5a2fb7f4gconceptgsystemsjdefinitionxwIf solved, this technology could represent a best-case scenario for the safety and security of AI systems going forwardlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.362094jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.06612v1¨bidh12c87a15gconcepththe goaljdefinitionx The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.362166jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.06612v1¨bidhe679c479gconceptgmachinejdefinitionx§Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code formlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.362187jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1911.06612v1¨bidhc8711489gconceptfverifyjdefinitionx The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.362218jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.06612v1¨bidh1a1fd62egconceptfrefinejdefinitionx The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.362243jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.06612v1¨bidh7d9c5f3egconceptecouldjdefinitionxwIf solved, this technology could represent a best-case scenario for the safety and security of AI systems going forwardlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.362269jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.06612v1¨bidhd9e66584gconceptmintroduced asjdefinitionx§Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code formlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.362291jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.06612v1¨bidh0897cfb4gconceptfsystemjdefinitionx§Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code formlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.362313jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.06612v1¨bidhbf0ee540gconceptktransparentjdefinitionx§Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code formlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.362337jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1911.06612v1¨bidh9e81a91agconcepthto learnjdefinitionx The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.362380jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.06612v1¨bidhe78456bagconceptfcan bejdefinitionxòThe paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisislsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.468803jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1903.08801v1¨bidha07dba80gconceptlrunning withjdefinitionxÔThird, it enables a computer to translate core machine learning implementation from a single thread on a single machine to multiple threads on multiple machines running with blockchain by using a unified approachlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.468931jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1903.08801v1¨bidhc9c6ab13gconceptmcontributionsjdefinitionx$This paper makes three contributionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.468977jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1903.08801v1¨bidh138979b7gconceptdrulejdefinitionxòThe paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisislsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.469024jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1903.08801v1¨bidh543f78f9gconceptdusesjdefinitionxòThe paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisislsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.469073jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1903.08801v1¨bidhb488b85egconceptntranslate corejdefinitionxÔThird, it enables a computer to translate core machine learning implementation from a single thread on a single machine to multiple threads on multiple machines running with blockchain by using a unified approachlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.469121jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1903.08801v1¨bidh80d0a668gconceptgwithoutjdefinitionxuPreviously, machine learning and blockchain have been considered two independent technologies without an obvious linklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.469165jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1903.08801v1¨bidhfeba4b20gconcepththe datajdefinitionx€Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trustedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.469207jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1903.08801v1¨bidha43a059dgconceptipermanentjdefinitionx–This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable waylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.469249jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1903.08801v1¨bidhb92df15fgconceptganalyzejdefinitionxòThe paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisislsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.471643jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1903.08801v1¨bidh845921f7gconceptostructured datajdefinitionx’We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning cloudslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.574376jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1707.09562v3¨bidh7df462e1gconceptfto runjdefinitionyMachine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the restlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.574577jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1707.09562v3¨bidhb0e6d6e0gconceptlwinning codejdefinitionx We then compare the performance of the top winning code available from Kaggle with that of running machine learning clouds from both Azure and Amazon on mlbenchlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.574646jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1707.09562v3¨bidh28baa057gconceptlrarely comesjdefinitionx`Raising the level of abstraction, however, rarely comes free - a performance penalty is possiblelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.574698jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1707.09562v3¨bidh41149f6dgconceptiamazon onjdefinitionx We then compare the performance of the top winning code available from Kaggle with that of running machine learning clouds from both Azure and Amazon on mlbenchlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.574745jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1707.09562v3¨bidh5ab528f7gconceptgmachinejdefinitionx’We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning cloudslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.574793jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1707.09562v3¨bidh0e0f3465gconcepthprovidedjdefinitionx’We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning cloudslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.574839jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1707.09562v3¨bidhe3f92e2cgconcepthstudy ofjdefinitionx’We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning cloudslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.574880jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1707.09562v3¨bidh57777f5dgconceptetasksjdefinitionx’We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning cloudslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.574965jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1707.09562v3¨bidh2aec61dcgconceptkfrom kagglejdefinitionxaWe present mlbench, a novel benchmark constructed by harvesting datasets from Kaggle competitionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.575024jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1707.09562v3¨bidhd6ab5c52gconceptestartjdefinitionxHWe start with a brief review of data marketplaces and pricing desideratalsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.676775jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.07915v1¨bidh67cbafb7gconcepthresearchjdefinitionx{In this article, we survey the principles and the latest research development of data pricing in machine learning pipelineslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.676854jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.07915v1¨bidhd6fc2ecfgconceptjdeploymentjdefinitionxÃWe also investigate pricing in the step of collaborative training of machine learning models, and overview pricing machine learning models for end users in the step of machine learning deploymentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.676908jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.07915v1¨bidh553f70f8gconceptkpenetratingjdefinitionxHData is critical and penetrating in the whole machine learning pipelineslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.676953jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.07915v1¨bidh632c222dgconceptewholejdefinitionxHData is critical and penetrating in the whole machine learning pipelineslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.676996jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.07915v1¨bidh38979a72gconceptgmachinejdefinitionxMachine learning is disruptivelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.677036jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2108.07915v1¨bidh44b2da3bgconcepthoverviewjdefinitionxÃWe also investigate pricing in the step of collaborative training of machine learning models, and overview pricing machine learning models for end users in the step of machine learning deploymentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.677083jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.07915v1¨bidh4b775ee3gconceptfsurveyjdefinitionx{In this article, we survey the principles and the latest research development of data pricing in machine learning pipelineslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.677126jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.07915v1¨bidh70f4914egconceptjas machinejdefinitionxæAs machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many partieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.677170jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.07915v1¨bidhcc1b15abgconceptkdynamic ecojdefinitionxæAs machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many partieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.677216jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.07915v1¨bidhe2681871gconceptgwithoutjdefinitionxÏIt could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experiencelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.782450jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1907.08908v1¨bidhc77d31begconceptgoff-thejdefinitionxÏIt could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experiencelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.782619jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1907.08908v1¨bidh1ad60057gconceptegivenjdefinitionx‚Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.782733jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1907.08908v1¨bidhddeda6abgconceptjand enablejdefinitionxÏIt could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experiencelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.782845jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1907.08908v1¨bidhf2a565ebgconceptkthe currentjdefinitionxİIn this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.782959jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1907.08908v1¨bidh9bdb8e9bgconcepthstate-ofjdefinitionx¹State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approacheslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.783077jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1907.08908v1¨bidh51a5f919gconceptgmachinejdefinitionx‚Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.783186jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1907.08908v1¨bidhfab40a8agconceptipresentedjdefinitionx¹State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approacheslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.783303jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1907.08908v1¨bidhbc046db5gconceptodata scientistsjdefinitionxÏIt could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experiencelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.789819jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1907.08908v1¨bidh4e023d28gconceptjadopted injdefinitionx¹State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approacheslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.790004jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1907.08908v1¨bidh45dc8e99gconceptjhas becomejdefinitionxşWith the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer productslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.898282jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.03120v1¨bidh4d9ab500gconceptnlevel overviewjdefinitionxrWe provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.898447jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.03120v1¨bidhbd89ae96gconceptgsystemsjdefinitionxşWith the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer productslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.898499jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.03120v1¨bidh144f7ac8gconcepthresearchjdefinitionxşWith the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer productslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.898568jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.03120v1¨bidh0464ce6dgconceptjdiscussionjdefinitionxeOur discussion encompasses parallel distributed learning, deep learning as well as federated learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.898615jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.03120v1¨bidhb7f2c348gconceptgmachinejdefinitionxşWith the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer productslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.898651jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2312.03120v1¨bidh60f36d6dgconcepthoverviewjdefinitionxrWe provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.898691jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.03120v1¨bidh73cf116cgconcepthand deepjdefinitionxFIn this study, we present a review of modern machine and deep learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.898726jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.03120v1¨bidhb42571ffgconceptgprovidejdefinitionxrWe provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.898765jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.03120v1¨bidh94ca8f53gconceptddeepjdefinitionxFIn this study, we present a review of modern machine and deep learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:33.898807jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.03120v1¨bidh9d389ec7gconcepthresearchjdefinitionxÃTo solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Sparklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.002011jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2206.07090v2¨bidhcbf21f4cgconceptganalyzejdefinitionxˆHowever, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machinelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.002120jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2206.07090v2¨bidhb2e4e5f9gconceptgmachinejdefinitionxˆHowever, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machinelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.002163jconfidenceû?ìÌÌÌÌÌÌnparent_sourcesx"https://arxiv.org/abs/2206.07090v2¨bidhc42be446gconceptlthe researchjdefinitionx}The research results have shown significant improvement in runtime and efficiency of parallelized machine learning algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.002207jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2206.07090v2¨bidha1c5ca6cgconceptlrespectivelyjdefinitionxÃTo solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Sparklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.002249jconfidenceû?ìÌÌÌÌÌÌnparent_sourcesx"https://arxiv.org/abs/2206.07090v2¨bidh1ec78c6agconceptlparallelizedjdefinitionx´We compare the runtime and efficiency of traditional machine learning algorithms with parallelized machine learning algorithms respectively on the single machine and Spark platformlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.002292jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2206.07090v2¨bidh40815685gconceptjthis paperjdefinitionxÃTo solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Sparklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.002327jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2206.07090v2¨bidh71f2e6degconceptesparkjdefinitionxÃTo solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Sparklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.002390jconfidenceû?ìÌÌÌÌÌÌnparent_sourcesx"https://arxiv.org/abs/2206.07090v2¨bidh622487a0gconceptlresults havejdefinitionx}The research results have shown significant improvement in runtime and efficiency of parallelized machine learning algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.002433jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2206.07090v2¨bidhfd474bd2gconceptdsomejdefinitionxÃTo solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Sparklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.002468jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2206.07090v2¨bidhd8f968a2gconcepthrequiredjdefinitionx§It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challengelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.115166jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1507.02188v1¨bidh77786f3bgconceptmgiven machinejdefinitionx§It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challengelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.115259jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1507.02188v1¨bidhbcb4303egconceptdoverjdefinitionx›This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.115293jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1507.02188v1¨bidhdc039dbegconceptegivenjdefinitionx§It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challengelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.115326jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1507.02188v1¨bidh89c8eaeagconceptdthanjdefinitionx›This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.115371jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1507.02188v1¨bidh82cb45fcgconceptgmachinejdefinitionxIn this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.115406jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1507.02188v1¨bidh7a5491d8gconceptphighly automatedjdefinitionxIn this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.115440jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1507.02188v1¨bidhc925abd5gconcepthprovidedjdefinitionxºThe proposed system helps in identifying data types, choosing a machine learn- ing model, tuning hyper-parameters, avoiding over-fitting and optimization for a provided evaluation metriclsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.115487jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1507.02188v1¨bidhcaf13d96gconceptdmorejdefinitionx›This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.115537jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1507.02188v1¨bidh9707f8f4gconceptitwo yearsjdefinitionx›This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.115571jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1507.02188v1¨bidhc5e21db4gconcepthgreedilyjdefinitionx¥Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.218713jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1212.2686v1¨bidh54502b71gconceptjtrains thejdefinitionx¥Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.218865jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1212.2686v1¨bidh2f5c3ee9gconceptdtimejdefinitionx¥Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.218924jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1212.2686v1¨bidh69fc5dfbgconceptjnew methodjdefinitionxFWe introduce a new method for training deep Boltzmann machines jointlylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.218986jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1212.2686v1¨bidh19497b53gconceptdwelljdefinitionx¥Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.219038jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1212.2686v1¨bidh6a92e88bgconceptiintroducejdefinitionxFWe introduce a new method for training deep Boltzmann machines jointlylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.219084jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1212.2686v1¨bidh4a3c6d24gconceptgrequirejdefinitionx¥Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.219135jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1212.2686v1¨bidhcb10ed70gconceptgmachinejdefinitionxFWe introduce a new method for training deep Boltzmann machines jointlylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.219222jconfidenceû?îffffffnparent_sourcesx!https://arxiv.org/abs/1212.2686v1¨bidhb96b555bgconceptione layerjdefinitionx¥Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.219288jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1212.2686v1¨bidh6e881959gconceptelayerjdefinitionx¥Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.219335jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1212.2686v1¨bidh5ab3e26egconceptfin newjdefinitionx•This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New Yorklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.325767jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1607.02450v2¨bidh0217fa7bgconceptiwhich wasjdefinitionx•This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New Yorklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.325923jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1607.02450v2¨bidhc408864bgconceptlapplicationsjdefinitionx•This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New Yorklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.326050jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1607.02450v2¨bidhfbf19d8fgconcepthworkshopjdefinitionx•This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New Yorklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.326160jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1607.02450v2¨bidh460616fcgconceptgmachinejdefinitionx•This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New Yorklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.326261jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1607.02450v2¨bidha5668207gconceptdjunejdefinitionx•This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New Yorklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.326380jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1607.02450v2¨bidh445770f1gconceptdyorkjdefinitionx•This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New Yorklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.326486jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1607.02450v2¨bidh005e4514gconceptdheldjdefinitionx•This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New Yorklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.326588jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1607.02450v2¨bidh45caf1c0gconceptfof thejdefinitionx•This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New Yorklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.326701jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1607.02450v2¨bidh52f57127gconceptksocial goodjdefinitionx•This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New Yorklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.326811jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1607.02450v2¨bidh4564ec63gconceptgto rnnsjdefinitionyWe take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspectivelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.429727jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2007.01503v1¨bidhfcac6b1fgconcepthfunctionjdefinitionyWe take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspectivelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.429874jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2007.01503v1¨bidha204ca92gconceptmapproximationjdefinitionyWe take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspectivelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.430001jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2007.01503v1¨bidhc3493209gconceptgmachinejdefinitionyWe take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspectivelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.430119jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2007.01503v1¨bidh058f4c06gconceptgat somejdefinitionyWe take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspectivelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.430257jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2007.01503v1¨bidh87509a8cgconcepthnetworksjdefinitionyWe take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspectivelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.430395jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2007.01503v1¨bidh5632b601gconceptfcloserjdefinitionyWe take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspectivelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.430509jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2007.01503v1¨bidh6f68f931gconceptnlimitations ofjdefinitionyWe take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspectivelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.430621jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2007.01503v1¨bidhbe614b59gconceptpgradient descentjdefinitionyWe take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspectivelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.430745jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2007.01503v1¨bidhfda07d94gconceptvtheoretical challengesjdefinitionyWe take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspectivelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:34.430863jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2007.01503v1¨bidh5a250bedgconceptpanswers multiplejdefinitionyThis dissertation proposes that the neural network of deep learning is a physical system, examines deep learning from three different perspectives: microscopic, macroscopic, and physical world views, answers multiple theoretical puzzles in deep learning by using physics principleslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.050710jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1805.08355v1¨bidh104058b5gconcepthresearchjdefinitionxUAt present, most of the theoretical research on deep learning is based on mathematicslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.050784jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1805.08355v1¨bidh688bc2a5gconcepthexplainsjdefinitiony×For example, from the perspective of quantum mechanics and statistical physics, this dissertation presents the calculation methods for convolution calculation, pooling, normalization, and Restricted Boltzmann Machine, as well as the selection of cost functions, explains why deep learning must be deep, what characteristics are learned in deep learning, why Convolutional Neural Networks do not have to be trained layer by layer, and the limitations of deep learning, etclsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.050824jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1805.08355v1¨bidh03d837a4gconceptminsights intojdefinitionyKThe great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanismlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.050863jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1805.08355v1¨bidh23d857a2gconceptkapplicationjdefinitionyKThe great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanismlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.050901jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1805.08355v1¨bidhe39d4a52gconceptgmachinejdefinitiony×For example, from the perspective of quantum mechanics and statistical physics, this dissertation presents the calculation methods for convolution calculation, pooling, normalization, and Restricted Boltzmann Machine, as well as the selection of cost functions, explains why deep learning must be deep, what characteristics are learned in deep learning, why Convolutional Neural Networks do not have to be trained layer by layer, and the limitations of deep learning, etclsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.050944jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1805.08355v1¨bidhc545c5b0gconceptiblack boxjdefinitionyKThe great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanismlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.050989jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1805.08355v1¨bidh78c5bdd1gconceptjmeaningfuljdefinitionyKThe great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanismlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.051030jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1805.08355v1¨bidh776ed066gconceptddeepjdefinitionyKThe great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanismlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.051071jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1805.08355v1¨bidh7522f8fegconceptimechanismjdefinitionyKThe great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanismlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.051114jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1805.08355v1¨bidhad5fc5dbgconceptkconvergencejdefinitionx¹Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimatelylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.153107jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1908.02130v1¨bidh5e99e55egconcepthresearchjdefinitionxGThe past, present and future of deep learning is presented in this worklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.153222jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1908.02130v1¨bidh1229c143gconceptegivenjdefinitionx¹Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimatelylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.153301jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1908.02130v1¨bidh2b4a2e5bgconceptjand futurejdefinitionxGThe past, present and future of deep learning is presented in this worklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.153373jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1908.02130v1¨bidh76f8d631gconceptgtowardsjdefinitionxGThe past, present and future of deep learning is presented in this worklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.153429jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1908.02130v1¨bidh9473ed57gconceptipresentedjdefinitionxGThe past, present and future of deep learning is presented in this worklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.153476jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1908.02130v1¨bidh2c3347aegconceptqcortical learningjdefinitionx¹Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimatelylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.153522jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1908.02130v1¨bidh87c92936gconceptddeepjdefinitionxGThe past, present and future of deep learning is presented in this worklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.153564jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1908.02130v1¨bidh08d26242gconceptffuturejdefinitionxGThe past, present and future of deep learning is presented in this worklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.153606jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1908.02130v1¨bidh5830cfbfgconcepththe pastjdefinitionxGThe past, present and future of deep learning is presented in this worklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.153650jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1908.02130v1¨bidhf7cf0021gconceptpinterpretabilityjdefinitionxªCODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training datalsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.258259jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1806.01756v1¨bidh0387de65gconceptgsystemsjdefinitionxÃWe discuss the major aspects of CODL including concept graph, concept representations, concept exemplars, and concept representation learning systems supporting incremental and continual learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.258405jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1806.01756v1¨bidhf5f0da0agconcepticontinualjdefinitionxÃWe discuss the major aspects of CODL including concept graph, concept representations, concept exemplars, and concept representation learning systems supporting incremental and continual learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.258458jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1806.01756v1¨bidh63b602c8gconceptkand conceptjdefinitionx›We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.258493jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1806.01756v1¨bidhf616d23agconceptoincremental andjdefinitionxÃWe discuss the major aspects of CODL including concept graph, concept representations, concept exemplars, and concept representation learning systems supporting incremental and continual learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.258528jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1806.01756v1¨bidh8ced6f8egconceptglabeledjdefinitionxªCODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training datalsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.258565jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1806.01756v1¨bidhe3822d2dgconceptncodl includingjdefinitionxÃWe discuss the major aspects of CODL including concept graph, concept representations, concept exemplars, and concept representation learning systems supporting incremental and continual learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.258601jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1806.01756v1¨bidhedbbaf10gconceptjwe proposejdefinitionx›We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.258636jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1806.01756v1¨bidh47b9bf81gconceptmoriented deepjdefinitionx›We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.258670jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1806.01756v1¨bidhbc95c155gconceptithe majorjdefinitionxªCODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training datalsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.258703jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1806.01756v1¨bidhc543ebb6gconcepththey usejdefinitionx¬Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do their deep learning models look likelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.363957jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1812.05448v4¨bidh7e01e3d6gconcepthresearchjdefinitionxÁTo bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wildlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.364024jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1812.05448v4¨bidh6e9c1a11gconceptnthe protectionjdefinitionx´On the other hand, our findings urge optimizations on deep learning models deployed on smartphones, the protection of these models, and validation of research ideas on these modelslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.364062jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1812.05448v4¨bidh5381564agconceptmprosperity ofjdefinitionxÙOn one hand, our findings paint a promising picture of deep learning for smartphones, showing the prosperity of mobile deep learning frameworks as well as the prosperity of apps building their cores atop deep learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.364099jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1812.05448v4¨bidh80cc2b3dgconceptgat deepjdefinitionx=We are in the dawn of deep learning explosion for smartphoneslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.364136jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1812.05448v4¨bidhb0c83df7gconceptdhandjdefinitionxÙOn one hand, our findings paint a promising picture of deep learning for smartphones, showing the prosperity of mobile deep learning frameworks as well as the prosperity of apps building their cores atop deep learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.364172jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1812.05448v4¨bidheb16dc84gconceptkof researchjdefinitionx´On the other hand, our findings urge optimizations on deep learning models deployed on smartphones, the protection of these models, and validation of research ideas on these modelslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.364208jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1812.05448v4¨bidh65968ff7gconceptdtheyjdefinitionx¬Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do their deep learning models look likelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.364240jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1812.05448v4¨bidhe345beb1gconcepthand deepjdefinitionx`Our study has strong implications for app developers, smartphone vendors, and deep learning R\&Dlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.364274jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1812.05448v4¨bidhedd594fcgconceptjtheir deepjdefinitionxeTo this end, we build a new static tool that dissects apps and analyzes their deep learning functionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.364306jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1812.05448v4¨bidh5bec037dgconceptgsystemsjdefinitionx?How to understand deep learning systems remains an open problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.465131jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1901.02354v2¨bidhb9ed2af6gconceptdrulejdefinitionxªGeometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical worldlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.465199jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1901.02354v2¨bidhd58e7953gconceptfof thejdefinitionxªGeometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical worldlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.465244jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1901.02354v2¨bidh77018a2cgconceptddeepjdefinitionx?How to understand deep learning systems remains an open problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.465284jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1901.02354v2¨bidh9ea9d418gconceptjnew schemejdefinitionxªGeometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical worldlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.465324jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1901.02354v2¨bidhb989dc99gconceptgnetworkjdefinitionxWIn this paper we propose that the answer may lie in the geometrization of deep networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.465378jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1901.02354v2¨bidhe6d0fcdfgconceptfhow tojdefinitionx?How to understand deep learning systems remains an open problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.465420jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1901.02354v2¨bidh155f91c9gconceptkand quantumjdefinitionxªGeometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical worldlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.465460jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1901.02354v2¨bidh180a8164gconceptgbe usedjdefinitionxøBy comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it may also help to solve the interpretability problem of deep learning systemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.465501jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1901.02354v2¨bidhac091cf4gconceptdopenjdefinitionx?How to understand deep learning systems remains an open problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.465538jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1901.02354v2¨bidh7138d467gconcepthresearchjdefinitionx·The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.573782jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1705.03921v1¨bidhf88f906dgconceptiasked sixjdefinitiony.We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of "Why & When Deep Learning works", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potentiallsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.573923jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1705.03921v1¨bidhfbcf10e8gconceptgmachinejdefinitionx·The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.573975jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1705.03921v1¨bidhf93b7a12gconceptminstitute forjdefinitionx·The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.574025jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1705.03921v1¨bidha6e14bcagconcepthand deepjdefinitionx·The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.574068jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1705.03921v1¨bidh7f6d9e3dgconceptddeepjdefinitionx·The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.574123jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1705.03921v1¨bidhca313264gconcepthresultedjdefinitionxcThe output of this challenge resulted in five papers that address different facets of deep learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.574171jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1705.03921v1¨bidhf23cd5bagconceptlintelligencejdefinitionx·The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.574211jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1705.03921v1¨bidh3f8b898egconceptmdeep learningjdefinitionx·The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.574252jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1705.03921v1¨bidh6fa91bd4gconceptetheirjdefinitiony.We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of "Why & When Deep Learning works", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potentiallsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.574296jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1705.03921v1¨bidh4fec510egconceptfcan bejdefinitionxxOur method can be viewed as improving the robustness of deep learning systems from both the learning task and deep modellsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.697649jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2010.05125v2¨bidh4847e629gconceptlaware methodjdefinitionx•Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.697842jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2010.05125v2¨bidh5631daebgconceptgsystemsjdefinitionx—Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning tasklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.697974jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2010.05125v2¨bidh41243da0gconceptmon robustnessjdefinitionx—Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning tasklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.698097jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2010.05125v2¨bidh1acac75dgconceptdthanjdefinitionx•Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.698222jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2010.05125v2¨bidh15b77e70gconceptjmany worksjdefinitionxTMany works demonstrate that deep learning system is vulnerable to adversarial attacklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.698335jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2010.05125v2¨bidh67788c15gconceptjtask-awarejdefinitionx•Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.701604jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2010.05125v2¨bidh310ad79agconcepthand deepjdefinitionxxOur method can be viewed as improving the robustness of deep learning systems from both the learning task and deep modellsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.701821jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2010.05125v2¨bidh5e2697cdgconceptfof thejdefinitionx—Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning tasklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.701953jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2010.05125v2¨bidh4ef08a9agconceptdmorejdefinitionx•Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.702082jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2010.05125v2¨bidh8eb081c9gconceptkse problemsjdefinitionxşHow do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.817517jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1805.04825v1¨bidh785033abgconcepthresearchjdefinitionxşHow do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.817646jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1805.04825v1¨bidhe3b62cb9gconceptkimprove thejdefinitionxŸHow to improve the effectiveness, efficiency, understandability, and testability of deep learning based solutions may attract more SE researchers in the futurelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.817702jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1805.04825v1¨bidhf313f65cgconcepthvariantsjdefinitionxX7% papers only use standard deep learning models and their variants to solve SE problemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.817747jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1805.04825v1¨bidhe032ce52gconceptestilljdefinitionx9However, many open issues still remain to be investigatedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.817793jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1805.04825v1¨bidh8cd56a71gconceptfrecentjdefinitionx_Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.817836jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1805.04825v1¨bidh75339728gconceptdmorejdefinitionxŸHow to improve the effectiveness, efficiency, understandability, and testability of deep learning based solutions may attract more SE researchers in the futurelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.817895jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1805.04825v1¨bidh95eca6fegconceptgimprovejdefinitionxŸHow to improve the effectiveness, efficiency, understandability, and testability of deep learning based solutions may attract more SE researchers in the futurelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.817947jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1805.04825v1¨bidh4f89fbacgconceptddeepjdefinitionx_Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.817989jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1805.04825v1¨bidh71c016a8gconceptetasksjdefinitionxşHow do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.818036jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1805.04825v1¨bidh9aa87b8agconceptjbased deepjdefinitionxRecently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browserslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.935440jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1901.09388v2¨bidh876c5d32gconceptkapplicationjdefinitionxšOur findings could help application developers, deep-learning framework vendors and browser vendors to improve the efficiency of deep learning in browserslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.944754jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1901.09388v2¨bidh7d2b5a5agconceptgimprovejdefinitionxšOur findings could help application developers, deep-learning framework vendors and browser vendors to improve the efficiency of deep learning in browserslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.944907jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1901.09388v2¨bidhf29693edgconceptecouldjdefinitionxšOur findings could help application developers, deep-learning framework vendors and browser vendors to improve the efficiency of deep learning in browserslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.944972jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1901.09388v2¨bidh3e8cc974gconceptddeepjdefinitionxRecently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browserslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.945025jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1901.09388v2¨bidheb516220gconceptfcan wejdefinitionxRecently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browserslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.945078jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1901.09388v2¨bidhaa6a2b2cgconceptetasksjdefinitionxRecently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browserslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.945129jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1901.09388v2¨bidh06326dafgconceptiknowledgejdefinitionxmTo bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browserslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.945185jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1901.09388v2¨bidh488cd5begconceptefirstjdefinitionxmTo bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browserslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.945236jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1901.09388v2¨bidh574ed44agconceptnperformance ofjdefinitionxbThen we measure the performance of different frameworks when running different deep learning taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:36.945285jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1901.09388v2¨bidhdbabf7c6gconceptnand challengesjdefinitionxwQuantum deep learning is a research field for the use of quantum computing techniques for training deep neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.046567jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2108.01468v1¨bidh26fe2f1agconceptpquantum circuitsjdefinitionxóThe research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adoptedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.046695jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.01468v1¨bidh45c966d8gconcepthresearchjdefinitionxwQuantum deep learning is a research field for the use of quantum computing techniques for training deep neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.046749jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.01468v1¨bidhb7364051gconcepthexplainsjdefinitionxxThis paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievementslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.046799jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.01468v1¨bidhcce1e6a2gconceptqquantum computingjdefinitionxwQuantum deep learning is a research field for the use of quantum computing techniques for training deep neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.046842jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.01468v1¨bidh15f0a598gconceptpintroduces majorjdefinitionxxThis paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievementslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.046884jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.01468v1¨bidh9028d18bgconcepthfor longjdefinitionxóThe research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adoptedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.046925jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.01468v1¨bidh314b6ac4gconceptdlongjdefinitionxóThe research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adoptedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.046964jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.01468v1¨bidh10fe73b0gconceptkapplicationjdefinitionxnLastly, this paper presents various future research directions and application fields of quantum deep learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.047004jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2108.01468v1¨bidh7dc22252gconceptlthe researchjdefinitionxóThe research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adoptedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.047041jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.01468v1¨bidh7d9c3e79gconcepthstate-ofjdefinitionxqExtensive experiments show that NetBooster consistently outperforms state-of-the-art tiny deep learning solutionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.158089jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2306.13586v1¨bidh20537f67gconceptideployingjdefinitionx¥Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things deviceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.158277jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2306.13586v1¨bidh966a8901gconceptestilljdefinitionxåHowever, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.164555jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2306.13586v1¨bidh1b742532gconceptgempowerjdefinitionx¦To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the architectures of TNNs via an expansion-then-contraction strategylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.164763jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2306.13586v1¨bidh84d3f354gconcepthnumerousjdefinitionx¥Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things deviceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.164919jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2306.13586v1¨bidh6ceb1c5fgconcepttattracted increasingjdefinitionx¥Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things deviceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.165052jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2306.13586v1¨bidh3994476egconceptfvia anjdefinitionx¦To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the architectures of TNNs via an expansion-then-contraction strategylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.165184jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2306.13586v1¨bidhde62a5d4gconceptfdemandjdefinitionx¥Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things deviceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.165314jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2306.13586v1¨bidhfcc0ebddgconceptishouldersjdefinitionx¥Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things deviceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.165478jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2306.13586v1¨bidh9e3033c5gconceptddeepjdefinitionx¥Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things deviceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.165614jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2306.13586v1¨bidh668ab307gconceptiand statejdefinitionxÔWe compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVDlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.275285jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1602.00203v1¨bidh7f7d9b3dgconceptdthanjdefinitionx)Our method yields better results than alllsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.275510jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1602.00203v1¨bidh31a09252gconceptnsome benchmarkjdefinitionxHWe apply the proposed technique on some benchmark deep learning datasetslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.275663jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1602.00203v1¨bidhed162c94gconceptgshallowjdefinitionxhThis requires solving a simple (shallow) dictionary learning problem, the solution to this is well knownlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.275806jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1602.00203v1¨bidh0cbb3e27gconcepthand deepjdefinitionxÔWe compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVDlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.275951jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1602.00203v1¨bidh94b1153egconceptfof thejdefinitionxÔWe compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVDlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.276092jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1602.00203v1¨bidhedd58062gconceptltechnique onjdefinitionxHWe apply the proposed technique on some benchmark deep learning datasetslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.276231jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1602.00203v1¨bidh6458097agconceptdsomejdefinitionxHWe apply the proposed technique on some benchmark deep learning datasetslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.282466jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1602.00203v1¨bidha7985dfegconceptnart supervisedjdefinitionxÔWe compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVDlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.282761jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1602.00203v1¨bidhcf772aedgconceptddeepjdefinitionxPIn this work we propose a new deep learning tool called deep dictionary learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.282862jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1602.00203v1¨bidha504c19dgconceptestartjdefinitionxrWe start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.400983jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidh064c8628gconcepthresearchjdefinitionxIn this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.401191jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidh496e0fa6gconceptiand statejdefinitionxIn this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.401343jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidhd48632d5gconceptlsegmentationjdefinitionxÑ(i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.401530jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidhcde139e2gconceptravailable datasetsjdefinitionx~Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.401691jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidh27aaa621gconceptsresearch directionsjdefinitionx}Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.401859jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidhc06fc9aegconcepthstate-ofjdefinitionxIn this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.402008jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidhf3133057gconcepteimagejdefinitionxÑ(i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.402180jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidhb9a334f9gconceptfrecentjdefinitionx½Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.402344jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidh3b377d4bgconcepthand deepjdefinitionxrWe start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.410675jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidhe8ed9565gconceptdusesjdefinitiony"The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neural network components which capture complex non-linear stochastic relationships between the random variableslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.515620jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2106.00120v3¨bidha5a799f1gconceptjthe formerjdefinitiony"The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neural network components which capture complex non-linear stochastic relationships between the random variableslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.515768jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2106.00120v3¨bidhddc2c9d3gconceptntwo approachesjdefinitionxyWe distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic modelslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.515829jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2106.00120v3¨bidh374b0a3cgconceptjcomponentsjdefinitiony"The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neural network components which capture complex non-linear stochastic relationships between the random variableslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.515886jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2106.00120v3¨bidhe5cfebbegconcepthfor bothjdefinitionx‘TensorFlow Probability is a library for probabilistic modeling and inference which can be used for both approaches of probabilistic deep learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.515942jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2106.00120v3¨bidh8eab0a49gconceptruses probabilisticjdefinitiony"The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neural network components which capture complex non-linear stochastic relationships between the random variableslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.515994jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2106.00120v3¨bidh297fab3dgconceptdsomejdefinitionyWe discuss some major examples of each approach including Bayesian neural networks and mixture density networks (for probabilistic neural networks), and variational autoencoders, deep Gaussian processes and deep mixed effects models (for deep probabilistic models)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.516044jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2106.00120v3¨bidh97a242d1gconceptddeepjdefinitionxwProbabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertaintylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.516092jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2106.00120v3¨bidhbdb64a5dgconceptjwe includejdefinitionx-We include its code examples for illustrationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.516147jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2106.00120v3¨bidh1a9f17e3gconceptgnetworkjdefinitionxGIt is based on the use of probabilistic models and deep neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.516231jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2106.00120v3¨bidhf40c4a59gconceptjdeploymentjdefinitionyThis paper aims to gather information about these advances from the literature and show how and at which points along the lifecycle of Deep Learning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation) it is possible to reduce energy consumptionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.624014jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.01980v1¨bidhfae60fd7gconceptxassociated environmentaljdefinitionx™However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.624183jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.01980v1¨bidh481a1889gconceptulearning applicationsjdefinitionx^Deep Learning has enabled many advances in machine learning applications in the last few yearslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.624309jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.01980v1¨bidh6df7397dgconceptoresearchers andjdefinitionxŠEnergy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of yearslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.629727jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.01980v1¨bidh2f26811fgconceptgmachinejdefinitionx^Deep Learning has enabled many advances in machine learning applications in the last few yearslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.629936jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.01980v1¨bidh91b1bf4cgconceptkabout thesejdefinitionyThis paper aims to gather information about these advances from the literature and show how and at which points along the lifecycle of Deep Learning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation) it is possible to reduce energy consumptionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.630106jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.01980v1¨bidhc9df71c2gconceptpenergy-efficientjdefinitionxŠEnergy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of yearslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.630249jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2303.01980v1¨bidh998aa459gconcepthoverviewjdefinitionx^Deep Learning has enabled many advances in machine learning applications in the last few yearslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.630414jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2303.01980v1¨bidh17d89d0egconceptjthis paperjdefinitionyThis paper aims to gather information about these advances from the literature and show how and at which points along the lifecycle of Deep Learning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation) it is possible to reduce energy consumptionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.630554jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.01980v1¨bidh81acaa5egconceptddeepjdefinitionx^Deep Learning has enabled many advances in machine learning applications in the last few yearslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.630688jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2303.01980v1¨bidh5ad3b99dgconceptrgreat significancejdefinitiony	This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.734988jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1805.03551v2¨bidha2147f50gconceptqunified frameworkjdefinitionxKThen, we set up a unified framework for deep learning with capsule networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.735194jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1805.03551v2¨bidhb4d6371cgconceptgcapsulejdefinitionxKThen, we set up a unified framework for deep learning with capsule networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.735339jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1805.03551v2¨bidhba081faegconceptgprovidejdefinitiony	This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.748599jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1805.03551v2¨bidh9ecfc69agconceptecouldjdefinitiony	This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.748776jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1805.03551v2¨bidh1d26fa0fgconceptqtheoretical basisjdefinitiony	This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.748926jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1805.03551v2¨bidh4be7fe14gconceptddeepjdefinitionxoWith the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issuelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.749058jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1805.03551v2¨bidhf7db4692gconceptefirstjdefinitionx»We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.749796jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1805.03551v2¨bidhde787fb5gconceptlof connectedjdefinitionx»We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.749994jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1805.03551v2¨bidhd957db6cgconceptggraphicjdefinitiony	This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.750132jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1805.03551v2¨bidha6f03cf4gconcepthresearchjdefinitionx¨The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.867889jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1901.04195v1¨bidh56e7a304gconceptjespeciallyjdefinitionx£Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patternslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.868076jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1901.04195v1¨bidh405e1f43gconceptgto takejdefinitionx|Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environmentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.868200jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1901.04195v1¨bidhceb538a7gconcepthpatternsjdefinitionx£Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patternslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.868317jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1901.04195v1¨bidhd00c6c06gconceptdhandjdefinitionx|Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environmentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.868469jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1901.04195v1¨bidh99278293gconceptelogicjdefinitionx|Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environmentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.868600jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1901.04195v1¨bidhf454f646gconceptestilljdefinitionx¨The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.868784jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1901.04195v1¨bidh3cb610f1gconceptfof thejdefinitionx¯Deep Logic Models create an end-to-end differentiable architecture, where deep learners are embedded into a network implementing a continuous relaxation of the logic knowledgelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.868946jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1901.04195v1¨bidhe58987a7gconceptndifferentiablejdefinitionx¯Deep Logic Models create an end-to-end differentiable architecture, where deep learners are embedded into a network implementing a continuous relaxation of the logic knowledgelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.869094jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1901.04195v1¨bidhd571d119gconceptrand classificationjdefinitionx£Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patternslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.869233jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1901.04195v1¨bidh9667f2adgconceptkrecent pastjdefinitionx‡In the recent past, deep learning techniques have been frequently applied in biometric template protection systems for various purposeslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.987992jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.02715v1¨bidhf1d2edbagconceptjto achievejdefinitionx‚Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.988170jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.02715v1¨bidh11444271gconceptgsystemsjdefinitionx‚Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.988298jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.02715v1¨bidhfce0b53dgconceptjdeploymentjdefinitionx}Technologies of biometric template protection are designed to enable a secure and privacy-preserving deployment of biometricslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.988470jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.02715v1¨bidhbeb6c08fgconceptkor templatejdefinitionxêApart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fairness, vulnerability to attacks, or template protectionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.988615jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.02715v1¨bidh61e0f998gconceptkrecognitionjdefinitionxbDeep learning has revolutionised the field of pattern recognition, including biometric recognitionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.988743jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.02715v1¨bidh1d0af46dgconceptetodayjdefinitionxXToday, deep learning represents the most popular and successful form of machine learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.988897jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.02715v1¨bidh4760a4eegconceptgmachinejdefinitionxXToday, deep learning represents the most popular and successful form of machine learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.989023jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.02715v1¨bidhd233a8f6gconcepthoverviewjdefinitionx|This work provides an overview of how advances in deep learning take influence on the field of biometric template protectionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.989180jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2303.02715v1¨bidh0896ac85gconceptfrecentjdefinitionx‡In the recent past, deep learning techniques have been frequently applied in biometric template protection systems for various purposeslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:37.989326jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2303.02715v1¨bidh39bb7efegconceptnsolve problemsjdefinitionx´Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.106254jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidh904ae4dcgconceptkfurthermorejdefinitionx©Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.106482jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidh92296ac4gconcepthresearchjdefinitionx´Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.106966jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidh9f3d2ac9gconceptfwithinjdefinitionxÒFrom exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth viewlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.107497jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidhce707deegconceptmformalize andjdefinitionxZIn this paper, we will formalize and analyze generalization in deep reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.107990jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidhea0709a8gconceptganalyzejdefinitionxZIn this paper, we will formalize and analyze generalization in deep reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.108159jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidh516bd362gconceptmquestions thejdefinitionyWhile deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.110081jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidh41fbbbc4gconceptestilljdefinitionyWhile deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.112508jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidh88e7ad4egconceptianalyzingjdefinitionx´Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.112849jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidh20ab45d6gconceptgongoingjdefinitionyWhile deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.113039jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidh37d66c68gconceptgmachinejdefinitionx•In order to address this very crucial question, here we see deep learning from perspective of mechanical learning and learning machine (see [1], [2])lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.224240jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1711.03577v1¨bidh48fa3a06gconcepthtried tojdefinitionxšFor example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.225309jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1711.03577v1¨bidhbf271d4agconceptestilljdefinitionxAYet, what deep learning is really doing is still an open questionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.227169jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1711.03577v1¨bidh857db328gconceptnadvantages andjdefinitionxZWe also will discuss advantages and disadvantages of deep learning at the end of this worklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.227455jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1711.03577v1¨bidhe5e81e7egconceptdmorejdefinitionxDeep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much morelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.227588jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1711.03577v1¨bidhfd284f3bgconcepththe viewjdefinitionxšFor example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.227725jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1711.03577v1¨bidh4dc0aa75gconcepthlanguagejdefinitionxDeep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much morelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.227852jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1711.03577v1¨bidh2a56f557gconceptddeepjdefinitionxDeep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much morelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.227970jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1711.03577v1¨bidh94314bdegconcepthworks injdefinitionx*There are a lot of works in this directionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.228094jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1711.03577v1¨bidh201f0f76gconceptfat thejdefinitionxZWe also will discuss advantages and disadvantages of deep learning at the end of this worklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:38.228219jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1711.03577v1¨bidhecdf6f24gconceptgseen asjdefinitionxTEspecially, Machine Learning with Neural Networks is seen as an optimization problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.882139jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.05133v2¨bidh3dda55e4gconceptmarchitecturesjdefinitionxiThese lecture notes provide an overview of Neural Network architectures from a mathematical point of viewlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.882262jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2304.05133v2¨bidhedf71064gconceptjespeciallyjdefinitionxTEspecially, Machine Learning with Neural Networks is seen as an optimization problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.882314jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.05133v2¨bidhb9abd98egconceptkwith neuraljdefinitionxTEspecially, Machine Learning with Neural Networks is seen as an optimization problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.884518jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.05133v2¨bidhff9d9a24gconceptunetwork architecturesjdefinitionxiThese lecture notes provide an overview of Neural Network architectures from a mathematical point of viewlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.884763jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2304.05133v2¨bidhffa35be9gconceptgmachinejdefinitionxTEspecially, Machine Learning with Neural Networks is seen as an optimization problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.884840jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.05133v2¨bidh8142dae1gconceptkcovered arejdefinitionx®Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Networklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.884924jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.05133v2¨bidh421d4fbcgconcepthoverviewjdefinitionxiThese lecture notes provide an overview of Neural Network architectures from a mathematical point of viewlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.884977jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.05133v2¨bidh15e88642gconceptdseenjdefinitionxTEspecially, Machine Learning with Neural Networks is seen as an optimization problemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.885025jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.05133v2¨bidh82fa52f0gconceptfneuraljdefinitionxiThese lecture notes provide an overview of Neural Network architectures from a mathematical point of viewlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.885073jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2304.05133v2¨bidhc9a7bd23gconceptfto thejdefinitionx¦The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnosticslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.989267jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0504056v1¨bidhddd7bd54gconceptjconsideredjdefinitionxThe principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning setlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.990293jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0504056v1¨bidh8cb2f5cdgconceptglogicaljdefinitionx¦The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnosticslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.990420jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0504056v1¨bidh71515dffgconceptgused tojdefinitionx¦The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnosticslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.990473jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0504056v1¨bidh17c7da1egconceptkdiagnosticsjdefinitionx¦The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnosticslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.990518jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0504056v1¨bidh41984468gconceptgmedicaljdefinitionx¦The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnosticslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.990561jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0504056v1¨bidh35d2849fgconceptfneuraljdefinitionxThe principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning setlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.990603jconfidenceû?îffffffnparent_sourcesxhttps://arxiv.org/abs/0504056v1¨bidhe318e3begconcepthnetworksjdefinitionxThe principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning setlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.990646jconfidenceû?îffffffnparent_sourcesxhttps://arxiv.org/abs/0504056v1¨bidh68f0e412gconceptsmedical diagnosticsjdefinitionx¦The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnosticslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.990683jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0504056v1¨bidhced811f9gconceptemultijdefinitionx¦The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnosticslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:39.990719jconfidenceû?îffffffnparent_sourcesxhttps://arxiv.org/abs/0504056v1¨bidhdd36bcd9gconceptfcan bejdefinitionxIFunctions are rich in meaning and can be interpreted in a variety of wayslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.096312jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.05640v2¨bidh1784fe1cgconceptmfunctions arejdefinitionxIFunctions are rich in meaning and can be interpreted in a variety of wayslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.096495jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.05640v2¨bidhe7d70338gconceptgprocessjdefinitionxÉIn this paper, we propose a new class of neural networks called "Neural Network Processing Neural Networks" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical valueslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.096547jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1911.05640v2¨bidh97568b1fgconceptfnnpnnsjdefinitionxÉIn this paper, we propose a new class of neural networks called "Neural Network Processing Neural Networks" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical valueslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.096595jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.05640v2¨bidhf1d363c7gconceptnjust numericaljdefinitionxÉIn this paper, we propose a new class of neural networks called "Neural Network Processing Neural Networks" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical valueslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.096641jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.05640v2¨bidhe99af4afgconceptjwe proposejdefinitionxÉIn this paper, we propose a new class of neural networks called "Neural Network Processing Neural Networks" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical valueslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.096687jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.05640v2¨bidh3b2412a1gconceptgproposejdefinitionxÉIn this paper, we propose a new class of neural networks called "Neural Network Processing Neural Networks" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical valueslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.096733jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.05640v2¨bidh090f567bgconceptelearnjdefinitionxIFunctions are rich in meaning and can be interpreted in a variety of wayslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.096777jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1911.05640v2¨bidh6a1a18b1gconcepteorderjdefinitionxIFunctions are rich in meaning and can be interpreted in a variety of wayslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.096823jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1911.05640v2¨bidh16f07c79gconceptmand numericaljdefinitionxÉIn this paper, we propose a new class of neural networks called "Neural Network Processing Neural Networks" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical valueslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.096871jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1911.05640v2¨bidh8aca9772gconceptqcomputation issuejdefinitionxNeural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.198969jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.13812v1¨bidhaa412ff8gconceptfmergedjdefinitionxŸA merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.199074jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.13812v1¨bidh332fc516gconceptgsystemsjdefinitionxNeural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.199122jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.13812v1¨bidhaad9d6d6gconceptjindustrialjdefinitionxNeural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.199165jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.13812v1¨bidh036e77b2gconceptiaddressedjdefinitionx{The guaranteed output error computation problem for neural network compression with quantization is addressed in this paperlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.199201jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.13812v1¨bidha2a1ba29gconceptmmerged neuraljdefinitionxŸA merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.199240jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.13812v1¨bidhc0c5c072gconceptfof thejdefinitionxqFinally, a numerical example is proposed to validate the applicability and effectiveness of the proposed approachlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.199282jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.13812v1¨bidhc3882ffbgconceptjversion tojdefinitionxŸA merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.199327jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.13812v1¨bidh8740558cgconceptkaddress thejdefinitionxNeural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.199391jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.13812v1¨bidh563924f7gconceptddeepjdefinitionxNeural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.199439jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.13812v1¨bidh57a9c3c5gconceptfof itsjdefinitionyôUsing this representation we show that: (1) a "sweet spot" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly simlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.305574jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2007.06559v2¨bidh469db2d1gconceptmbe identifiedjdefinitionyôUsing this representation we show that: (1) a "sweet spot" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly simlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.305703jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2007.06559v2¨bidh6a974e29gconceptqbiological neuraljdefinitionyôUsing this representation we show that: (1) a "sweet spot" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly simlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.305765jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2007.06559v2¨bidh1deb672cgconceptnperformance isjdefinitionyôUsing this representation we show that: (1) a "sweet spot" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly simlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.305820jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2007.06559v2¨bidh08ec95e1gconceptfof thejdefinitionx­However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.305870jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2007.06559v2¨bidh605c1c93gconceptqperforming neuraljdefinitionyôUsing this representation we show that: (1) a "sweet spot" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly simlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.305931jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2007.06559v2¨bidhb4fe84dcgconceptmcorrespond tojdefinitionxÔTo this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structurelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.305984jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2007.06559v2¨bidh299b6ecegconceptithe graphjdefinitionx­However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.306033jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2007.06559v2¨bidh60596bffgconceptetasksjdefinitionyôUsing this representation we show that: (1) a "sweet spot" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly simlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.306086jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2007.06559v2¨bidha2b6286cgconceptgdespitejdefinitionx­However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.306139jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2007.06559v2¨bidh1f2ef777gconceptjto combinejdefinitionxŒWe aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.410491jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2408.04747v1¨bidh5fc03a18gconceptfcan bejdefinitionx”The classical neural network can be jointly trained with the quantum neural network or pre-trained leading to a fine-tuning transfer learning methodlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.410654jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2408.04747v1¨bidh586724ccgconceptlone proposesjdefinitionxThe first one proposes a quantum neural network employing parameterized quantum circuits that follows a classical convolutional neural networklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.410729jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2408.04747v1¨bidh5e15e07bgconceptqquantum-classicaljdefinitionxsSpecifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink systemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.410789jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2408.04747v1¨bidh125914b3gconceptrmultiuser multiplejdefinitionxˆThis paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink systemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.410848jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2408.04747v1¨bidh50c0f97dgconcepthhardwarejdefinitionxThe robustness of the proposed methods is verified using both software simulators and hardware emulators considering noisy intermediate-scale quantum deviceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.410909jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2408.04747v1¨bidha59d8ab1gconceptmcircuits thatjdefinitionxThe first one proposes a quantum neural network employing parameterized quantum circuits that follows a classical convolutional neural networklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.410965jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2408.04747v1¨bidh2cff742agconceptjespeciallyjdefinitionytOur results demonstrate the feasibility of the proposed hybrid neural networks, and reveal that the first method can achieve similar sum rate performance compared to a benchmark classical neural network with significantly less training parameters; while the second method can achieve higher sum rate especially in presence of many users still with less training parameterslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.411024jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2408.04747v1¨bidh7ae8fdfdgconceptwclassical convolutionaljdefinitionxThe first one proposes a quantum neural network employing parameterized quantum circuits that follows a classical convolutional neural networklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.411081jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2408.04747v1¨bidh8384241agconcepthsum ratejdefinitionxsSpecifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink systemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.411137jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2408.04747v1¨bidhe537fce3gconceptkfurthermorejdefinitionyFurthermore, it is also shown that the accuracy of a poorly trained neural network model can be improved to the same level as that of an adequately trained neural network model, potentially decreasing the training cost and required data to train a neural networklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.521335jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2409.13654v2¨bidh13af69a1gconceptgsystemsjdefinitionxThe application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.521463jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2409.13654v2¨bidh0d2a69b6gconcepthrequiredjdefinitionyFurthermore, it is also shown that the accuracy of a poorly trained neural network model can be improved to the same level as that of an adequately trained neural network model, potentially decreasing the training cost and required data to train a neural networklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.521510jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2409.13654v2¨bidh8e1ef5bagconceptothe applicationjdefinitionxThe application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.521553jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2409.13654v2¨bidh41699431gconceptilong-termjdefinitionx£Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.521596jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2409.13654v2¨bidhdbe4c983gconceptdoverjdefinitionx£Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.521642jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2409.13654v2¨bidh48f22c40gconceptksystems hasjdefinitionxThe application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.521682jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2409.13654v2¨bidhae572c52gconceptdlongjdefinitionx£Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.521728jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2409.13654v2¨bidh3aefe0bdgconceptkapplicationjdefinitionxThe application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.521770jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2409.13654v2¨bidh4897fa83gconceptpimprove accuracyjdefinitionxThe application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.521825jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2409.13654v2¨bidh952ff51agconceptereachjdefinitionxXOur experiments proved its ability on the Cortex Neural Network can reach accuracy by 98lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.624889jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1804.03313v1¨bidh4c5f2b0dgconceptohigher accuracyjdefinitionxIn our implementation, the Cortex Neural Network is able to process different cognitive tasks and perform reflection to get a higher accuracylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.624960jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1804.03313v1¨bidh1c093cd5gconceptkthe currentjdefinitionx¤However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs togetherlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.624994jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1804.03313v1¨bidhd3eb2346gconceptkrecognitionjdefinitionx{Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.625026jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1804.03313v1¨bidh3ed922b6gconceptgmachinejdefinitionx{Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.625057jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1804.03313v1¨bidhff76aa33gconceptgprovidejdefinitionxvWe provide a series of experiments to examine the capability of the cortex architecture on traditional neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.625094jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1804.03313v1¨bidh1f534770gconceptvimportant architecturejdefinitionxqCortex, as an important architecture in the brain, is important for animals to perform the complex cognitive tasklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.625129jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1804.03313v1¨bidhe18480f4gconceptphandle differentjdefinitionx­The Cortex Neural Network is an upper architecture of neural networks which motivated from cerebral cortex in the brain to handle different tasks in the same learning systemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.625187jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1804.03313v1¨bidh658da9c0gconceptetasksjdefinitionx¤However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs togetherlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.625230jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1804.03313v1¨bidh3e03a29cgconceptganimalsjdefinitionxqCortex, as an important architecture in the brain, is important for animals to perform the complex cognitive tasklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.625265jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1804.03313v1¨bidh1a15c2a9gconceptnoptical neuraljdefinitionx‘A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is givenlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.726270jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0608073v1¨bidha4814b0egconcepthpnn havejdefinitionxoFor today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.726335jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0608073v1¨bidh83839ed4gconceptkbasic ideasjdefinitionx9Presentation of basic ideas and principles is accentuatedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.726413jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0608073v1¨bidhe78695e3gconceptlparametricaljdefinitionxNThe presentation is based on description of parametrical neural networks (PNN)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.726454jconfidenceû?îffffffnparent_sourcesxhttps://arxiv.org/abs/0608073v1¨bidh373fcf47gconceptmarchitecturesjdefinitionx‘A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is givenlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.726497jconfidenceû?îffffffnparent_sourcesxhttps://arxiv.org/abs/0608073v1¨bidh2961d877gconceptegivenjdefinitionx‘A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is givenlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.726531jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0608073v1¨bidhbbc469dagconceptnon descriptionjdefinitionxNThe presentation is based on description of parametrical neural networks (PNN)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.726564jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0608073v1¨bidhb4896b1cgconceptkrecognizingjdefinitionxoFor today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.726599jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0608073v1¨bidha3766507gconcepthimmunityjdefinitionxoFor today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.726632jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0608073v1¨bidh3417650egconceptetodayjdefinitionxoFor today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.726666jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/0608073v1¨bidh096c6475gconceptgaiq wasjdefinitionx.The LeNet-5 network with the highest aIQ was 2lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.829859jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2006.02909v1¨bidh74c3c087gconceptqlayer utilizationjdefinitionxòTo this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiencylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.829961jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2006.02909v1¨bidhb2cfceb5gconceptunetwork architecturesjdefinitionxˆThe purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.829998jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2006.02909v1¨bidhb6e98338gconceptdthanjdefinitionx_32% less accurate but contained 30,912 times fewer parameters than the highest accuracy networklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.830039jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2006.02909v1¨bidh4dd8e026gconceptfof thejdefinitionx451% even when 75% of the class labels are randomizedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.830078jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2006.02909v1¨bidh0a0ab5f8gconceptpand overtrainingjdefinitionxšFinally, high aIQ networks are shown to be memorization and overtraining resistant, capable of learning proper digit classification with an accuracy of 92lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.830116jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2006.02909v1¨bidh58459f53gconceptdevenjdefinitionx451% even when 75% of the class labels are randomizedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.830154jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2006.02909v1¨bidh0366f433gconcepthquotientjdefinitionxòTo this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiencylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.830194jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2006.02909v1¨bidh9d6e4d3fgconceptnare randomizedjdefinitionx451% even when 75% of the class labels are randomizedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.830235jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2006.02909v1¨bidh1e6d4f36gconceptfmetricjdefinitionxˆThe purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.830273jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2006.02909v1¨bidhba35e905gconcepththey usejdefinitionx–However, one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.935278jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2307.06287v1¨bidh66b20bf7gconceptgsystemsjdefinitionx­Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controllerlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.935455jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2307.06287v1¨bidh19623c84gconcepttactivation functionsjdefinitionx®This paper considers rational neural networks and presents novel rational activation functions, which can be used effectively in robustness problems for neural feedback loopslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.935513jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2307.06287v1¨bidhbe96116cgconceptkcontrollersjdefinitionx¿This means that, unless they are designed properly, they are not an ideal candidate for controllers due to issues with robustness and uncertainty, which are pivotal aspects of control systemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.935569jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2307.06287v1¨bidhf9c3b942gconceptunetwork architecturesjdefinitionx–However, one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.935613jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2307.06287v1¨bidh73dac9cegconceptgmachinejdefinitionxNeural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximatorslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.935654jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2307.06287v1¨bidhc9b62fd4gconceptkthis methodjdefinitionxÈNumerical examples show that this method can successfully recover stabilising rational neural network controllers for neural feedback loops with non-linear plants with noise and parametric uncertaintylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.935698jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2307.06287v1¨bidh9f87a6a7gconceptdtheyjdefinitionx¿This means that, unless they are designed properly, they are not an ideal candidate for controllers due to issues with robustness and uncertainty, which are pivotal aspects of control systemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.935739jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2307.06287v1¨bidh051c5196gconceptfrecentjdefinitionx­Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controllerlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.935778jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2307.06287v1¨bidhd9fd83f5gconceptmrelated tasksjdefinitionxNeural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximatorslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:40.935823jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2307.06287v1¨bidh185b7e8fgconceptoparameter spacejdefinitionx0We use sieve method to constrain parameter spacelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.036942jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2011.01218v1¨bidha7d81de2gconceptfand wejdefinitionxSAnd we prove its consistency and normality under nonparametric regression frameworklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.037043jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2011.01218v1¨bidhe8eaf6e0gconceptlapplicationsjdefinitionxKNeural networks are becoming an increasingly important tool in applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.037087jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2011.01218v1¨bidh7af46ca2gconceptqasymptotic theoryjdefinitionxKNeural networks are becoming an increasingly important tool in applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.037161jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2011.01218v1¨bidh10ef2481gconceptjwe proposejdefinitionxWIn this paper, we propose a new neural networks method called expectile neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.037203jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2011.01218v1¨bidha6ba231fgconceptnin statisticaljdefinitionxDHowever, neural networks are not widely used in statistical geneticslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.037239jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2011.01218v1¨bidhd941efcdgconceptgsize ofjdefinitionx`When the size of parameter is too large, the standard maximum likelihood procedures may not worklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.037274jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2011.01218v1¨bidh077686begconceptiprove itsjdefinitionxSAnd we prove its consistency and normality under nonparametric regression frameworklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.037309jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2011.01218v1¨bidh209c6844gconceptgproposejdefinitionxWIn this paper, we propose a new neural networks method called expectile neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.037342jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2011.01218v1¨bidh3aaa000cgconceptdsizejdefinitionx`When the size of parameter is too large, the standard maximum likelihood procedures may not worklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.037397jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/2011.01218v1¨bidhe0f10bbcgconceptkfurthermorejdefinitionx€Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimizationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.139650jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1605.07333v1¨bidhb1269843gconceptiintroducejdefinitionx€Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimizationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.139759jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1605.07333v1¨bidhf72db686gconceptlof differentjdefinitionxMFor both models, we demonstrate the effect of different architectural choiceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.139802jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1605.07333v1¨bidhd1fe87f1gconceptpinvestigates twojdefinitionxŸThis paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.139842jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1605.07333v1¨bidh1e918336gconcepthstate-ofjdefinitionxcOur neural models achieve state-of-the-art results on the SemEval 2010 relation classification tasklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.139889jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1605.07333v1¨bidh63a8d03agconceptmconnectionistjdefinitionx€Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimizationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.139927jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1605.07333v1¨bidh55118728gconcepthrelationjdefinitionxŸThis paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.139963jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1605.07333v1¨bidhf5eb5dfbgconceptgimprovejdefinitionxFinally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve resultslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.140038jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1605.07333v1¨bidh9713be18gconceptjthis paperjdefinitionxŸThis paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.140085jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1605.07333v1¨bidhb2d8bb8dgconcepthfor bothjdefinitionxMFor both models, we demonstrate the effect of different architectural choiceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.140127jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1605.07333v1¨bidhff1e0d59gconceptmcontributionsjdefinitionxòKey contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitionerslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.241625jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2303.10780v2¨bidh9c194a4fgconcepthhardwarejdefinitionyAnd yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settingslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.241782jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2303.10780v2¨bidh9e10b0e9gconceptqbiological neuraljdefinitionxZBiological neural networks continue to inspire breakthroughs in neural network performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.241843jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2303.10780v2¨bidhfb5fec64gconceptjdiscussionjdefinitionxòKey contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitionerslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.241890jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2303.10780v2¨bidh14b11fffgconceptjespeciallyjdefinitionyAnd yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settingslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.241942jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2303.10780v2¨bidhd05313d8gconceptjof spikingjdefinitionxWe present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.241990jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2303.10780v2¨bidh28b7f96cgconceptpenergy-efficientjdefinitionyAnd yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settingslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.242040jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2303.10780v2¨bidheb1b1693gconceptfrecentjdefinitionxWe present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.242089jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2303.10780v2¨bidhc1bf9ad8gconceptmbreakthroughsjdefinitionxZBiological neural networks continue to inspire breakthroughs in neural network performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.242134jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2303.10780v2¨bidh0c7936e5gconceptefirstjdefinitionxòKey contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitionerslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.242182jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2303.10780v2¨bidh96074302gconcepttprecision activationjdefinitionxëThis work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.344976jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.10289v1¨bidh1f4ee799gconceptdoverjdefinitionx…Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.345047jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.10289v1¨bidh07ddab86gconceptmcomparable tojdefinitionxëThis work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.345092jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.10289v1¨bidh30b1dd87gconceptdstdpjdefinitionxëThis work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.345133jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.10289v1¨bidh24895fbcgconceptiprecisionjdefinitionxëThis work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.345190jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.10289v1¨bidh8870bde8gconceptjactivationjdefinitionxëThis work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.345235jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.10289v1¨bidh348cd205gconceptgnetworkjdefinitionx•Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many otherslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.345274jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2401.10289v1¨bidhacfab479gconceptgrealismjdefinitionxmLiving neural networks offer advantages of lower power consumption, faster processing, and biological realismlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.345320jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.10289v1¨bidh6ff58116gconceptrachieving accuracyjdefinitionxëThis work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.345374jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.10289v1¨bidhb4b98270gconceptjindirectlyjdefinitionxëThis work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.345436jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.10289v1¨bidh77970e96gconceptlsuch systemsjdefinitionxşIt was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.449078jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1911.07626v1¨bidhd3bf4c39gconceptgsystemsjdefinitionxşIt was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.449197jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1911.07626v1¨bidh6b8f0f82gconceptjshown thatjdefinitionxşIt was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.449256jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1911.07626v1¨bidh45072803gconceptganalyzejdefinitionxvHowever, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpracticelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.449305jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1911.07626v1¨bidh3b7d9709gconcepttof overparameterizedjdefinitionx]Analysis of over-parameterized neural networks has drawn significant attention in recentyearslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.449839jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1911.07626v1¨bidh5d2c835fgconceptdoverjdefinitionx]Analysis of over-parameterized neural networks has drawn significant attention in recentyearslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.449988jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1911.07626v1¨bidh7ff0afacgconceptrconvex formulationjdefinitionx]Analysis of over-parameterized neural networks has drawn significant attention in recentyearslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.450039jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1911.07626v1¨bidh8f208656gconcepthfeaturesjdefinitionxêIt is shown that under suitable representations, overparameterized deep neural networks are inherently convex, and when optimized, the system can learn effective features suitable for the underlying learning task under mild conditionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.450083jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1911.07626v1¨bidhde82cb77gconceptjthis paperjdefinitionxyThis paper solves this fundamental problem by investigating such overparameterizeddeep neural networks when fully trainedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.450124jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1911.07626v1¨bidh8ce79c68gconceptvencountered inpracticejdefinitionxvHowever, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpracticelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.450162jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1911.07626v1¨bidh79fe24d3gconceptkapplicationjdefinitionxhIn this paper, we propose a concept of approximate bisimulation relation for feedforward neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.555456jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2202.01214v1¨bidh73599210gconcepthrelationjdefinitionxhIn this paper, we propose a concept of approximate bisimulation relation for feedforward neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.555518jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2202.01214v1¨bidh17e78bfegconceptnperform neuraljdefinitionxThen, we apply the approximate bisimulation relation results to perform neural networks model reduction and compute the compression precision, ilsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.555558jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.01214v1¨bidh167d3d44gconceptiprecisionjdefinitionxThen, we apply the approximate bisimulation relation results to perform neural networks model reduction and compute the compression precision, ilsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.555597jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.01214v1¨bidh09cc9d07gconceptlthe distancejdefinitionxƒThe developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.555633jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.01214v1¨bidhdf97e1c1gconceptgconceptjdefinitionxhIn this paper, we propose a concept of approximate bisimulation relation for feedforward neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.555666jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.01214v1¨bidh4ea7d7b6gconceptlrelation forjdefinitionxhIn this paper, we propose a concept of approximate bisimulation relation for feedforward neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.555699jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.01214v1¨bidh313ce712gconceptgnetworkjdefinitionxhIn this paper, we propose a concept of approximate bisimulation relation for feedforward neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.555732jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2202.01214v1¨bidh40fb462egconcepthdistancejdefinitionxƒThe developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.555766jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.01214v1¨bidh1e494408gconceptlbisimulationjdefinitionxhIn this paper, we propose a concept of approximate bisimulation relation for feedforward neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.555799jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2202.01214v1¨bidhd2db35d2gconceptkconvergencejdefinitionx³As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNNlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.657776jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.01561v3¨bidhc7aae0cagconceptqfor nonparametricjdefinitionx³As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNNlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.657841jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.01561v3¨bidh3e7ec7f2gconcepttspaces correspondingjdefinitionxnWe study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.657880jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.01561v3¨bidh1c6e6b6fgconceptjshown thatjdefinitionxhIt is shown that sufficiently smooth functions are contained in these spaces with finite variation normslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.657921jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.01561v3¨bidh4fb3fa7agconceptkusing thesejdefinitionxUsing these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.657959jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.01561v3¨bidh609a1242gconceptdoverjdefinitionx³As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNNlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.658001jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.01561v3¨bidh2ca98e94gconceptjthree relujdefinitionx³As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNNlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.658042jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.01561v3¨bidha1e30736gconceptmnonparametricjdefinitionx³As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNNlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.658082jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2304.01561v3¨bidhb545a808gconceptkestablishedjdefinitionxjFor functions with less smoothness, the approximation rates in terms of the variation norm are establishedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.658118jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2304.01561v3¨bidhcd150ff3gconceptgshallowjdefinitionxnWe study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.658152jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2304.01561v3¨bidh68bafc1agconceptfcan bejdefinitionx¢Precisely, a vector-valued neural network can be obtained by placing restrictions on a real-valued model to consider the intercorrelation between feature channelslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.760059jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2309.07716v2¨bidhe2825bcegconceptkfurthermorejdefinitionxgFurthermore, this paper explains the relationship between vector-valued and traditional neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.760197jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2309.07716v2¨bidhb9ec6244gconcepthexplainsjdefinitionxgFurthermore, this paper explains the relationship between vector-valued and traditional neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.760258jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2309.07716v2¨bidh8a1dce17gconceptpintercorrelationjdefinitionxšThe intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful traininglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.760298jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2309.07716v2¨bidh01ec9078gconceptdthanjdefinitionxxConsequently, they usually have fewer parameters and often undergo more robust training than traditional neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.760337jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2309.07716v2¨bidhd0b6a1bfgconceptdtheyjdefinitionxxConsequently, they usually have fewer parameters and often undergo more robust training than traditional neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.760411jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2309.07716v2¨bidhe32c0560gconcepthnumerousjdefinitionxšThe intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful traininglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.760452jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2309.07716v2¨bidh65ad67aagconcepthreferredjdefinitionxeThis paper aims to present a broad framework for vector-valued neural networks, referred to as V-netslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.760490jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2309.07716v2¨bidh5fb240e9gconceptdmorejdefinitionxxConsequently, they usually have fewer parameters and often undergo more robust training than traditional neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.760530jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2309.07716v2¨bidhb82ab873gconceptjthis paperjdefinitionxeThis paper aims to present a broad framework for vector-valued neural networks, referred to as V-netslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.760573jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2309.07716v2¨bidhf6b643d7gconceptione weirdjdefinitionxeI present a new way to parallelize the training of convolutional neural networks across multiple GPUslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.861230jconfidenceû?îffffffnparent_sourcesx!https://arxiv.org/abs/1404.5997v2¨bidha0269c02gconceptito modernjdefinitionxqThe method scales significantly better than all alternatives when applied to modern convolutional neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.861297jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1404.5997v2¨bidha92ed5a9gconcepthmultiplejdefinitionxeI present a new way to parallelize the training of convolutional neural networks across multiple GPUslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.861320jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1404.5997v2¨bidh987bc975gconceptlwhen appliedjdefinitionxqThe method scales significantly better than all alternatives when applied to modern convolutional neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.861342jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1404.5997v2¨bidh43d95ca0gconceptdthanjdefinitionxqThe method scales significantly better than all alternatives when applied to modern convolutional neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.861390jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1404.5997v2¨bidh5c7332bdgconceptfneuraljdefinitionxeI present a new way to parallelize the training of convolutional neural networks across multiple GPUslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.861414jconfidenceû?îffffffnparent_sourcesx!https://arxiv.org/abs/1404.5997v2¨bidhb1e34163gconcepthnetworksjdefinitionxeI present a new way to parallelize the training of convolutional neural networks across multiple GPUslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.861435jconfidenceû?îffffffnparent_sourcesx!https://arxiv.org/abs/1404.5997v2¨bidh25068f32gconceptdgpusjdefinitionxeI present a new way to parallelize the training of convolutional neural networks across multiple GPUslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.861457jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1404.5997v2¨bidh4505b518gconceptgnew wayjdefinitionxeI present a new way to parallelize the training of convolutional neural networks across multiple GPUslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.861478jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1404.5997v2¨bidh405abeecgconcepttscales significantlyjdefinitionxqThe method scales significantly better than all alternatives when applied to modern convolutional neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:41.861531jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1404.5997v2¨bidh39d64629gconcepthrequiredjdefinitionxqHadoop is one of the platforms that can process the large amount of data required for natural language processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.456409jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1608.04434v1¨bidh636ec8eagconcepthresearchjdefinitionx¶Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.456533jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1608.04434v1¨bidhcaa92defgconceptganalyzejdefinitionxxThis study describes how to build a KOSHIK platform with the relevant tools, and provides the steps to analyze wiki datalsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.456576jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1608.04434v1¨bidhf1eba0c7gconceptorecommendationsjdefinitionx¢Finally, it evaluates and discusses the advantages and disadvantages of the KOSHIK architecture, and gives recommendations on improving the processing performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.456613jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1608.04434v1¨bidh04fead55gconceptpprocessing usingjdefinitionx¶Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.456651jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1608.04434v1¨bidha38b0821gconceptjcomponentsjdefinitionx¤KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains language processing components such as Stanford CoreNLP and OpenNLPlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.456686jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1608.04434v1¨bidh79ff4954gconceptfof thejdefinitionxqHadoop is one of the platforms that can process the large amount of data required for natural language processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.456722jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1608.04434v1¨bidhcec30013gconceptkand opennlpjdefinitionx¤KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains language processing components such as Stanford CoreNLP and OpenNLPlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.456756jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1608.04434v1¨bidh5763284egconcepthlanguagejdefinitionx¶Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.456789jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1608.04434v1¨bidha096586egconceptmdescribes howjdefinitionxxThis study describes how to build a KOSHIK platform with the relevant tools, and provides the steps to analyze wiki datalsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.456826jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1608.04434v1¨bidh404e0887gconceptlare utilizedjdefinitionxKTraditional and data-driven methods are utilized in the preprocessing stagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.578531jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2503.02435v1¨bidh08ba6e08gconceptdrulejdefinitionx¡Traditional approaches rely on predefined rules and grammars, and involve techniques such as regular expressions, dependency parsing and named entity recognitionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.580814jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2503.02435v1¨bidh5b87554bgconceptgsystemsjdefinitionyLFinally, we summarize the classification, development, and enhancement of NLIDB systems, and discuss deep language understanding and database interaction techniques related to NLIDB, including (i) using LLM for Text2SQL tasks, (ii) generating natural language interpretations from SQL, and (iii) transforming speech queries into SQLlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.581114jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2503.02435v1¨bidhba46228egconceptgdevotedjdefinitionx³As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.581288jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2503.02435v1¨bidhc0177ce3gconceptqunderstanding andjdefinitionyLFinally, we summarize the classification, development, and enhancement of NLIDB systems, and discuss deep language understanding and database interaction techniques related to NLIDB, including (i) using LLM for Text2SQL tasks, (ii) generating natural language interpretations from SQL, and (iii) transforming speech queries into SQLlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.581506jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2503.02435v1¨bidh37279354gconceptrentity recognitionjdefinitionx¡Traditional approaches rely on predefined rules and grammars, and involve techniques such as regular expressions, dependency parsing and named entity recognitionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.581693jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2503.02435v1¨bidh2dff8b9agconceptidepend onjdefinitionxŒData-driven approaches depend on large-scale data and machine learning models, using techniques including word embedding and pattern linkinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.581871jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2503.02435v1¨bidhb5451537gconceptkrecognitionjdefinitionx¡Traditional approaches rely on predefined rules and grammars, and involve techniques such as regular expressions, dependency parsing and named entity recognitionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.582036jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2503.02435v1¨bidhc77a4337gconceptdoverjdefinitiony/We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation process from natural language to executable database languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.588402jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2503.02435v1¨bidh137bc357gconceptgmachinejdefinitionxŒData-driven approaches depend on large-scale data and machine learning models, using techniques including word embedding and pattern linkinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.588770jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2503.02435v1¨bidh67a2ddc2gconceptmpreconditionsjdefinitionxO, preconditions and effects of action models, and learn from tacit knowledge, elsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.708346jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.07138v2¨bidh10f0102egconceptiintroducejdefinitionx…One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.708566jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.07138v2¨bidhcb1eb5aegconceptnthe challengesjdefinitionxlHowever, the challenges of explainability and complexity come along with the developments of language modelslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.708708jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.07138v2¨bidhe3df7224gconceptlrespectivelyjdefinitionx, neural models, respectivelylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.709015jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.07138v2¨bidha3344536gconceptianalyzingjdefinitionx£Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language datalsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.709171jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.07138v2¨bidhfeb92a0fgconceptjfirst workjdefinitionx“To the best of our knowledge, this survey is the first work that addresses the deep connections between AI planning and Natural language processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.709376jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.07138v2¨bidhac9d3813gconceptkcommons andjdefinitionyiThis paper outlines the commons and relations between AI planning and natural language processing, argues that each of them can effectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based natural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and (5) applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.709552jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.07138v2¨bidh2ef27601gconceptkargues thatjdefinitionyiThis paper outlines the commons and relations between AI planning and natural language processing, argues that each of them can effectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based natural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and (5) applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.709708jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.07138v2¨bidh9881b0bbgconceptjthis paperjdefinitionyiThis paper outlines the commons and relations between AI planning and natural language processing, argues that each of them can effectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based natural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and (5) applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.709866jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2202.07138v2¨bidh895ef5cfgconcepthlanguagejdefinitionx£Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language datalsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.709994jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2202.07138v2¨bidh138c7df0gconceptnnote describesjdefinitionx]This technical note describes a set of baseline tools for automatic processing of Danish textlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.812404jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1906.11608v2¨bidh50ef48e7gconceptjcopenhagenjdefinitionxIThey are maintained at ITU Copenhagen and will always be freely availablelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.812497jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1906.11608v2¨bidhaccc5cc4gconcepthand willjdefinitionxIThey are maintained at ITU Copenhagen and will always be freely availablelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.812543jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1906.11608v2¨bidh03e66adbgconceptdoverjdefinitionxzThe tools are machine-learning based, using natural language processing models trained over previously annotated documentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.812584jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1906.11608v2¨bidhb8691112gconceptpfreely availablejdefinitionxIThey are maintained at ITU Copenhagen and will always be freely availablelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.812625jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1906.11608v2¨bidh04b84b05gconceptgmachinejdefinitionxzThe tools are machine-learning based, using natural language processing models trained over previously annotated documentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.812666jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1906.11608v2¨bidh5acb2fb8gconceptfset ofjdefinitionx]This technical note describes a set of baseline tools for automatic processing of Danish textlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.812707jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1906.11608v2¨bidh5804b202gconceptdtextjdefinitionx]This technical note describes a set of baseline tools for automatic processing of Danish textlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.812758jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1906.11608v2¨bidh27f2c100gconceptdtheyjdefinitionxIThey are maintained at ITU Copenhagen and will always be freely availablelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.812830jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1906.11608v2¨bidh3f2b5ea5gconcepthlanguagejdefinitionxzThe tools are machine-learning based, using natural language processing models trained over previously annotated documentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.812993jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1906.11608v2¨bidha1c210b8gconcepteis nojdefinitionxSome NLG researchers exclude MT from their definition of the field, since there is no content selection involved where the system has to determine what to saylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.914449jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2503.16728v2¨bidhe085ea17gconceptgsystemsjdefinitionx§The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.914566jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2503.16728v2¨bidhe6ec73begconceptdfalljdefinitionxÚConversely, dialog systems do not typically fall under the header of Natural Language Generation since NLG is just one component of dialog systems (the others being Natural Language Understanding and Dialog Management)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.914621jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2503.16728v2¨bidhd81c4c9cgconceptsdifferent subfieldsjdefinitionxìHowever, with the rise of Large Language Models (LLMs), different subfields of Natural Language Processing have converged on similar methodologies for the production of natural language and the evaluation of automatically generated textlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.914671jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2503.16728v2¨bidhff6c21e0gconceptllanguage andjdefinitionxìHowever, with the rise of Large Language Models (LLMs), different subfields of Natural Language Processing have converged on similar methodologies for the production of natural language and the evaluation of automatically generated textlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.914720jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2503.16728v2¨bidh26372c99gconceptjof systemsjdefinitionx§The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.914775jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2503.16728v2¨bidhe16d5707gconceptfstoredjdefinitionxØThat information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for examplelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.914821jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2503.16728v2¨bidh670054b1gconceptfto sayjdefinitionxSome NLG researchers exclude MT from their definition of the field, since there is no content selection involved where the system has to determine what to saylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.914884jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2503.16728v2¨bidh6dc0ca79gconceptgmachinejdefinitionx‘As a subfield of Natural Language Processing, NLG is closely related to other sub-disciplines such as Machine Translation (MT) and Dialog Systemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.914930jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2503.16728v2¨bidhba4be1f8gconceptiunder thejdefinitionxÚConversely, dialog systems do not typically fall under the header of Natural Language Generation since NLG is just one component of dialog systems (the others being Natural Language Understanding and Dialog Management)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:43.914983jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2503.16728v2¨bidh813ef624gconcepteno orjdefinitionxNThere is no or little work on natural language processing of Tangkhul languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.016180jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2006.16212v1¨bidh957b1318gconceptkthe currentjdefinitionxrThe current work is a humble beginning of morphological processing of this language using an unsupervised approachlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.016337jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2006.16212v1¨bidh0c65aa21gconceptfof thejdefinitionxNThere is no or little work on natural language processing of Tangkhul languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.016430jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2006.16212v1¨bidh599b2b7dgconceptnoutput despitejdefinitionxœBased on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpuslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.016501jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2006.16212v1¨bidh9608fb72gconcepthlanguagejdefinitionxNThere is no or little work on natural language processing of Tangkhul languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.016554jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2006.16212v1¨bidh67d77336gconceptqdifferent sourcesjdefinitionxpWe use a small corpus collected from different sources of text books, short stories and articles of other topicslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.016612jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2006.16212v1¨bidh4521b73egconcepthstudy ofjdefinitionxNThere is no or little work on natural language processing of Tangkhul languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.016668jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2006.16212v1¨bidhe29bd079gconceptpof morphologicaljdefinitionxrThe current work is a humble beginning of morphological processing of this language using an unsupervised approachlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.016722jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2006.16212v1¨bidhd974f750gconceptnidentificationjdefinitionxœBased on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpuslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.016772jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2006.16212v1¨bidh39eed378gconceptgdespitejdefinitionxœBased on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpuslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.016825jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2006.16212v1¨bidh8d43542fgconceptiprimer onjdefinitionx¹Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.118372jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1510.00726v1¨bidh53b4e7d6gconceptmbring naturaljdefinitionxÄThis tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniqueslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.118552jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1510.00726v1¨bidhdea3041agconcepthresearchjdefinitionxÄThis tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniqueslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.118613jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1510.00726v1¨bidh90b00d04gconceptdoverjdefinitionx¹Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.118662jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1510.00726v1¨bidh3d99813egconceptkrecognitionjdefinitionx¹Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.118708jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1510.00726v1¨bidh495a3476gconcepthstate-ofjdefinitionx¹Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.118756jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1510.00726v1¨bidhe3df0a8dgconceptgmachinejdefinitionx¹Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.118802jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1510.00726v1¨bidhe39de6ffgconceptjand speechjdefinitionx¹Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.118839jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1510.00726v1¨bidhd34d2c69gconceptgtextualjdefinitionx†More recently, neural network models started to be applied also to textual natural language signals, again with very promising resultslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.118878jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1510.00726v1¨bidh598bbf5bgconcepthpast fewjdefinitionx¹Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.118915jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1510.00726v1¨bidh491b79e6gconceptnfundamental tojdefinitionxÂOn the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understandinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.222628jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1511.07916v1¨bidh58b9e1a4gconceptdfalljdefinitionx´This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.222813jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1511.07916v1¨bidh89dcd735gconceptjpersonallyjdefinitionxÂOn the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understandinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.222888jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1511.07916v1¨bidhf4fdcb8cgconceptnlanguage frontjdefinitionxÂOn the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understandinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.222955jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1511.07916v1¨bidhf0e8c2ffgconceptgmachinejdefinitionxÉIn order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introducedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.223014jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1511.07916v1¨bidh88b73f2agconceptdtheyjdefinitionxÉIn order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introducedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.223074jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1511.07916v1¨bidhc46fd598gconcepthlanguagejdefinitionx´This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.223131jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1511.07916v1¨bidh2df21fafgconceptglecturejdefinitionx´This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.223216jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1511.07916v1¨bidhc4f8ff60gconceptrlanguage modellingjdefinitionxÂOn the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understandinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.223261jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1511.07916v1¨bidhfd43ee8agconceptpmost fascinatingjdefinitionxÂOn the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understandinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.223318jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1511.07916v1¨bidh5d88992agconceptllinguists tojdefinitionyComputer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.324397jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1908.08971v2¨bidh5ec9d5a1gconceptonative speakersjdefinitionyComputer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.324618jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1908.08971v2¨bidhdfbb42degconceptjtechnologyjdefinitionyComputer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.324700jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1908.08971v2¨bidhc6c60fc5gconcepthand textjdefinitionyComputer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.324766jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1908.08971v2¨bidhbf949688gconcepthcomputerjdefinitionyComputer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.324823jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1908.08971v2¨bidhf74efb5cgconceptjespeciallyjdefinitionyComputer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.324880jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1908.08971v2¨bidh6a33c0d4gconceptfnativejdefinitionyComputer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.324935jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1908.08971v2¨bidh823e913dgconceptkrecognitionjdefinitionyComputer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.325004jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1908.08971v2¨bidh27dcbbacgconceptideployingjdefinitionyComputer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.325069jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1908.08971v2¨bidhfba7760bgconceptdtextjdefinitionyComputer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.325177jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1908.08971v2¨bidhcbdccf2egconceptuprocessing downstreamjdefinitionxÃWe suggest several ways of supporting undotted texts with pre-trained models, without additional training, and measure their performance on two Arabic natural-language-processing downstream taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.427858jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2111.09791v1¨bidh9c95a279gconceptgwithoutjdefinitionxÃWe suggest several ways of supporting undotted texts with pre-trained models, without additional training, and measure their performance on two Arabic natural-language-processing downstream taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.428012jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2111.09791v1¨bidh0303792egconceptfin onejdefinitionx\The results are encouraging; in one of the tasks our method shows nearly perfect performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.428081jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2111.09791v1¨bidh6a7db2acgconceptvcontent-classificationjdefinitionx­We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.428152jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2111.09791v1¨bidh6840b07dgconceptfrecentjdefinitionx­We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.428201jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2111.09791v1¨bidha3fa6f57gconceptfof thejdefinitionx\The results are encouraging; in one of the tasks our method shows nearly perfect performancelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.428373jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2111.09791v1¨bidh30012412gconceptdfinejdefinitionx§Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.428502jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2111.09791v1¨bidha3525187gconcepthlanguagejdefinitionx§Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.428551jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2111.09791v1¨bidh46e1cf2agconceptnarabic naturaljdefinitionxÃWe suggest several ways of supporting undotted texts with pre-trained models, without additional training, and measure their performance on two Arabic natural-language-processing downstream taskslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.428651jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2111.09791v1¨bidhb1a27f25gconceptfsocialjdefinitionx­We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.428703jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2111.09791v1¨bidhc8278d88gconcepthresearchjdefinitionxAs the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.531748jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2112.01705v1¨bidh7dabf220gconceptdhandjdefinitionxQOn the one hand, the framework used the LaBSE pre-trained model as the base modellsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.532033jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2112.01705v1¨bidh03f0f7c6gconceptllarge numberjdefinitionxyAlthough the Dravidian languages contain a large number of languages, there are relatively few public available resourceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.532117jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2112.01705v1¨bidh21c7ff29gconceptestilljdefinitionxÛBesides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.532169jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2112.01705v1¨bidh1df3e070gconceptfof thejdefinitionxæOn the other hand, in view of the problem that the model cannot well recognize and utilize the correlation among languages, we further proposed a language-specific representation module to enrich semantic information for the modellsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.532272jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2112.01705v1¨bidh17042795gconceptqcorrelation amongjdefinitionxæOn the other hand, in view of the problem that the model cannot well recognize and utilize the correlation among languages, we further proposed a language-specific representation module to enrich semantic information for the modellsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.532678jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2112.01705v1¨bidhf9cc1b14gconcepthlanguagejdefinitionxAs the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.532845jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2112.01705v1¨bidh7a1a5748gconceptldemonstratedjdefinitionx¾The experimental results demonstrated that the framework we proposed has a significant performance in multilingual text classification tasks with each strategy achieving certain improvementslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.532912jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2112.01705v1¨bidh3df81c63gconceptgperturbjdefinitionxºAiming at the problem of text information bias in multi-task learning, we propose to use the MLM strategy to select language-specific words, and used adversarial training to perturb themlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.532958jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2112.01705v1¨bidh9bed2fedgconceptetasksjdefinitionx¾The experimental results demonstrated that the framework we proposed has a significant performance in multilingual text classification tasks with each strategy achieving certain improvementslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.533020jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2112.01705v1¨bidh1da9cc09gconceptithe widerjdefinitionx¨The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognitionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.634326jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2205.07634v1¨bidh883b1fcbgconceptqour understandingjdefinitionx¨The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognitionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.634482jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2205.07634v1¨bidh28b0c149gconceptqartificial neuraljdefinitionx£Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.634534jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2205.07634v1¨bidhb5a69d43gconceptiprecis ofjdefinitionx£Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.634591jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2205.07634v1¨bidhff54ec01gconceptkspearheadedjdefinitionx£Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.634660jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2205.07634v1¨bidh6289c6f2gconceptlsuccesses atjdefinitionx¡We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.634737jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2205.07634v1¨bidh783419a0gconceptmnot representjdefinitionx¨The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognitionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.634805jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2205.07634v1¨bidh58c55881gconceptkapplicationjdefinitionx£Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.634863jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2205.07634v1¨bidh4d8e67dcgconceptnspearheaded byjdefinitionx£Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.634929jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2205.07634v1¨bidhadbfd692gconceptuperforming linguisticjdefinitionx¡We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.634977jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2205.07634v1¨bidh2a6036aegconcepthlanguagejdefinitionyModel-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of modelslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.736559jconfidenceû?îffffffnparent_sourcesx!https://arxiv.org/abs/1107.4687v2¨bidh14d28a92gconceptjup parsingjdefinitionx½In this paper, we propose Fence, an efficient bottom-up parsing algorithm with lexical and syntactic ambiguity support that enables the use of model-based language specification in practicelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.736728jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1107.4687v2¨bidh94b99976gconceptngeneral parserjdefinitionyModel-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguitieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.736792jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1107.4687v2¨bidh1bfcc978gconceptogenerators ablejdefinitionyModel-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguitieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.736846jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1107.4687v2¨bidhffe40b12gconceptrspecific languagesjdefinitionyModel-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of modelslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.736896jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1107.4687v2¨bidh91a52382gconceptfuse ofjdefinitionx½In this paper, we propose Fence, an efficient bottom-up parsing algorithm with lexical and syntactic ambiguity support that enables the use of model-based language specification in practicelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.736953jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1107.4687v2¨bidha7966fabgconceptkmodel-basedjdefinitionyModel-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of modelslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.737002jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1107.4687v2¨bidha365195bgconceptekindsjdefinitionyModel-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguitieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.737053jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1107.4687v2¨bidha0d7cf9fgconceptowhich constrainjdefinitionyModel-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguitieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.737109jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1107.4687v2¨bidhf8aac569gconcepthit needsjdefinitionyModel-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguitieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.737206jconfidenceû?ë333333nparent_sourcesx!https://arxiv.org/abs/1107.4687v2¨bidh4d8b84e7gconceptjto achievejdefinitionx³Quantum computers offer the best chance to achieve translation fluency in that they are better suited to simulating the natural world and natural phenomenon such as natural speechlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.839714jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.04451v1¨bidh161aa9c8gconceptsgeographic locationjdefinitionx¸Why is this endeavor important? Hundreds of languages have developed over the course of millennia coinciding with the evolution of human interaction across time and geographic locationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.839890jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.04451v1¨bidh481971cbgconcepthresearchjdefinitionx—The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of researchlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.839988jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.04451v1¨bidh9c0a1b96gconceptdoverjdefinitionx¸Why is this endeavor important? Hundreds of languages have developed over the course of millennia coinciding with the evolution of human interaction across time and geographic locationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.840063jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.04451v1¨bidh55f2bd1fgconceptkthe currentjdefinitionx—The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of researchlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.840115jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.04451v1¨bidh3e01540bgconceptdthanjdefinitionxsTools like Google Translate and DeepL make it easier than ever before to share our experiences with people globallylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.840172jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.04451v1¨bidh0d989f8agconceptpfurther researchjdefinitionxsAdditionally, topological principles of these diagrams and many potential avenues for further research are proposedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.842572jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.04451v1¨bidh3f343af2gconceptdtheyjdefinitionxCThey are also, however, the strongest barrier between people groupslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.842869jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.04451v1¨bidhc835a751gconceptestilljdefinitionx¤Nevertheless, these tools are still inadequate as they fail to convey our ideas across the language barrier fluently, leaving people feeling anxious and embarrassedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.842977jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.04451v1¨bidh13b4c3d9gconceptfof thejdefinitionx«This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theorylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.843040jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.04451v1¨bidhcce9fa28gconceptpincluding signedjdefinitionxeSigned languages are the primary means of communication for many deaf and hard of hearing individualslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.944903jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2105.05222v2¨bidh0cb81937gconcepthresearchjdefinitionx“However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.945014jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2105.05222v2¨bidh0363b32egconceptkcommunitiesjdefinitiony'Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of researchlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.945070jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2105.05222v2¨bidh049c9279gconceptecallsjdefinitionx„This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impactlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.945116jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2105.05222v2¨bidh944d5cfdgconcepthadoptionjdefinitiony'Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of researchlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.945178jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2105.05222v2¨bidha7c32103gconceptlworld signedjdefinitiony'Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of researchlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.945234jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2105.05222v2¨bidh6aa6692egconceptkof researchjdefinitiony'Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of researchlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.945281jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2105.05222v2¨bidhf19650ccgconceptfsignedjdefinitionxeSigned languages are the primary means of communication for many deaf and hard of hearing individualslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.945324jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2105.05222v2¨bidhe3738f37gconcepthlanguagejdefinitionxeSigned languages are the primary means of communication for many deaf and hard of hearing individualslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.945375jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2105.05222v2¨bidhd5811d80gconceptfsocialjdefinitionx„This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impactlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:44.945425jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2105.05222v2¨bidh8714cafcgconcepthresearchjdefinitionx\Evaluation in natural language processing guides and promotes research on models and methodslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.046692jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2104.09712v1¨bidhb860326agconceptgmachinejdefinitionyQFinally, this article refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability evaluation, and proposes a series of basic principles and implementation ideas for hu-man-like machine language ability evaluation from the three aspects of reliability, difficulty and validitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.046820jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2104.09712v1¨bidhbbd09f76gconceptfrecentjdefinitionx_In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.046881jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2104.09712v1¨bidhb4af3b05gconceptland promotesjdefinitionx\Evaluation in natural language processing guides and promotes research on models and methodslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.046913jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2104.09712v1¨bidhd650c64agconcepthlanguagejdefinitionx\Evaluation in natural language processing guides and promotes research on models and methodslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.046938jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2104.09712v1¨bidh146b755dgconceptetasksjdefinitionx_In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.046981jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2104.09712v1¨bidhd7060508gconceptgconceptjdefinitiony(Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteristics of mainstream natural language evaluation, and then summarizes the problems and causes of natural language pro-cessing evaluationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.047008jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2104.09712v1¨bidhefe838ffgconceptplanguage abilityjdefinitionyQFinally, this article refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability evaluation, and proposes a series of basic principles and implementation ideas for hu-man-like machine language ability evaluation from the three aspects of reliability, difficulty and validitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.047035jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2104.09712v1¨bidh0efb8905gconceptjhuman-likejdefinitionyQFinally, this article refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability evaluation, and proposes a series of basic principles and implementation ideas for hu-man-like machine language ability evaluation from the three aspects of reliability, difficulty and validitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.047072jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2104.09712v1¨bidh3746d02bgconceptdputsjdefinitionyQFinally, this article refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability evaluation, and proposes a series of basic principles and implementation ideas for hu-man-like machine language ability evaluation from the three aspects of reliability, difficulty and validitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.047110jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2104.09712v1¨bidhdaa79d1bgconceptdusesjdefinitionx‚Specifications in controlled natural language are automatically translated into Prolog clauses, hence become formal and executablelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.148428jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/9507009v1¨bidh59c212c3gconceptothe applicationjdefinitionx±Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software developmentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.148495jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/9507009v1¨bidheccb9458gconceptlprototypicaljdefinitionxŠWe have implemented a prototypical specification system that successfully processes the specification of a simple automated teller machinelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.148541jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/9507009v1¨bidhaa8957e9gconceptqspecifications injdefinitionx‚Specifications in controlled natural language are automatically translated into Prolog clauses, hence become formal and executablelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.148582jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/9507009v1¨bidhd8dcf7ffgconceptidisparatejdefinitionx±Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software developmentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.148620jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/9507009v1¨bidh0532ccfcgconceptitake intojdefinitionx±Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software developmentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.148661jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/9507009v1¨bidhd4a6deb5gconceptelogicjdefinitionx±Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software developmentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.148702jconfidenceû?îffffffnparent_sourcesxhttps://arxiv.org/abs/9507009v1¨bidh80e1cd3agconceptkapplicationjdefinitionx±Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software developmentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.148738jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/9507009v1¨bidhf96d5ed1gconceptfworldsjdefinitionx±Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software developmentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.148772jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/9507009v1¨bidh0e4328c6gconceptgmachinejdefinitionxŠWe have implemented a prototypical specification system that successfully processes the specification of a simple automated teller machinelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.148831jconfidenceû?é™™™™™™nparent_sourcesxhttps://arxiv.org/abs/9507009v1¨bidhcb186307gconceptfcan bejdefinitionx PersianLLaMA marks an important step in the development of Persian natural language processing and can be a valuable resource for the Persian-speaking communitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.253983jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.15713v1¨bidh107f7f5fgconcepthhardwarejdefinitionx¨The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resourceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.254270jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.15713v1¨bidhac4ef7d5gconceptooutperforms itsjdefinitionx‚The results indicate that PersianLLaMA significantly outperforms its competitors in both understanding and generating Persian textlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.254490jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.15713v1¨bidh817bb5b8gconceptjespeciallyjdefinitionx¾This large language model can be used for various natural language processing tasks, especially text generation like chatbots, question-answering, machine translation, and text summarizationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.254695jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.15713v1¨bidha4eaee2agconceptnbeen evaluatedjdefinitionxâPersianLLaMA has been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, and for natural language understanding tasks based on automated machine metricslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.254868jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.15713v1¨bidh17c88d25gconceptgtextualjdefinitionx¨The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resourceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.255020jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.15713v1¨bidh153ee5a5gconceptolarger languagejdefinitionxâPersianLLaMA has been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, and for natural language understanding tasks based on automated machine metricslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.255232jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.15713v1¨bidhb012e7d4gconceptxpersianllama significantlyjdefinitionx‚The results indicate that PersianLLaMA significantly outperforms its competitors in both understanding and generating Persian textlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.255665jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.15713v1¨bidh5abb8e9cgconceptgmachinejdefinitionxâPersianLLaMA has been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, and for natural language understanding tasks based on automated machine metricslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.255862jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.15713v1¨bidh35d66639gconceptfof thejdefinitionx˜Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.256008jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2312.15713v1¨bidh7f5c75f4gconceptmcontributionsjdefinitionxÁThus, the contributions of this paper are as follows: First, to our knowledge, it is the first attempt to propose a natural language interface to graph-based bibliographic information retrievallsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.364308jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1612.03231v1¨bidh89a6fccagconceptgsystemsjdefinitionxºWith the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectivelylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.364551jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1612.03231v1¨bidhef73584bgconceptganalyzejdefinitionx³A series of text- and linguistic-based techniques are used to analyze and answer natural language queries, including tokenization, named entity recognition, and syntactic analysislsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.364722jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1612.03231v1¨bidhfbe1b1b9gconceptkrecognitionjdefinitionx³A series of text- and linguistic-based techniques are used to analyze and answer natural language queries, including tokenization, named entity recognition, and syntactic analysislsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.364885jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1612.03231v1¨bidh75a77d7agconceptuintegrates algorithmsjdefinitionxtOur framework integrates algorithms/heuristics for interpreting and analyzing natural language bibliographic querieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.365070jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1612.03231v1¨bidh53a64e89gconceptipresentedjdefinitionx Our experimental results show that the presented system can correctly answer 39 out of 40 example natural language queries with varying lengths and complexitieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.365256jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1612.03231v1¨bidhcbfe6a59gconceptianalyzingjdefinitionxtOur framework integrates algorithms/heuristics for interpreting and analyzing natural language bibliographic querieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.365459jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1612.03231v1¨bidh01bb9a3cgconceptgthroughjdefinitionx]NLI-GIBIR allows users to search for a variety of bibliographic data through natural languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.365640jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1612.03231v1¨bidhfb13c213gconceptgprovidejdefinitionxÊThird, we show that the proposed framework and natural language interface provide a practical solution in building real-world natural language interface-based bibliographic information retrieval systemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.365815jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1612.03231v1¨bidh97d491a7gconceptpour experimentaljdefinitionx Our experimental results show that the presented system can correctly answer 39 out of 40 example natural language queries with varying lengths and complexitieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.365997jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1612.03231v1¨bidh13df8833gconceptfcan bejdefinitionx«We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during traininglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.467184jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1612.07486v2¨bidh17700bafgconceptocharacter-basedjdefinitionx«We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during traininglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.467280jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1612.07486v2¨bidh58b8dbb5gconceptjthat thesejdefinitionx«We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during traininglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.467330jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1612.07486v2¨bidh7229dcd3gconceptdseenjdefinitionx«We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during traininglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.467374jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1612.07486v2¨bidh7a647264gconceptgimprovejdefinitionx«We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during traininglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.467406jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1612.07486v2¨bidhbc0aac03gconcepthlanguagejdefinitionx¨Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the otherlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.467428jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1612.07486v2¨bidh1b7d167dgconceptovectors capturejdefinitionxæIn experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.467458jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1612.07486v2¨bidh209646eagconceptkempiricallyjdefinitionxæIn experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.467489jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1612.07486v2¨bidh80712b6dgconceptfvectorjdefinitionxKIn contrast, we propose using continuous vector representations of languagelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.467515jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1612.07486v2¨bidh1e056049gconceptlbased neuraljdefinitionx«We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during traininglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:45.467553jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1612.07486v2¨bidh0ff940f5gconcepthresearchjdefinitionx‰Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:46.952814jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1905.07844v1¨bidh0e4965c4gconceptqvisual impairmentjdefinitionxŠOne application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairmentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:46.952985jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1905.07844v1¨bidh4e417c0agconceptland autonomyjdefinitionx¬Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potential usefulness ariselsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:46.953061jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1905.07844v1¨bidh94cdee28gconceptdoverjdefinitionx¬Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potential usefulness ariselsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:46.953127jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1905.07844v1¨bidh10e1ddc2gconcepthmitigatejdefinitionyThis paper addresses the positive and negative implications computer vision based assistive technologies have on individuals with visual impairment, as well as considerations for computer vision researchers and developers in order to mitigate the amount of negative implicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:46.953191jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1905.07844v1¨bidhf1f0e02fgconceptkapplicationjdefinitionxŠOne application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairmentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:46.953251jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1905.07844v1¨bidhc84562c6gconceptkto mitigatejdefinitionyThis paper addresses the positive and negative implications computer vision based assistive technologies have on individuals with visual impairment, as well as considerations for computer vision researchers and developers in order to mitigate the amount of negative implicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:46.953317jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1905.07844v1¨bidhd5026261gconceptdseenjdefinitionxŠOne application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairmentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:46.953398jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1905.07844v1¨bidh09dc1198gconcepthpositivejdefinitionyThis paper addresses the positive and negative implications computer vision based assistive technologies have on individuals with visual impairment, as well as considerations for computer vision researchers and developers in order to mitigate the amount of negative implicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:46.953463jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1905.07844v1¨bidh959dee39gconceptgprovidejdefinitionx‰Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:46.953519jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1905.07844v1¨bidha0f9103cgconceptjthe centerjdefinitionxbWorkshop was organized by the Center of Excellence for Computer Vision of the University of Zagreblsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.054538jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1310.0319v3¨bidh115e6e95gconcepthcomputerjdefinitionxRProceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://wwwlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.054672jconfidenceû?îffffffnparent_sourcesx!https://arxiv.org/abs/1310.0319v3¨bidh8b5be2f8gconceptjthe secondjdefinitionxRProceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://wwwlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.054741jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1310.0319v3¨bidh4ef744e5gconcepthworkshopjdefinitionxRProceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://wwwlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.054804jconfidenceû?îffffffnparent_sourcesx!https://arxiv.org/abs/1310.0319v3¨bidh369f54fegconceptdheldjdefinitionx<hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatialsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.054859jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1310.0319v3¨bidh2db17d28gconceptlorganized byjdefinitionxbWorkshop was organized by the Center of Excellence for Computer Vision of the University of Zagreblsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.054914jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1310.0319v3¨bidhaa960a61gconceptkproceedingsjdefinitionxRProceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://wwwlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.054964jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1310.0319v3¨bidhe839445agconceptmof excellencejdefinitionxbWorkshop was organized by the Center of Excellence for Computer Vision of the University of Zagreblsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.055013jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1310.0319v3¨bidh073ed21agconceptdhttpjdefinitionxRProceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://wwwlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.055064jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1310.0319v3¨bidhb0686c06gconceptiseptemberjdefinitionx<hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatialsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.055111jconfidenceû?é™™™™™™nparent_sourcesx!https://arxiv.org/abs/1310.0319v3¨bidhe4806c24gconceptjto achievejdefinitionxoIn this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughputlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.156018jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.03720v1¨bidh8bc059e6gconceptohigh-throughputjdefinitionx2Vision sensors lie in the heart of computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.156118jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1707.03720v1¨bidh058dd09bgconcepthrequiredjdefinitionx©In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.156166jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.03720v1¨bidh881c2414gconceptiwith highjdefinitionx©In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.156220jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.03720v1¨bidh43b833a6gconceptgsuch asjdefinitionx©In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.156266jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.03720v1¨bidh211ba7ffgconcepthcomputerjdefinitionx2Vision sensors lie in the heart of computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.156311jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1707.03720v1¨bidh66dfc872gconceptlapplicationsjdefinitionx©In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.156370jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.03720v1¨bidh62f644a4gconcepthmultiplejdefinitionxoIn this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughputlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.156417jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.03720v1¨bidh8b8b2629gconceptmto algorithmsjdefinitionx©In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.156458jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.03720v1¨bidha3a464d3gconceptithe heartjdefinitionx2Vision sensors lie in the heart of computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.156502jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1707.03720v1¨bidh6b095595gconceptfcan bejdefinitionxPThe paper will also explore how the two sides of computer vision can be combinedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.257934jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1910.13796v1¨bidh7b012cedgconceptjdiscussionjdefinitionx‚The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintainedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.258036jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1910.13796v1¨bidhf3035305gconceptfrecentjdefinitionx®Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.258079jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1910.13796v1¨bidha038b940gconceptgimprovejdefinitionx®Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.258118jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1910.13796v1¨bidh41e06c33gconceptjthis paperjdefinitionxCThis paper will analyse the benefits and drawbacks of each approachlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.258166jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1910.13796v1¨bidh7d177519gconceptgthe aimjdefinitionx‚The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintainedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.258205jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1910.13796v1¨bidhdf372fcagconceptldemonstratedjdefinitionx®Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.258241jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1910.13796v1¨bidhe585acd5gconceptddeepjdefinitionxbDeep Learning has pushed the limits of what was possible in the domain of Digital Image Processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.258271jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1910.13796v1¨bidhde16f7dcgconceptgof whatjdefinitionxbDeep Learning has pushed the limits of what was possible in the domain of Digital Image Processinglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.258300jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1910.13796v1¨bidh7cc60f7bgconceptlthe benefitsjdefinitionxCThis paper will analyse the benefits and drawbacks of each approachlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.258344jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1910.13796v1¨bidhdefbc6b3gconceptolive monitoringjdefinitionyE$\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus on computer vision as a solution for two common surveillance camera tasks (live monitoring of multiple surveillance cameras and summarizing archived video files)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.361184jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1808.03998v1¨bidh530f5861gconceptivision orjdefinitionxJThere is little knowledge of computer vision or its potential in the fieldlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.361345jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1808.03998v1¨bidh835d1e43gconcepthresearchjdefinitionxsAn ongoing research project on the application of computer vision within a municipal police department is describedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.361435jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1808.03998v1¨bidhfb98d47fgconceptfwithinjdefinitionxsAn ongoing research project on the application of computer vision within a municipal police department is describedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.361492jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1808.03998v1¨bidh938f3369gconceptgsystemsjdefinitionyTThree unaddressed research questions (can specialized computer vision applications for law enforcement be developed at this time, how will computer vision be utilized within existing public safety camera monitoring rooms, and what are the system-wide impacts of a computer vision capability on local criminal justice systems) are consideredlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.361553jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1808.03998v1¨bidhcde5c869gconceptkapplicationjdefinitionxsAn ongoing research project on the application of computer vision within a municipal police department is describedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.361609jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1808.03998v1¨bidheb969e14gconceptpresearch projectjdefinitionxsAn ongoing research project on the application of computer vision within a municipal police department is describedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.361665jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1808.03998v1¨bidh2d50dba7gconceptoperspective andjdefinitionxÿ$\mathbf{Originality/value}$ - This paper introduces and discusses computer vision from a law enforcement perspective and will be valuable to police personnel tasked with monitoring large camera networks and considering computer vision as a system upgradelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.361727jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1808.03998v1¨bidh67fb7906gconceptjfor policejdefinitionyE$\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus on computer vision as a solution for two common surveillance camera tasks (live monitoring of multiple surveillance cameras and summarizing archived video files)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.361778jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1808.03998v1¨bidh340027bfgconceptgongoingjdefinitionxsAn ongoing research project on the application of computer vision within a municipal police department is describedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.361832jconfidenceû?è      nparent_sourcesx"https://arxiv.org/abs/1808.03998v1¨bidh60f33b12gconceptnand challengesjdefinitionxJThe maritime environment offers its own unique requirements and challengeslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.463544jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1809.04659v2¨bidhc91d291dgconcepttdetection assessmentjdefinitionxzMaritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detectionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.463696jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1809.04659v2¨bidh3618c9f3gconceptkapplicationjdefinitionxfHowever, application of computer vision techniques in maritime domain received attention only recentlylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.463767jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1809.04659v2¨bidh486f7bc4gconceptrreceived attentionjdefinitionxfHowever, application of computer vision techniques in maritime domain received attention only recentlylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.463827jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1809.04659v2¨bidhbf6d7d26gconcepthmaritimejdefinitionxzMaritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detectionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.463877jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1809.04659v2¨bidhb9b57cfdgconceptlinapplicablejdefinitionxuThus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sightlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.463967jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1809.04659v2¨bidhed037549gconceptefirstjdefinitionxuThus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sightlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.464027jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1809.04659v2¨bidhd26f4438gconceptgwork injdefinitionxuThus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sightlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.464092jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1809.04659v2¨bidh54b1a15egconceptqmaritime computerjdefinitionx[We discuss the problem of defining assessment metrics suitable for maritime computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.464146jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1809.04659v2¨bidh8e846fd9gconceptfat thejdefinitionxuThus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sightlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.464196jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1809.04659v2¨bidh496cffb1gconceptiseptemberjdefinitionxvProceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.564814jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.07245v1¨bidh01dbdef3gconceptqinterpretable andjdefinitionxvProceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.564898jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1909.07245v1¨bidh9e430b70gconceptkexplainablejdefinitionxvProceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.564938jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1909.07245v1¨bidhe7144f12gconceptdbmvcjdefinitionxvProceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.564975jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1909.07245v1¨bidh4a06e728gconceptgcardiffjdefinitionxvProceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.565013jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.07245v1¨bidha0816546gconcepthworkshopjdefinitionxvProceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.565050jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1909.07245v1¨bidha487515agconcepththe bmvcjdefinitionxvProceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.565109jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.07245v1¨bidhf05d621cgconceptgmachinejdefinitionxvProceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.565146jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1909.07245v1¨bidh9286e22cgconceptnproceedings ofjdefinitionxvProceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.565182jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1909.07245v1¨bidh8b4cb08bgconceptfvisionjdefinitionxvProceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.565220jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1909.07245v1¨bidhb8692206gconceptfwithinjdefinitionx¤Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within imageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.667195jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2203.15269v1¨bidh4fdaebffgconceptlsegmentationjdefinitionyyWe surveyed the application of Vision transformers in different areas of medical computer vision such as image-based disease classification, anatomical structure segmentation, registration, region-based lesion Detection, captioning, report generation, reconstruction using multiple medical imaging modalities that greatly assist in medical diagnosis and hence treatment processlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.667299jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2203.15269v1¨bidh109f38efgconceptothe applicationjdefinitionyyWe surveyed the application of Vision transformers in different areas of medical computer vision such as image-based disease classification, anatomical structure segmentation, registration, region-based lesion Detection, captioning, report generation, reconstruction using multiple medical imaging modalities that greatly assist in medical diagnosis and hence treatment processlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.667376jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2203.15269v1¨bidh5383ed50gconceptlpotential tojdefinitionx¤Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within imageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.667432jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2203.15269v1¨bidh8f35967bgconceptjdiscussionjdefinitionx¢Conclusively, we also put some light on available data sets, adopted methodology, their performance measures, challenges and their solutions in form of discussionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.667484jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2203.15269v1¨bidh9c333356gconceptgin formjdefinitionx¢Conclusively, we also put some light on available data sets, adopted methodology, their performance measures, challenges and their solutions in form of discussionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.667549jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2203.15269v1¨bidhe1bf3449gconceptgis alsojdefinitionxzMoreover, to get more insight and deeper understanding, self-attention mechanism of transformers is also explained brieflylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.667605jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2203.15269v1¨bidh2b426f10gconceptoinvestigate thejdefinitionyHere, in this article we investigate the intersection of Vision Transformers and Medical images and proffered an overview of various ViTs based frameworks that are being used by different researchers in order to decipher the obstacles in Medical Computer Visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.667653jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2203.15269v1¨bidh91bd4f14gconceptqdemystify severaljdefinitionx]Along with this, we also demystify several imaging modalities used in Medical Computer Visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.667699jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2203.15269v1¨bidhc645ae53gconceptkapplicationjdefinitionyyWe surveyed the application of Vision transformers in different areas of medical computer vision such as image-based disease classification, anatomical structure segmentation, registration, region-based lesion Detection, captioning, report generation, reconstruction using multiple medical imaging modalities that greatly assist in medical diagnosis and hence treatment processlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.667743jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2203.15269v1¨bidhf535cdddgconceptxcorresponding contextualjdefinitionxˆThey fail to convey the fundamental multi-vision sensor information from the dataset and the corresponding contextual knowledge properlylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.774106jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2408.12114v3¨bidha25b7f7cgconceptpacross differentjdefinitionxùWe generated 6,248 vision-language test samples to investigate multi-vision sensory perception and multi-vision sensory reasoning on physical sensor knowledge proficiency across different formats, covering different types of sensor-related questionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.774444jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2408.12114v3¨bidh10410773gconceptgwithoutjdefinitionxÂHowever, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteristics of multi-vision sensorslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.774665jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2408.12114v3¨bidh51085d16gconceptgextentsjdefinitionxoThe results showed that most models displayed deficiencies in multi-vision sensory reasoning to varying extentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.774835jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2408.12114v3¨bidh06f608c3gconceptjlvlms viewjdefinitionxÂHowever, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteristics of multi-vision sensorslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.774976jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2408.12114v3¨bidh6561bee7gconceptdtheyjdefinitionxhThey have made remarkable progress in computer vision tasks by aligning text modality with vision inputslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.775107jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2408.12114v3¨bidhbafe38d5gconceptiendeavorsjdefinitionx{There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray imageslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.775250jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2408.12114v3¨bidheedb5711gconcepthlanguagejdefinitionxfLarge-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.775421jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2408.12114v3¨bidh425abf85gconceptesparkjdefinitionxÔIn this paper, we aim to establish a multi-vision Sensor Perception And Reasoning benchmarK called SPARK that can reduce the fundamental multi-vision sensor information gap between images and multi-vision sensorslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.775587jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2408.12114v3¨bidhce5a83e2gconceptetasksjdefinitionxhThey have made remarkable progress in computer vision tasks by aligning text modality with vision inputslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.775732jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2408.12114v3¨bidh72598145gconceptsprojection commonlyjdefinitionx”For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image sizelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.876704jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1907.09233v1¨bidh3762ca53gconceptnlevel overviewjdefinitionx¨In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional videolsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.876793jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1907.09233v1¨bidh068a260bgconceptgpopularjdefinitionxqOmnidirectional (360{\deg}) video has got quite popular because it provides a highly immersive viewing experiencelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.876834jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1907.09233v1¨bidh4be70d71gconceptopopular becausejdefinitionxqOmnidirectional (360{\deg}) video has got quite popular because it provides a highly immersive viewing experiencelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.876910jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1907.09233v1¨bidhb3f1add8gconceptlspecifics ofjdefinitionx¨In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional videolsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.876954jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1907.09233v1¨bidhc57c529agconceptfhighlyjdefinitionxqOmnidirectional (360{\deg}) video has got quite popular because it provides a highly immersive viewing experiencelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.876994jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1907.09233v1¨bidhafad4f0egconceptgseveraljdefinitionx”For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image sizelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.877033jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1907.09233v1¨bidh61df2fe7gconcepthcomputerjdefinitionx”For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image sizelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.877072jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1907.09233v1¨bidh6fee8f4bgconceptdsizejdefinitionx”For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image sizelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.877114jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1907.09233v1¨bidhb268e189gconcepthof thesejdefinitionx¨In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional videolsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:47.877156jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1907.09233v1¨bidh934fb1ffgconceptgsystemsjdefinitionx¦A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetimelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.622456jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2001.09608v1¨bidh6d1a4dbdgconceptmthrough trailjdefinitionx¦A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetimelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.622574jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2001.09608v1¨bidh653f474cgconceptmreinforcementjdefinitionx¦A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetimelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.622617jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2001.09608v1¨bidha532e475gconceptminto lifelongjdefinitionx‰Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning systemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.622666jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2001.09608v1¨bidh35f915fdgconceptothe traditionaljdefinitionxIn this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning systemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.622721jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2001.09608v1¨bidhd7ed6f65gconceptdoverjdefinitionx¦A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetimelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.622798jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2001.09608v1¨bidh7244990dgconceptolearning systemjdefinitionx¦A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetimelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.622840jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2001.09608v1¨bidh0ce57debgconceptmsome insightsjdefinitionx‰Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning systemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.622888jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2001.09608v1¨bidh9f2cc070gconceptelearnjdefinitionx¦A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetimelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.622973jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2001.09608v1¨bidha53c9272gconceptiprototypejdefinitionx‰Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning systemlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.623034jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2001.09608v1¨bidh0e067f88gconceptgsystemsjdefinitionxyWe devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety criticslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.724201jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2405.15430v1¨bidh636e8b65gconceptfwe mayjdefinitionxTo avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviourlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.724347jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2405.15430v1¨bidh4a7edc9agconceptmreinforcementjdefinitionx_Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraintslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.724437jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2405.15430v1¨bidh0ba89b96gconceptpof reinforcementjdefinitionx_Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraintslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.724491jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2405.15430v1¨bidhe0612696gconceptfcostlyjdefinitionxTo avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviourlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.724550jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2405.15430v1¨bidh6d10c22cgconceptgobviatejdefinitionxTo avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviourlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.724593jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2405.15430v1¨bidh1bcc35f0gconceptfdesirejdefinitionxTo avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviourlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.724638jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2405.15430v1¨bidh2e6a5e8dgconceptfunsafejdefinitionxTo avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviourlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.724683jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2405.15430v1¨bidhf71ebe09gconceptmguided repairjdefinitionxyWe devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety criticslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.724767jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2405.15430v1¨bidh4ea812f7gconceptrpreviously trainedjdefinitionxTo avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviourlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.724830jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2405.15430v1¨bidh9a7c347egconceptestartjdefinitionxrWe start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.826611jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidh8506ed54gconcepthresearchjdefinitionxIn this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.826659jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidhc23e83b9gconceptiand statejdefinitionxIn this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.826684jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidhc17ce4f2gconceptlsegmentationjdefinitionxÑ(i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.826710jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidhb267b716gconceptravailable datasetsjdefinitionx~Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.826737jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidhf652d1e1gconceptsresearch directionsjdefinitionx}Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.826765jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidh487d0ccfgconcepthstate-ofjdefinitionxIn this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.826789jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidh380c30c6gconcepteimagejdefinitionxÑ(i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.826814jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidh0b4df765gconceptfrecentjdefinitionx½Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer visionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.826836jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidh1b1a836egconcepthand deepjdefinitionxrWe start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.826859jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2108.11510v1¨bidh47a6a8d7gconcepththey mayjdefinitionxsThey may also face challenges in providing explanations for their decisions and generalizing the acquired knowledgelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.928561jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2307.01452v2¨bidhd1086a98gconceptracquired knowledgejdefinitionxsThey may also face challenges in providing explanations for their decisions and generalizing the acquired knowledgelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.928663jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2307.01452v2¨bidh2235e1a0gconceptressential paradigmjdefinitionxjReinforcement learning is an essential paradigm for solving sequential decision problems under uncertaintylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.928711jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2307.01452v2¨bidhc2718ccegconceptssequential decisionjdefinitionxjReinforcement learning is an essential paradigm for solving sequential decision problems under uncertaintylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.928757jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2307.01452v2¨bidh044e9e36gconceptiintroducejdefinitionx®We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.928806jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2307.01452v2¨bidhdb8987a0gconceptdtheyjdefinitionxsThey may also face challenges in providing explanations for their decisions and generalizing the acquired knowledgelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.928850jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2307.01452v2¨bidh4a22de62gconceptfrecentjdefinitionx…Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenginglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.928893jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2307.01452v2¨bidh35681036gconcepththe mainjdefinitionxÁOne of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.928936jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2307.01452v2¨bidh204b2b12gconceptlobstacles isjdefinitionxÁOne of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.928980jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2307.01452v2¨bidh2511a83bgconceptjreal worldjdefinitionx…Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenginglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:49.929023jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2307.01452v2¨bidh3d033625gconceptjto achievejdefinitiony~In this paper, we conclude the state of this exciting field, by comparing the classical distributed deep reinforcement learning methods, and studying important components to achieve efficient distributed learning, covering single player single agent distributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.032738jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2212.00253v1¨bidh77a3b01bgconceptkfurthermorejdefinitionx¬Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modifications of their non-distributed versionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.032886jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2212.00253v1¨bidha59fde98gconceptnand weaknessesjdefinitiony_By analyzing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex gameslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.032960jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2212.00253v1¨bidh26c0e830gconceptgwithoutjdefinitionx¬Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modifications of their non-distributed versionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.033017jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2212.00253v1¨bidh520f3969gconceptssequential decisionjdefinitionxŒWith the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.033071jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2212.00253v1¨bidha74273cfgconceptotheir strengthsjdefinitiony_By analyzing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex gameslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.033128jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2212.00253v1¨bidhc2e48647gconceptjin variousjdefinitionyOPlenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, distributed deep reinforcement learning has shown its potential in various applications, such as human-computer gaming, and intelligent transportationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.033182jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2212.00253v1¨bidh6c144108gconceptjcomponentsjdefinitiony~In this paper, we conclude the state of this exciting field, by comparing the classical distributed deep reinforcement learning methods, and studying important components to achieve efficient distributed learning, covering single player single agent distributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.033235jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2212.00253v1¨bidha3854511gconceptianalyzingjdefinitiony_By analyzing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex gameslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.033291jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2212.00253v1¨bidh09f97d11gconceptfof thejdefinitionx¬Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modifications of their non-distributed versionslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.033344jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2212.00253v1¨bidhf640e598gconcepthresearchjdefinitionxÃWe also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progresslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.136698jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2009.07888v7¨bidh38dcaf41gconceptqresearch progressjdefinitionxÃWe also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progresslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.136778jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2009.07888v7¨bidh3baa7af7gconceptganalyzejdefinitionxãSpecifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.136812jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2009.07888v7¨bidh12409f77gconceptssequential decisionjdefinitionx]Reinforcement learning is a learning paradigm for solving sequential decision-making problemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.136843jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2009.07888v7¨bidhcad7f0a6gconceptlprospects ofjdefinitionyGAlong with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning processlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.136873jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2009.07888v7¨bidh90fece65gconceptoinvestigate thejdefinitionxIn this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.136903jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2009.07888v7¨bidh803faac8gconcepthstate-ofjdefinitionxãSpecifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applicationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.136932jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2009.07888v7¨bidhe8465b04gconceptqwe systematicallyjdefinitionxIn this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.136960jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2009.07888v7¨bidh5c0689a6gconceptqlearning paradigmjdefinitionx]Reinforcement learning is a learning paradigm for solving sequential decision-making problemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.136987jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2009.07888v7¨bidhf0ad6acegconceptfrecentjdefinitionx{Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networkslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.137014jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2009.07888v7¨bidh461efd26gconceptmreinforcementjdefinitionxÇWe investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma gamelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.237915jconfidenceû?ìÌÌÌÌÌÌnparent_sourcesx"https://arxiv.org/abs/2108.03258v2¨bidh27b95fbfgconceptdbothjdefinitionxÇWe investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma gamelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.238007jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2108.03258v2¨bidh5ef9576fgconceptdgamejdefinitionxÇWe investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma gamelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.238054jconfidenceû?ìÌÌÌÌÌÌnparent_sourcesx"https://arxiv.org/abs/2108.03258v2¨bidhfb9d5633gconceptkinvestigatejdefinitionxÇWe investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma gamelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.238088jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2108.03258v2¨bidh60e4e415gconceptelearnjdefinitionxÇWe investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma gamelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.238124jconfidenceû?ìÌÌÌÌÌÌnparent_sourcesx"https://arxiv.org/abs/2108.03258v2¨bidh9795611egconceptjmemory-twojdefinitionxÇWe investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma gamelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.238160jconfidenceû?ìÌÌÌÌÌÌnparent_sourcesx"https://arxiv.org/abs/2108.03258v2¨bidh9d687590gconceptisymmetricjdefinitionxÇWe investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma gamelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.238196jconfidenceû?ìÌÌÌÌÌÌnparent_sourcesx"https://arxiv.org/abs/2108.03258v2¨bidh99f8eed4gconcepthrepeatedjdefinitionxÇWe investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma gamelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.238219jconfidenceû?ìÌÌÌÌÌÌnparent_sourcesx"https://arxiv.org/abs/2108.03258v2¨bidh4195fb8dgconceptilearn thejdefinitionxÇWe investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma gamelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.238238jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2108.03258v2¨bidh064771a1gconceptiof mutualjdefinitionxÇWe investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma gamelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.238269jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2108.03258v2¨bidh1200747dgconceptlthree-factorjdefinitionxjThe reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning ruleslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.339088jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2204.05437v1¨bidh834aa77bgconceptmreinforcementjdefinitionxŒA Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.339178jconfidenceû?ìÌÌÌÌÌÌnparent_sourcesx"https://arxiv.org/abs/2204.05437v1¨bidh01a89dcdgconceptgbackendjdefinitionx«The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.339209jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2204.05437v1¨bidh6921726dgconceptklearning isjdefinitionxŒA Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.339234jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2204.05437v1¨bidh3e0f6a81gconceptlbalancing anjdefinitionx„As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.339262jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2204.05437v1¨bidhbcf0fd1dgconceptdpolejdefinitionx„As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.339299jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2204.05437v1¨bidh8da951acgconceptiprototypejdefinitionx„As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.339331jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2204.05437v1¨bidh8876aeebgconceptolearning systemjdefinitionx«The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.339371jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2204.05437v1¨bidhe7576a17gconceptnclustering andjdefinitionx«The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.339409jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2204.05437v1¨bidhdc1c35d9gconcepticart-polejdefinitionx„As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulationlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.339435jconfidenceû?æffffffnparent_sourcesx"https://arxiv.org/abs/2204.05437v1¨bidh9ccc75bcgconceptgsystemsjdefinitionxCurrently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual worldlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.440651jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1709.05067v1¨bidh6d79a13cgconceptwartificial intelligencejdefinitionxPDeep reinforcement learning is revolutionizing the artificial intelligence fieldlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.440734jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1709.05067v1¨bidh4afd9db1gconceptqconversational aijdefinitionxKey challenges related to the implementation of reinforcement learning in conversational AI domain are identified as well as discussed in detaillsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.440778jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1709.05067v1¨bidhd9429537gconceptnand supervisedjdefinitionxÕIn this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement learning and supervised learning and models for implementation of reinforcement are discussedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.440824jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1709.05067v1¨bidhae3022cagconceptldiscussed injdefinitionxKey challenges related to the implementation of reinforcement learning in conversational AI domain are identified as well as discussed in detaillsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.440853jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1709.05067v1¨bidh7081a93dgconceptjin summaryjdefinitionxŠIn summary, this paper discusses key aspects of deep reinforcement learning which are crucial for designing an efficient conversational AIlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.440876jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1709.05067v1¨bidh8d2b900fgconceptfof thejdefinitionxCurrently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual worldlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.440897jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1709.05067v1¨bidh46181b2dgconceptjthis paperjdefinitionxÕIn this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement learning and supervised learning and models for implementation of reinforcement are discussedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.440934jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1709.05067v1¨bidh5b77bdc5gconceptddeepjdefinitionxPDeep reinforcement learning is revolutionizing the artificial intelligence fieldlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.440957jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1709.05067v1¨bidhdeabcb8fgconceptetasksjdefinitionx“It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video gameslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.440978jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1709.05067v1¨bidh47488521gconceptfwithinjdefinitionxReinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of latelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.545044jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2308.11336v1¨bidhf8cfe176gconceptkfurthermorejdefinitionxFurthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving fieldlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.545170jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2308.11336v1¨bidh8d08586agconceptgsystemsjdefinitionxReinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of latelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.545209jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2308.11336v1¨bidh693a400fgconcepthresearchjdefinitionxReinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of latelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.545246jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2308.11336v1¨bidha2875344gconceptiintroducejdefinitionxªThis survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domainlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.545277jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2308.11336v1¨bidh59a94040gconceptegivenjdefinitionx„Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlesslylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.545308jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2308.11336v1¨bidh3fec5ae7gconceptqpossess extensivejdefinitionx„Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlesslylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.545375jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2308.11336v1¨bidhc9ace212gconceptfrecentjdefinitionxJRecent strides in offline reinforcement learning present a new perspectivelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.545442jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2308.11336v1¨bidh81b56020gconceptqaligns seamlesslyjdefinitionx„Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlesslylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.545501jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2308.11336v1¨bidh36519736gconcepthdrawbackjdefinitionxhHowever, a significant drawback persists: its poor data efficiency, stemming from its interactive naturelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.545553jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2308.11336v1¨bidh32b37c90gconceptnsolve problemsjdefinitionx´Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.647306jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidh29f18fecgconceptkfurthermorejdefinitionx©Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.647391jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidh5261b8degconcepthresearchjdefinitionx´Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.647436jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidhd271790egconceptfwithinjdefinitionxÒFrom exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth viewlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.647482jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidhb1542f2bgconceptmformalize andjdefinitionxZIn this paper, we will formalize and analyze generalization in deep reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.647522jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidhbdf65304gconceptganalyzejdefinitionxZIn this paper, we will formalize and analyze generalization in deep reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.647557jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidhbece721agconceptmquestions thejdefinitionyWhile deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.647592jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidhe328c618gconceptestilljdefinitionyWhile deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.647630jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidheef24026gconceptianalyzingjdefinitionx´Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaceslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.647716jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidh290f9813gconceptgongoingjdefinitionyWhile deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policieslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.647844jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2401.02349v2¨bidh662d3c41gconceptjprocess ofjdefinitionx…We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \emph{any} algorithmlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.749753jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.11897v3¨bidh695404cbgconceptmimprove theirjdefinitionxcMachine learning algorithms learn to solve a task, but are unable to improve their ability to learnlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.749914jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.11897v3¨bidh3afcd975gconceptiintroducejdefinitionx¶To learn an effective teaching policy, we introduce the parametric-behavior embedder that learns a representation of the student's learnable parameters from its input/output behaviorlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.749952jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.11897v3¨bidhd0f779a7gconceptpheuristic rewardjdefinitionx—Reinforcement Teaching outperforms previous work using heuristic reward functions and state representations, as well as other parameter representationslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.749991jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.11897v3¨bidh2d0b0b94gconceptdhandjdefinitionxHowever, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.750023jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.11897v3¨bidh2d622b51gconceptgmachinejdefinitionxcMachine learning algorithms learn to solve a task, but are unable to improve their ability to learnlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.750050jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.11897v3¨bidh49d00757gconceptdtheyjdefinitionxnMeta-learning methods learn about machine learning algorithms and improve them so that they learn more quicklylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.750075jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.11897v3¨bidhb1168782gconceptndifferentiablejdefinitionxHowever, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.750105jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.11897v3¨bidhf96fe09egconceptdmorejdefinitionxnMeta-learning methods learn about machine learning algorithms and improve them so that they learn more quicklylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.750132jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.11897v3¨bidhabaafd78gconceptgimprovejdefinitionxcMachine learning algorithms learn to solve a task, but are unable to improve their ability to learnlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.750162jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2204.11897v3¨bidhdb859436gconcepthindirectjdefinitionx)This approach is indirect and can be slowlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.851918jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1606.03476v1¨bidha96c1b16gconceptgwithoutjdefinitionx~Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signallsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.852046jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1606.03476v1¨bidhaf083f8fgconceptdoverjdefinitionx¤One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.852114jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1606.03476v1¨bidh1dc4cbf4gconceptqfollowing inversejdefinitionx¨We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.852170jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1606.03476v1¨bidhbffef1b2gconceptoframework drawsjdefinitionyXWe show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environmentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.852220jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1606.03476v1¨bidh0395ff50gconceptnfree imitationjdefinitionyXWe show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environmentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.852272jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1606.03476v1¨bidh49f7859agconceptgrecoverjdefinitionx¤One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.852323jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1606.03476v1¨bidh6aba22c4gconceptjmodel-freejdefinitionyXWe show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environmentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.852400jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1606.03476v1¨bidhb5d7bf1egconceptjfrom whichjdefinitionyXWe show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environmentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.852636jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1606.03476v1¨bidh2f28f38cgconceptkrecover thejdefinitionx¤One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.852694jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1606.03476v1¨bidh4c7a5582gconceptjto combinejdefinitionxªMoreover, the proposed approach provides a general framework that can be used to combine any episodic memory agent with other off-policy reinforcement learning algorithmslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.955220jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.10098v2¨bidh18dc739egconceptgpart tojdefinitionxªThe 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each otherlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.955382jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.10098v2¨bidhc8101603gconceptgsystemsjdefinitionxnNotably, humans can leverage multiple memory systems concurrently during learning and benefit from all of themlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.955463jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.10098v2¨bidhd0ed4d4dgconceptdusesjdefinitionxÔNon-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selectionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.955495jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.10098v2¨bidh514d55f7gconceptoepisodic memoryjdefinitionxÔNon-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selectionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.955521jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.10098v2¨bidhe8c0f0a4gconceptnand weaknessesjdefinitionxWEpisodic memory and reinforcement learning both have their own strengths and weaknesseslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.955550jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.10098v2¨bidh44262d43gconceptmmethod calledjdefinitionx±In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengthslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.955579jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.10098v2¨bidh6e0b0826gconceptlstate-actionjdefinitionxÔNon-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selectionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.955607jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.10098v2¨bidh188ae81cgconcepthstate-ofjdefinitionxÇOur experiments demonstrate that the 2M agent is more data efficient and outperforms both pure episodic memory and pure reinforcement learning, as well as a state-of-the-art memory-augmented RL agentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.955635jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.10098v2¨bidh7b7396d3gconceptdhandjdefinitionxÔNon-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selectionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:50.955660jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/2304.10098v2¨bidh5782d109gconceptmimprove theirjdefinitionx¤At the same time, low-fitness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and improve their adaptabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.057567jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1912.06310v1¨bidh33ab9611gconceptgrecruitjdefinitionx»In this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning, a scalable framework that combines advantages of the three methods mentioned abovelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.057750jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1912.06310v1¨bidh9d2c6bb9gconceptiinstructsjdefinitionx˜This agent can recruit high-fitness actors from the population of evolutionary algorithms, which instructs itself to learn from experience replay bufferlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.057819jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1912.06310v1¨bidhe423fef8gconceptgimitatejdefinitionx¤At the same time, low-fitness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and improve their adaptabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.057875jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1912.06310v1¨bidhd6cf0336gconcepthpatternsjdefinitionx¤At the same time, low-fitness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and improve their adaptabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.057930jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1912.06310v1¨bidha943ae81gconceptdthanjdefinitionxıThe performance of RIM's components is significantly better than components of previous evolutionary reinforcement learning algorithm, and the recruitment using soft update enables reinforcement learning agent to learn faster than that using hard updatelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.057991jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1912.06310v1¨bidhe3545e5bgconceptfbufferjdefinitionx˜This agent can recruit high-fitness actors from the population of evolutionary algorithms, which instructs itself to learn from experience replay bufferlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.058058jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1912.06310v1¨bidhb557c1a4gconceptjcomponentsjdefinitionxıThe performance of RIM's components is significantly better than components of previous evolutionary reinforcement learning algorithm, and the recruitment using soft update enables reinforcement learning agent to learn faster than that using hard updatelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.058117jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1912.06310v1¨bidh2125cae0gconceptlpolicy actorjdefinitionxReinforcement and imitation learners in this framework can be replaced with any off-policy actor-critic reinforcement learner or data-driven imitation learnerlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.058179jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1912.06310v1¨bidhb976a02bgconceptfof thejdefinitionx»In this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning, a scalable framework that combines advantages of the three methods mentioned abovelsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.058247jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1912.06310v1¨bidh3cf692c9gconceptmreinforcementjdefinitionx|We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.159236jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2210.00770v1¨bidh1525f90dgconceptxaccelerate reinforcementjdefinitionx|We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.159433jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2210.00770v1¨bidhebf6c033gconceptjwe proposejdefinitionx|We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.159507jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.00770v1¨bidh4a36d6f6gconceptkcontrollersjdefinitionx|We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.159562jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2210.00770v1¨bidh3f0c13b7gconceptgproposejdefinitionx|We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.159617jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.00770v1¨bidh3519680bgconceptuproportional integraljdefinitionx|We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.159721jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.00770v1¨bidh68e97f36gconcepthintegraljdefinitionx|We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.159791jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.00770v1¨bidh41e06a55gconcepthlearningjdefinitionx|We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.159838jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2210.00770v1¨bidhad9eab3cgconcepthpendulumjdefinitionx|We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.159878jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2210.00770v1¨bidh6a7102f6gconcepthcoachingjdefinitionx|We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)lsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.159941jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2210.00770v1¨bidh8c66bce4gconceptkconvergencejdefinitionxàThis paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.261385jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2004.00993v2¨bidh6756e675gconceptjlearns viajdefinitionx«In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedbacklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.261483jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2004.00993v2¨bidhe02e8e20gconceptgmachinejdefinitionx«In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedbacklsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.261533jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2004.00993v2¨bidhf4e0f7cdgconceptdaqiljdefinitionx~The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.261599jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/2004.00993v2¨bidh90278f0bgconceptimethod byjdefinitionxàThis paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.261646jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2004.00993v2¨bidh5dd0c44agconceptjthis paperjdefinitionxàThis paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.261687jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2004.00993v2¨bidh18061b59gconceptnoptimal policyjdefinitionx{Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.261727jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2004.00993v2¨bidhf7012081gconcepteto anjdefinitionx{Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.261766jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2004.00993v2¨bidh49584a8egconceptddeepjdefinitionx{Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.261806jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2004.00993v2¨bidh7c01fb37gconceptpsignificant timejdefinitionx{Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.261860jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/2004.00993v2¨bidh147e6a31gconceptfhas onjdefinitionxWe finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attentionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.363243jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1706.05749v1¨bidh68839f1fgconceptdthanjdefinitionxWe show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environmentslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.363410jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1706.05749v1¨bidhace86ceegconceptjthis paperjdefinitionxÀThis paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.363482jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1706.05749v1¨bidhb7f958d3gconceptqnetwork attentionjdefinitionxWe finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attentionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.363531jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1706.05749v1¨bidha960f412gconceptddeepjdefinitionxÀThis paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.363589jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1706.05749v1¨bidh69e76036gconceptefirstjdefinitionxÒWe also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environmentlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.363651jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1706.05749v1¨bidhdbc2253agconceptgnetworkjdefinitionxWe finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attentionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.363699jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1706.05749v1¨bidh12ade612gconceptkqualitativejdefinitionxWe finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attentionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.363745jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1706.05749v1¨bidh2d0a581bgconceptkanalysis ofjdefinitionxWe finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attentionlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.363787jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1706.05749v1¨bidh0aebe6fegconceptgtoolkitjdefinitionxÀThis paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.363833jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1706.05749v1¨bidh9e7d5638gconceptmimprove theirjdefinitionxƒBoosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.465244jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1809.06995v1¨bidh55d03efagconceptgwithoutjdefinitionxƒBoosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.465412jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1809.06995v1¨bidh4978076cgconceptgmachinejdefinitionx°Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.465459jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1809.06995v1¨bidh5f7ed4a0gconceptpaccuracy withoutjdefinitionxƒBoosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.465492jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1809.06995v1¨bidh8c4abe12gconceptpour experimentaljdefinitionx©Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methodslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.465519jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1809.06995v1¨bidh5ac36d4agconcepthensemblejdefinitionxWe propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problemslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.465547jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1809.06995v1¨bidhfdec90f5gconceptgimprovejdefinitionxƒBoosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.465622jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1809.06995v1¨bidh0c3dce8bgconceptiand matchjdefinitionx©Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methodslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.465677jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1809.06995v1¨bidh2407a312gconceptxinterpretable reinforcementjdefinitionx°Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learninglsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.465704jconfidenceû?îffffffnparent_sourcesx"https://arxiv.org/abs/1809.06995v1¨bidh8be322a5gconceptvsignificantly reducingjdefinitionxƒBoosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretabilitylsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.465732jconfidenceû?é™™™™™™nparent_sourcesx"https://arxiv.org/abs/1809.06995v1¨bidh5cd3cec6gconceptfcan bejdefinitionxáOur conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learnerslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.568087jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1806.04640v3¨bidhad33428bgconceptfrecipejdefinitionx…We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approachlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.568220jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1806.04640v3¨bidh2e8eba7dgconceptmcontributionsjdefinitionxáOur conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learnerslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.568276jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1806.04640v3¨bidh1357a5aegconceptucontributions consistjdefinitionxáOur conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learnerslsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.568325jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1806.04640v3¨bidh0507ae7bgconceptpfor unsupervisedjdefinitionx…We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approachlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.568385jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1806.04640v3¨bidhb35df7bbgconceptidesign asjdefinitionxvIf we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automatedlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.568437jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1806.04640v3¨bidhf4cfbcb2gconcepthmotivatejdefinitionx…We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approachlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.568475jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1806.04640v3¨bidhcddbcbb5gconceptland describejdefinitionx…We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approachlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.568514jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1806.04640v3¨bidh376a003agconcepthacquiresjdefinitionxÿOur experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratchlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.568552jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1806.04640v3¨bidhbd847383gconceptgwithoutjdefinitionxÿOur experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratchlsource_agentrarxiv_collector_v1ksource_typeearxivitimestampx2025-08-15T21:55:51.568591jconfidenceû?ë333333nparent_sourcesx"https://arxiv.org/abs/1806.04640v3eindex¤pconcept_to_atoms¹Šxintelligence artificielleh36f5017cpmachine learningh9a0d07a4srÃ©seau de neuronesh572f6068uapprentissage profondh5e32af2eltransformersh5d0f7310jalgorithmeh70d5fb14fbuenoshe528b903dwell‚h20d0d6cch19497b53egiven†hac2db510h1ad60057hdc039dbeh1229c143h2961d877h59a94040gmachine˜&h570c3fc2h84c99158ha6369815h9eef4a92h9cd5598ah7ee52082h0dca6eb0h83561e8fhe679c479h5ab528f7h38979a72h51a5f919hb7f2c348hb2e4e5f9h82cb45fchcb10ed70h460616fchc3493209he39d4a52hfbcf10e8h2f26811fh4760a4eeh37d66c68hffa35be9h3ed922b6h73dac9ceh137bc357h04b84b05h6dc0ca79he3df0a8dhf0e8c2ffhb860326ah0e4328c6h5abb8e9chf05d621ch2d622b51he02e8e20h4978076ciprincetonh47bf5016icourse ath9102595fglecture‚hff0583d9h2df21fafgderivedhe0004a75hlearningƒha2eb1487hff679b88h41e06a55lbuenos aireshef25baacsadversarial machineh2d15e853nincluding testh11918eb5gdo harmhe4408f88omachine learnerhd38c7c8ehthe view‚h0e1334f3hfd284f3bditemha76329a2jencouragesh21284da8htypes ofh68cfa4cefsystem‚hb97130c8h0897cfb4fto the‚ha5dcfc32hc9a7bd23oand recognitionh7a9ba002klearning is‚h872244f6h6921726dgdevoted‚h5c8955e3hba46228ekand minimaxhd362dcfcnsmall learningh08f7415dkrecognition‡h35cc20aah61e0f998hd3eb2346hb5451537h3d99813eh823e913dhfbe1b1b9flookedhceb03d78jis devotedhd0737d10ravailable datasetsƒh30410f6fhcde139e2hb267b716hoverview†h6ce47fe9h44b2da3bh60f36d6dh998aa459hd233a8f6h421d4fbcdthey‹h0228d0d4h6af99423h65968ff7h9f87a6a7hd0b6a1bfh27f2c100h88b73f2ah3f343af2h6561bee7hdb8987a0h49d00757gprovide†h0b49794ahb42571ffhba081faehff76aa33hfb13c213h959dee39dsome…hb811269bh6135cf38hfd474bd2h6458097ah297fab3detasksŠh5804ed0eh57777f5dh71c016a8haa6a2b2ch60596bffh658da9c0h9bed2fedh146b755dhce5a83e2hdeabcb8ffvector‚hb1eecbe8h80712b6dhto applyh6cb689a8fhow to‚h0608e8cche6d0fcdflbenefits andha818d166ganalyze‰h1d948ac3hb92df15fhcbf21f4chea0709a8h45072803hcaa92defhef73584bh3baa7af7hbdf65304hadoption‚h53defd32h944d5cfdomodular machinehbec78dc8ibased andh53a48f2dkimage basedhef93d97aetodayƒhed828c86h1d0af46dh3417650etsolution developmenth70efa5eefformalh429081f3nmaxent dualityh8e64aec7idimensionh18754c90gand pach094ee57ajclusteringhaaec0264lvc dimensionh4fe6fb2cfmaxenthace5f85diinferenceh3ae67ca5jinfluencedh937c762ahresearch˜ha2945ff8h67cbafb7h144f7ac8h9d389ec7h104058b5h5e99e55eh7e01e3d6h7138d467h785033abh45c966d8h064c8628ha6f03cf4h92296ac4h636ec8eahdea3041ahc8278d88h481971cbh0cb81937h8714cafch0ff940f5h835d1e43h8506ed54hf640e598h693a400fh5261b8deothe outstandinghc36a700fllearning andh572eee85hfield ofh4503a937jbe appliedhc3d64e60jto exploith953a478bhcomputer…hd464b970hbf949688h115e6e95h211ba7ffh61df2fe7koutstandingh551dbba9jtechniquesh547f0bc2hrequired…h811e7cc0hd8f968a2h0d2a69b6h39d64629h058dd09bkfurthermoreˆh846c0e19h904ae4dche537fce3he0f10bbche2825bceh77a3b01bhf8cfe176h29f18fechprovidedƒhf46debcfh0e0f3465hc925abd5plearning methodsh2dde2f9bjthis paper‹h75ec0eefh40815685h17d89d0eh9713be18hde82cb77hb82ab873h9881b0bbh41e06c33h46181b2dh5dd0c44ahace86ceehlanguageh8d972cdfh4dc0aa75h5763284eh895ef5cfh3f2b5ea5h9608fb72hc46fd598ha3525187hf9cc1b14h2a6036aehe3738f37hd650c64ahbc0aac03heedb5711iand lowerheb3655fdldemonstratedƒhf64cde61h7a1a5748hdf372fcafcan beŠhb7ab5ec0he78456bah4fec510ehdd36bcd9h5fc03a18h68bafc1ahcb186307h13df8833h6b095595h5cd3cec6ereach‚h76a052f5h952ff51adfairh84607bbedeven‚h311eae89h58459f53mimpediment toha6c2b9f3nfair decisionsh242dc6f3kmethods forh941740fbhhope hash2f5d9f15fat theƒh69c22d60h201f0f76h8e846fd9gsystems“h5a2fb7f4hbd89ae96h0387de65h5bec037dh5631daebh11444271h332fc516h13af69a1h66b20bf7hd3bf4c39h5b87554bhe085ea17h89a6fccah938f3369h934fb1ffh0e067f88h9ccc75bch8d08586ahc8101603hthe goalh12c87a15fverifyhc8711489frefineh1a1fd62eecouldƒh7d9c5f3ehf29693edh9ecfc69amintroduced ashd9e66584ktransparenthbf0ee540hto learnh9e81a91alrunning withha07dba80mcontributions„hc9c6ab13hff1e0d59h7f5c75f4h2e8eba7ddruleƒh138979b7hb9ed2af6h08ba6e08duses„h543f78f9he8ed9565hdaa79d1bhd0ed4d4dntranslate corehb488b85egwithoutˆh80d0a668he2681871h9c95a279h10410773h26c0e830ha96c1b16h55d03efahbd847383hthe datahfeba4b20ipermanentha43a059dostructured datah845921f7fto runh7df462e1lwinning codehb0e6d6e0lrarely comesh28baa057iamazon onh41149f6dhstudy of‚he3f92e2ch4521b73ekfrom kaggleh2aec61dcestartƒhd6ab5c52ha504c19dh9a7c347ejdeploymentƒhd6fc2ecfhf40c4a59hfce0b53dkpenetratingh553f70f8ewholeh632c222dfsurveyh4b775ee3jas machineh70f4914ekdynamic ecohcc1b15abgoff-thehc77d31bejand enablehddeda6abkthe current„hf2a565ebh1c093cd5h957b1318h55f2bd1fhstate-ofˆh9bdb8e9bh7d9c3e79hc06fc9aeh1e918336h495a3476h487d0ccfh803faac8h188ae81cipresentedƒhfab40a8ah9473ed57h53a64e89odata scientistshbc046db5jadopted inh4e023d28jhas becomeh45dc8e99nlevel overview‚h4d9ab500h3762ca53jdiscussion„h0464ce6dhfb5fec64h7b012cedh8f35967bhand deep‡h73cf116che345beb1ha6e14bcah310ad79ah0cbb3e27h3b377d4bh1b1a836eddeep’h94ca8f53h776ed066h87c92936h77018a2ch7f6d9e3dh4f89fbach3e8cc974h9e3033c5hcf772aedh97a242d1h81acaa5eh4be7fe14h2a56f557h563924f7he585acd5h5b77bdc5h49584a8eha960f412lthe research‚hc42be446h7dc22252lrespectively‚ha1c5ca6che3df7224lparallelizedh1ec78c6aespark‚h71f2e6deh425abf85lresults haveh622487a0mgiven machineh77786f3bdoverŒhbcb4303ehdbe4c983h1f4ee799h5d2c835fh609a1242hc77a4337h03e66adbh90b00d04h9c0a1b96h94cdee28hd7ed6f65haf083f8fdthan‰h89c8eaeah1acac75dh7f7d9b3dhb6e98338h01ec9078h43d95ca0h3e01540bha943ae81h68839f1fphighly automatedh7a5491d8dmore†hcaf13d96h4ef08a9ah75339728he5e81e7eh5fb240e9hf96fe09eitwo yearsh9707f8f4hgreedilyhc5e21db4jtrains theh54502b71dtimeh2f5c3ee9jnew methodh69fc5dfbiintroduce†h6a92e88bhb1269843h10f0102eh044e9e36ha2875344h3afcd975grequireh4a3c6d24ione layerhb96b555belayerh6e881959fin newh5ab3e26eiwhich wash0217fa7blapplicationsƒhc408864bhe8eaf6e0h66dfc872hworkshopƒhfbf19d8fh4ef744e5ha0816546djuneha5668207dyorkh445770f1dheld‚h005e4514h369f54fefof the‘h45caf1c0hd58e7953h5e2697cdh94b1153eh3cb610f1hc0c5c072h08ec95e1h4dd8e026h79ff4954h0c65aa21ha3fa6f57h1df3e070h13b4c3d9h35d66639h09f97d11h8d2b900fhb976a02bksocial goodh52f57127gto rnnsh4564ec63hfunctionhfcac6b1fmapproximationha204ca92gat someh058f4c06hnetworksƒh87509a8che318e3behb1e34163fcloserh5632b601nlimitations ofh6f68f931pgradient descenthbe614b59vtheoretical challengeshfda07d94panswers multipleh5a250bedhexplainsƒh688bc2a5hb7364051hb9ec6244minsights intoh03d837a4kapplication‹h23d857a2h876c5d32h10fe73b0h3aefe0bdh79fe24d3h58c55881h80e1cd3ahf1f0e02fhcde5c869h3618c9f3hc645ae53iblack boxhc545c5b0jmeaningfulh78c5bdd1imechanismh7522f8fekconvergenceƒhad5fc5dbhd2db35d2h8c66bce4jand futureh2b4a2e5bgtowardsh76f8d631qcortical learningh2c3347aeffutureh08d26242hthe pasth5830cfbfpinterpretabilityhf7cf0021icontinualhf5f0da0akand concepth63b602c8oincremental andhf616d23aglabeledh8ced6f8encodl includinghe3822d2djwe propose„hedbbaf10he99af4afh10ef2481hebf6c033moriented deeph47b9bf81ithe majorhbc95c155hthey use‚hc543ebb6hba35e905nthe protectionh6e9c1a11mprosperity ofh5381564agat deeph80cc2b3ddhand…hb0c83df7hd00c6c06h7dabf220h2d0b0b94h7b7396d3kof research‚heb16dc84h6aa6692ejtheir deephedd594fcjnew schemeh9ea9d418gnetwork…hb989dc99h1a9f17e3h348cd205h313ce712hdbc2253akand quantumh155f91c9gbe usedh180a8164dopenhac091cf4iasked sixhf88f906dminstitute forhf93b7a12hresultedhca313264lintelligencehf23cd5bamdeep learningh3f8b898eetheirh6fa91bd4laware methodh4847e629mon robustnessh41243da0jmany worksh15b77e70jtask-awareh67788c15kse problemsh8eb081c9kimprove thehe3b62cb9hvariantshf313f65cestillˆhe032ce52h966a8901hf454f646h41fbbbc4hbf271d4ah21c7ff29hc835a751he328c618frecentŒh8cd56a71hb9a334f9h0896ac85h051c5196heb1b1693h6840b07dhbbd09f76hf3035305h0b4df765h4a22de62hf0ad6acehc9ace212gimprove‡h95eca6feh7d2b5a5ahf5eb5dfbh7a647264ha038b940habaafd78hfdec90f5jbased deeph9aa87b8afcan weheb516220iknowledgeh06326dafefirst…h488cd5behf7db4692h0c7936e5hed037549h69e76036nperformance ofh574ed44anand challenges‚hdbabf7c6h60f33b12pquantum circuitsh26fe2f1aqquantum computinghcce1e6a2pintroduces majorh15f0a598hfor longh9028d18bdlong‚h314b6ac4hae572c52ideploying‚h20537f67h27dcbbacgempowerh1b742532hnumerous‚h84d3f354he32c0560tattracted increasingh6ceb1c5ffvia anh3994476efdemandhde62a5d4ishouldershfcc0ebddiand stateƒh668ab307h496e0fa6hc23e83b9nsome benchmarkh31a09252gshallow‚hed162c94hcd150ff3ltechnique onhedd58062nart supervisedha7985dfelsegmentationƒhd48632d5h4fdaebffhc17ce4f2sresearch directions‚h27aaa621hf652d1e1eimage‚hf3133057h380c30c6jthe formerha5a799f1ntwo approacheshddc2c9d3jcomponents„h374b0a3cha38b0821h6c144108hb557c1a4hfor both‚he5cfebbehb2d8bb8druses probabilistich8eab0a49jwe includehbdb64a5dxassociated environmentalhfae60fd7ulearning applicationsh481a1889oresearchers andh6df7397dkabout theseh91b1bf4cpenergy-efficient‚hc9df71c2h28b7f96crgreat significanceh5ad3b99dqunified frameworkha2147f50gcapsulehb4d6371cqtheoretical basish1d26fa0flof connectedhde787fb5ggraphichd957db6cjespecially†h56e7a304hedf71064h2cff742ah14b11fffhf74efb5ch817bb5b8gto takeh405e1f43hpatterns‚hceb538a7hd6cf0336elogic‚h99278293hd4a6deb5ndifferentiable‚he58987a7hb1168782rand classificationhd571d119krecent pasth9667f2adjto achieve„hf1d2edbah4d8b84e7he4806c24h3d033625kor templatehbeb6c08fnsolve problems‚h39bb7efeh32b37c90fwithin…h9f3d2ac9hfb98d47fhb8692206h47488521hd271790emformalize and‚hce707deehb1542f2bmquestions the‚h516bd362hbece721aianalyzing…h88e7ad4eha3344536hcbfe6a59ha3854511heef24026gongoingƒh20ab45d6h340027bfh290f9813htried toh48fa3a06nadvantages andh857db328hworks inh94314bdegseen ashecdf6f24marchitectures‚h3dda55e4h373fcf47kwith neuralhb9abd98eunetwork architecturesƒhff9d9a24hb2cfceb5hf9c3b942kcovered areh8142dae1dseenƒh15e88642h7229dcd3hd5026261fneuralƒh82fa52f0h35d2849fh5c7332bdjconsideredhddd7bd54glogicalh8cb2f5cdgused toh71515dffkdiagnosticsh17c7da1egmedicalh41984468smedical diagnosticsh68f0e412emultihced811f9mfunctions areh1784fe1cgprocesshe7d70338fnnpnnsh97568b1fnjust numericalhf1d363c7gproposeƒh3b2412a1h209c6844h3f0c13b7elearnƒh090f567bh9f2cc070h60e4e415eorderh6a1a18b1mand numericalh16f07c79qcomputation issueh8aca9772fmergedhaa412ff8jindustrialhaad9d6d6iaddressedh036e77b2mmerged neuralha2a1ba29jversion tohc3882ffbkaddress theh8740558cfof itsh57a9c3c5mbe identifiedh469db2d1qbiological neural‚h6a974e29h9e10b0e9nperformance ish1deb672cqperforming neuralh605c1c93mcorrespond tohb4fe84dcithe graphh299b6ecegdespite‚ha2b6286ch39eed378jto combine‚h1f2ef777h4c7a5582lone proposesh586724ccqquantum-classicalh5e15e07brmultiuser multipleh125914b3hhardwareƒh50c0f97dh9c194a4fh107f7f5fmcircuits thatha59d8ab1wclassical convolutionalh7ae8fdfdhsum rateh8384241aothe applicationƒh8e1ef5bah59c212c3h109f38efilong-termh41699431ksystems hash48f22c40pimprove accuracyh4897fa83ohigher accuracyh4c5f2b0dvimportant architectureh1f534770phandle differenthe18480f4ganimalsh3e03a29cnoptical neuralh1a15c2a9hpnn haveha4814b0ekbasic ideash83839ed4lparametricalhe78695e3non descriptionhbbc469dakrecognizinghb4896b1chimmunityha3766507gaiq wash096c6475qlayer utilizationh74c3c087pand overtrainingh0a0ab5f8hquotienth0366f433nare randomizedh9d6e4d3ffmetrich1e6d4f36tactivation functionsh19623c84kcontrollers‚hbe96116ch4a36d6f6kthis methodhc9b62fd4mrelated taskshd9fd83f5oparameter spaceh185b7e8ffand weha7d81de2qasymptotic theoryh7af46ca2nin statisticalha6ba231fgsize ofhd941efcdiprove itsh077686bedsize‚h3aaa000ch6fee8f4blof differenthf72db686pinvestigates twohd1fe87f1mconnectionisth63a8d03ahrelation‚h55118728h73599210jof spikinghd05313d8mbreakthroughshc1bf9ad8tprecision activationh96074302mcomparable toh07ddab86dstdph30b1dd87iprecision‚h24895fbch167d3d44jactivationh8870bde8grealismhacfab479rachieving accuracyh6ff58116jindirectlyhb4b98270lsuch systemsh77970e96jshown that‚h6b8f0f82h1c6e6b6ftof overparameterizedh3b7d9709rconvex formulationh7ff0afachfeaturesh8f208656vencountered inpracticeh8ce79c68nperform neuralh17e78bfelthe distanceh09cc9d07gconcept‚hdf97e1c1hd7060508lrelation forh4ea7d7b6hdistanceh40fb462elbisimulationh1e494408qfor nonparametrichc7aae0catspaces correspondingh3e7ec7f2kusing theseh4fb3fa7ajthree reluh2ca98e94mnonparametricha1e30736kestablishedhb545a808pintercorrelationh8a1dce17hreferredh65ad67aaione weirdhf6b643d7ito modernha0269c02hmultiple‚ha92ed5a9h62f644a4lwhen appliedh987bc975dgpush25068f32gnew wayh4505b518tscales significantlyh405abeecorecommendationshf1eba0c7pprocessing usingh04fead55kand opennlphcec30013mdescribes howha096586elare utilizedh404e0887qunderstanding andhc0177ce3rentity recognitionh37279354idepend onh2dff8b9ampreconditionsh67a2ddc2nthe challengeshcb1eb5aejfirst workhfeb92a0fkcommons andhac9d3813kargues thath2ef27601nnote describesh138c7df0jcopenhagenh50ef48e7hand willhaccc5cc4pfreely availablehb8691112fset ofh5acb2fb8dtext‚h5804b202hfba7760beis noha1c210b8dfall‚he6ec73beh58b9e1a4sdifferent subfieldshd81c4c9cllanguage andhff6c21e0jof systemsh26372c99fstoredhe16d5707fto sayh670054b1iunder thehba4be1f8eno orh813ef624noutput despiteh599b2b7dqdifferent sourcesh67d77336pof morphologicalhe29bd079nidentificationhd974f750iprimer onh8d43542fmbring naturalh53b4e7d6jand speechhe39de6ffgtextual‚hd34d2c69h17c88d25hpast fewh598bbf5bnfundamental toh491b79e6jpersonallyh89dcd735nlanguage fronthf4fdcb8crlanguage modellinghc4f8ff60pmost fascinatinghfd43ee8allinguists toh5d88992aonative speakersh5ec9d5a1jtechnologyhdfbb42dehand texthc6c60fc5fnativeh6a33c0d4uprocessing downstreamhcbdccf2efin oneh0303792evcontent-classificationh6a7db2acdfineh30012412narabic naturalh46e1cf2afsocial‚hb1a27f25hd5811d80llarge numberh03f0f7c6qcorrelation amongh17042795gperturbh3df81c63ithe widerh1da9cc09qour understandingh883b1fcbqartificial neuralh28b0c149iprecis ofhb5a69d43kspearheadedhff54ec01lsuccesses ath6289c6f2mnot representh783419a0nspearheaded byh4d8e67dcuperforming linguistichadbfd692jup parsingh14d28a92ngeneral parserh94b99976ogenerators ableh1bfcc978rspecific languageshffe40b12fuse ofh91a52382kmodel-basedha7966fabekindsha365195bowhich constrainha0d7cf9fhit needshf8aac569sgeographic locationh161aa9c8pfurther researchh0d989f8apincluding signedhcce9fa28kcommunitiesh0363b32eecallsh049c9279lworld signedha7c32103fsignedhf19650ccland promoteshb4af3b05planguage abilityhefe838ffjhuman-likeh0efb8905dputsh3746d02blprototypicalheccb9458qspecifications inhaa8957e9idisparatehd8dcf7ffitake intoh0532ccfcfworldshf96d5ed1ooutperforms itshac4ef7d5nbeen evaluatedha4eaee2aolarger languageh153ee5a5xpersianllama significantlyhb012e7d4uintegrates algorithmsh75a77d7agthroughh01bb9a3cpour experimental‚h97d491a7h8c4abe12ocharacter-basedh17700bafjthat theseh58b8dbb5ovectors captureh1b7d167dkempiricallyh209646ealbased neuralh1e056049qvisual impairmenth0e4965c4land autonomyh4e417c0ahmitigateh10e1ddc2kto mitigatehc84562c6hpositiveh09dc1198jthe centerha0f9103cjthe secondh8b5be2f8lorganized byh2db17d28kproceedingshaa960a61mof excellencehe839445adhttph073ed21aiseptember‚hb0686c06h496cffb1ohigh-throughputh8bc059e6iwith highh881c2414gsuch ash43b833a6mto algorithmsh8b8b2629ithe heartha3a464d3gthe aimh7d177519gof whathde16f7dclthe benefitsh7cc60f7bolive monitoringhdefbc6b3ivision orh530f5861presearch projectheb969e14operspective andh2d50dba7jfor policeh67fb7906tdetection assessmenthc91d291drreceived attentionh486f7bc4hmaritimehbf6d7d26linapplicablehb9b57cfdgwork inhd26f4438qmaritime computerh54b1a15eqinterpretable andh01dbdef3kexplainableh9e430b70dbmvche7144f12gcardiffh4a06e728hthe bmvcha487515anproceedings ofh9286e22cfvisionh8b4cb08blpotential toh5383ed50gin formh9c333356gis alsohe1bf3449oinvestigate the‚h2b426f10h90fece65qdemystify severalh91bd4f14xcorresponding contextualhf535cdddpacross differentha25b7f7cgextentsh51085d16jlvlms viewh06f608c3iendeavorshbafe38d5sprojection commonlyh72598145gpopularh068a260bopopular becauseh4be70d71lspecifics ofhb3f1add8fhighlyhc57c529agseveralhafad4f0ehof thesehb268e189mthrough trailh6d1a4dbdmreinforcement…h653f474ch4a7edc9ah461efd26h834aa77bh3cf692c9minto lifelongha532e475othe traditionalh35f915fdolearning system‚h7244990dh8876aeebmsome insightsh0ce57debiprototype‚ha53c9272h8da951acfwe mayh636e8b65pof reinforcementh0ba89b96fcostlyhe0612696gobviateh6d10c22cfdesireh1bcc35f0funsafeh2e6a5e8dmguided repairhf71ebe09rpreviously trainedh4ea812f7hthey mayh47a6a8d7racquired knowledgehd1086a98ressential paradigmh2235e1a0ssequential decisionƒhc2718cceh520f3969h12409f77hthe mainh35681036lobstacles ish204b2b12jreal worldh2511a83bnand weaknesses‚ha59fde98he8c0f0a4otheir strengthsha74273cfjin varioushc2e48647qresearch progressh38dcaf41lprospects ofhcad7f0a6qwe systematicallyhe8465b04qlearning paradigmh5c0689a6dbothh27b95fbfdgameh5ef9576fkinvestigatehfb9d5633jmemory-twoh9795611eisymmetrich9d687590hrepeatedh99f8eed4ilearn theh4195fb8diof mutualh064771a1lthree-factorh1200747dgbackendh01a89dcdlbalancing anh3e0f6a81dpolehbcf0fd1dnclustering andhe7576a17icart-polehdc1c35d9wartificial intelligenceh6d79a13cqconversational aih4afd9db1nand supervisedhd9429537ldiscussed inhae3022cajin summaryh7081a93dqpossess extensiveh3fec5ae7qaligns seamlesslyh81b56020hdrawbackh36519736jprocess ofh662d3c41mimprove theirƒh695404cbh5782d109h9e7d5638pheuristic rewardhd0f779a7hindirecthdb859436qfollowing inverseh1dc4cbf4oframework drawshbffef1b2nfree imitationh0395ff50grecoverh49f7859ajmodel-freeh6aba22c4jfrom whichhb5d7bf1ekrecover theh2f28f38cgpart toh18dc739eoepisodic memoryh514d55f7mmethod calledh44262d43lstate-actionh6e0b0826grecruith33ab9611iinstructsh9d2c6bb9gimitatehe423fef8fbufferhe3545e5blpolicy actorh2125cae0xaccelerate reinforcementh1525f90duproportional integralh3519680bhintegralh68e97f36hpendulumhad9eab3chcoachingh6a7102f6jlearns viah6756e675daqilhf4e0f7cdimethod byh90278f0bnoptimal policyh18061b59eto anhf7012081psignificant timeh7c01fb37fhas onh147e6a31qnetwork attentionhb7f958d3kqualitativeh12ade612kanalysis ofh2d0a581bgtoolkith0aebe6fepaccuracy withouth5f7ed4a0hensembleh5ac36d4aiand matchh0c3dce8bxinterpretable reinforcementh2407a312vsignificantly reducingh8be322a5frecipehad33428bucontributions consisth1357a5aepfor unsupervisedh0507ae7bidesign ashb35df7bbhmotivatehf4cfbcb2land describehcddbcbb5hacquiresh376a003anagent_to_atoms¢uautonomous_copilot_v1†h36f5017ch9a0d07a4h572f6068h5e32af2eh5d0f7310h70d5fb14rarxiv_collector_v1™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_to_atoms¢iwikipedia†h36f5017ch9a0d07a4h572f6068h5e32af2eh5d0f7310h70d5fb14earxiv™Lhe528b903h20d0d6cchac2db510h570c3fc2h47bf5016h9102595fhff0583d9he0004a75ha2eb1487hef25baach2d15e853h11918eb5h84c99158he4408f88hd38c7c8eh0e1334f3ha76329a2h21284da8h68cfa4cehb97130c8ha5dcfc32h7a9ba002h872244f6h5c8955e3hd362dcfch08f7415dh35cc20aaha6369815hceb03d78hd0737d10h30410f6fh9eef4a92h6ce47fe9h0228d0d4h0b49794ahb811269bh5804ed0ehb1eecbe8h6cb689a8h0608e8ccha818d166h1d948ac3h53defd32h9cd5598ahbec78dc8h53a48f2dhef93d97ah6af99423hed828c86h70efa5eeh429081f3h8e64aec7h18754c90h094ee57ah7ee52082haaec0264h4fe6fb2chace5f85dh3ae67ca5hff679b88h937c762aha2945ff8hc36a700fh572eee85h4503a937hc3d64e60h953a478bhd464b970h551dbba9h547f0bc2h811e7cc0h846c0e19h0dca6eb0hf46debcfh2dde2f9bh75ec0eefh8d972cdfheb3655fdh6135cf38hf64cde61hb7ab5ec0h76a052f5h84607bbeh83561e8fh311eae89ha6c2b9f3h242dc6f3h941740fbh2f5d9f15h69c22d60h5a2fb7f4h12c87a15he679c479hc8711489h1a1fd62eh7d9c5f3ehd9e66584h0897cfb4hbf0ee540h9e81a91ahe78456baha07dba80hc9c6ab13h138979b7h543f78f9hb488b85eh80d0a668hfeba4b20ha43a059dhb92df15fh845921f7h7df462e1hb0e6d6e0h28baa057h41149f6dh5ab528f7h0e0f3465he3f92e2ch57777f5dh2aec61dchd6ab5c52h67cbafb7hd6fc2ecfh553f70f8h632c222dh38979a72h44b2da3bh4b775ee3h70f4914ehcc1b15abhe2681871hc77d31beh1ad60057hddeda6abhf2a565ebh9bdb8e9bh51a5f919hfab40a8ahbc046db5h4e023d28h45dc8e99h4d9ab500hbd89ae96h144f7ac8h0464ce6dhb7f2c348h60f36d6dh73cf116chb42571ffh94ca8f53h9d389ec7hcbf21f4chb2e4e5f9hc42be446ha1c5ca6ch1ec78c6ah40815685h71f2e6deh622487a0hfd474bd2hd8f968a2h77786f3bhbcb4303ehdc039dbeh89c8eaeah82cb45fch7a5491d8hc925abd5hcaf13d96h9707f8f4hc5e21db4h54502b71h2f5c3ee9h69fc5dfbh19497b53h6a92e88bh4a3c6d24hcb10ed70hb96b555bh6e881959h5ab3e26eh0217fa7bhc408864bhfbf19d8fh460616fcha5668207h445770f1h005e4514h45caf1c0h52f57127h4564ec63hfcac6b1fha204ca92hc3493209h058f4c06h87509a8ch5632b601h6f68f931hbe614b59hfda07d94h5a250bedh104058b5h688bc2a5h03d837a4h23d857a2he39d4a52hc545c5b0h78c5bdd1h776ed066h7522f8fehad5fc5dbh5e99e55eh1229c143h2b4a2e5bh76f8d631h9473ed57h2c3347aeh87c92936h08d26242h5830cfbfhf7cf0021h0387de65hf5f0da0ah63b602c8hf616d23ah8ced6f8ehe3822d2dhedbbaf10h47b9bf81hbc95c155hc543ebb6h7e01e3d6h6e9c1a11h5381564ah80cc2b3dhb0c83df7heb16dc84h65968ff7he345beb1hedd594fch5bec037dhb9ed2af6hd58e7953h77018a2ch9ea9d418hb989dc99he6d0fcdfh155f91c9h180a8164hac091cf4h7138d467hf88f906dhfbcf10e8hf93b7a12ha6e14bcah7f6d9e3dhca313264hf23cd5bah3f8b898eh6fa91bd4h4fec510eh4847e629h5631daebh41243da0h1acac75dh15b77e70h67788c15h310ad79ah5e2697cdh4ef08a9ah8eb081c9h785033abhe3b62cb9hf313f65che032ce52h8cd56a71h75339728h95eca6feh4f89fbach71c016a8h9aa87b8ah876c5d32h7d2b5a5ahf29693edh3e8cc974heb516220haa6a2b2ch06326dafh488cd5beh574ed44ahdbabf7c6h26fe2f1ah45c966d8hb7364051hcce1e6a2h15f0a598h9028d18bh314b6ac4h10fe73b0h7dc22252h7d9c3e79h20537f67h966a8901h1b742532h84d3f354h6ceb1c5fh3994476ehde62a5d4hfcc0ebddh9e3033c5h668ab307h7f7d9b3dh31a09252hed162c94h0cbb3e27h94b1153ehedd58062h6458097aha7985dfehcf772aedha504c19dh064c8628h496e0fa6hd48632d5hcde139e2h27aaa621hc06fc9aehf3133057hb9a334f9h3b377d4bhe8ed9565ha5a799f1hddc2c9d3h374b0a3che5cfebbeh8eab0a49h297fab3dh97a242d1hbdb64a5dh1a9f17e3hf40c4a59hfae60fd7h481a1889h6df7397dh2f26811fh91b1bf4chc9df71c2h998aa459h17d89d0eh81acaa5eh5ad3b99dha2147f50hb4d6371chba081faeh9ecfc69ah1d26fa0fh4be7fe14hf7db4692hde787fb5hd957db6cha6f03cf4h56e7a304h405e1f43hceb538a7hd00c6c06h99278293hf454f646h3cb610f1he58987a7hd571d119h9667f2adhf1d2edbah11444271hfce0b53dhbeb6c08fh61e0f998h1d0af46dh4760a4eehd233a8f6h0896ac85h39bb7efeh904ae4dch92296ac4h9f3d2ac9hce707deehea0709a8h516bd362h41fbbbc4h88e7ad4eh20ab45d6h37d66c68h48fa3a06hbf271d4ah857db328he5e81e7ehfd284f3bh4dc0aa75h2a56f557h94314bdeh201f0f76hecdf6f24h3dda55e4hedf71064hb9abd98ehff9d9a24hffa35be9h8142dae1h421d4fbch15e88642h82fa52f0hc9a7bd23hddd7bd54h8cb2f5cdh71515dffh17c7da1eh41984468h35d2849fhe318e3beh68f0e412hced811f9hdd36bcd9h1784fe1che7d70338h97568b1fhf1d363c7he99af4afh3b2412a1h090f567bh6a1a18b1h16f07c79h8aca9772haa412ff8h332fc516haad9d6d6h036e77b2ha2a1ba29hc0c5c072hc3882ffbh8740558ch563924f7h57a9c3c5h469db2d1h6a974e29h1deb672ch08ec95e1h605c1c93hb4fe84dch299b6eceh60596bffha2b6286ch1f2ef777h5fc03a18h586724cch5e15e07bh125914b3h50c0f97dha59d8ab1h2cff742ah7ae8fdfdh8384241ahe537fce3h13af69a1h0d2a69b6h8e1ef5bah41699431hdbe4c983h48f22c40hae572c52h3aefe0bdh4897fa83h952ff51ah4c5f2b0dh1c093cd5hd3eb2346h3ed922b6hff76aa33h1f534770he18480f4h658da9c0h3e03a29ch1a15c2a9ha4814b0eh83839ed4he78695e3h373fcf47h2961d877hbbc469dahb4896b1cha3766507h3417650eh096c6475h74c3c087hb2cfceb5hb6e98338h4dd8e026h0a0ab5f8h58459f53h0366f433h9d6e4d3fh1e6d4f36hba35e905h66b20bf7h19623c84hbe96116chf9c3b942h73dac9cehc9b62fd4h9f87a6a7h051c5196hd9fd83f5h185b7e8fha7d81de2he8eaf6e0h7af46ca2h10ef2481ha6ba231fhd941efcdh077686beh209c6844h3aaa000che0f10bbchb1269843hf72db686hd1fe87f1h1e918336h63a8d03ah55118728hf5eb5dfbh9713be18hb2d8bb8dhff1e0d59h9c194a4fh9e10b0e9hfb5fec64h14b11fffhd05313d8h28b7f96cheb1b1693hc1bf9ad8h0c7936e5h96074302h1f4ee799h07ddab86h30b1dd87h24895fbch8870bde8h348cd205hacfab479h6ff58116hb4b98270h77970e96hd3bf4c39h6b8f0f82h45072803h3b7d9709h5d2c835fh7ff0afach8f208656hde82cb77h8ce79c68h79fe24d3h73599210h17e78bfeh167d3d44h09cc9d07hdf97e1c1h4ea7d7b6h313ce712h40fb462eh1e494408hd2db35d2hc7aae0cah3e7ec7f2h1c6e6b6fh4fb3fa7ah609a1242h2ca98e94ha1e30736hb545a808hcd150ff3h68bafc1ahe2825bcehb9ec6244h8a1dce17h01ec9078hd0b6a1bfhe32c0560h65ad67aah5fb240e9hb82ab873hf6b643d7ha0269c02ha92ed5a9h987bc975h43d95ca0h5c7332bdhb1e34163h25068f32h4505b518h405abeech39d64629h636ec8eahcaa92defhf1eba0c7h04fead55ha38b0821h79ff4954hcec30013h5763284eha096586eh404e0887h08ba6e08h5b87554bhba46228ehc0177ce3h37279354h2dff8b9ahb5451537hc77a4337h137bc357h67a2ddc2h10f0102ehcb1eb5aehe3df7224ha3344536hfeb92a0fhac9d3813h2ef27601h9881b0bbh895ef5cfh138c7df0h50ef48e7haccc5cc4h03e66adbhb8691112h04b84b05h5acb2fb8h5804b202h27f2c100h3f2b5ea5ha1c210b8he085ea17he6ec73behd81c4c9chff6c21e0h26372c99he16d5707h670054b1h6dc0ca79hba4be1f8h813ef624h957b1318h0c65aa21h599b2b7dh9608fb72h67d77336h4521b73ehe29bd079hd974f750h39eed378h8d43542fh53b4e7d6hdea3041ah90b00d04h3d99813eh495a3476he3df0a8dhe39de6ffhd34d2c69h598bbf5bh491b79e6h58b9e1a4h89dcd735hf4fdcb8chf0e8c2ffh88b73f2ahc46fd598h2df21fafhc4f8ff60hfd43ee8ah5d88992ah5ec9d5a1hdfbb42dehc6c60fc5hbf949688hf74efb5ch6a33c0d4h823e913dh27dcbbachfba7760bhcbdccf2eh9c95a279h0303792eh6a7db2ach6840b07dha3fa6f57h30012412ha3525187h46e1cf2ahb1a27f25hc8278d88h7dabf220h03f0f7c6h21c7ff29h1df3e070h17042795hf9cc1b14h7a1a5748h3df81c63h9bed2fedh1da9cc09h883b1fcbh28b0c149hb5a69d43hff54ec01h6289c6f2h783419a0h58c55881h4d8e67dchadbfd692h2a6036aeh14d28a92h94b99976h1bfcc978hffe40b12h91a52382ha7966fabha365195bha0d7cf9fhf8aac569h4d8b84e7h161aa9c8h481971cbh9c0a1b96h55f2bd1fh3e01540bh0d989f8ah3f343af2hc835a751h13b4c3d9hcce9fa28h0cb81937h0363b32eh049c9279h944d5cfdha7c32103h6aa6692ehf19650cche3738f37hd5811d80h8714cafchb860326ahbbd09f76hb4af3b05hd650c64ah146b755dhd7060508hefe838ffh0efb8905h3746d02bhdaa79d1bh59c212c3heccb9458haa8957e9hd8dcf7ffh0532ccfchd4a6deb5h80e1cd3ahf96d5ed1h0e4328c6hcb186307h107f7f5fhac4ef7d5h817bb5b8ha4eaee2ah17c88d25h153ee5a5hb012e7d4h5abb8e9ch35d66639h7f5c75f4h89a6fccahef73584bhfbe1b1b9h75a77d7ah53a64e89hcbfe6a59h01bb9a3chfb13c213h97d491a7h13df8833h17700bafh58b8dbb5h7229dcd3h7a647264hbc0aac03h1b7d167dh209646eah80712b6dh1e056049h0ff940f5h0e4965c4h4e417c0ah94cdee28h10e1ddc2hf1f0e02fhc84562c6hd5026261h09dc1198h959dee39ha0f9103ch115e6e95h8b5be2f8h4ef744e5h369f54feh2db17d28haa960a61he839445ah073ed21ahb0686c06he4806c24h8bc059e6h058dd09bh881c2414h43b833a6h211ba7ffh66dfc872h62f644a4h8b8b2629ha3a464d3h6b095595h7b012cedhf3035305ha038b940h41e06c33h7d177519hdf372fcahe585acd5hde16f7dch7cc60f7bhdefbc6b3h530f5861h835d1e43hfb98d47fh938f3369hcde5c869heb969e14h2d50dba7h67fb7906h340027bfh60f33b12hc91d291dh3618c9f3h486f7bc4hbf6d7d26hb9b57cfdhed037549hd26f4438h54b1a15eh8e846fd9h496cffb1h01dbdef3h9e430b70he7144f12h4a06e728ha0816546ha487515ahf05d621ch9286e22ch8b4cb08bhb8692206h4fdaebffh109f38efh5383ed50h8f35967bh9c333356he1bf3449h2b426f10h91bd4f14hc645ae53hf535cdddha25b7f7ch10410773h51085d16h06f608c3h6561bee7hbafe38d5heedb5711h425abf85hce5a83e2h72598145h3762ca53h068a260bh4be70d71hb3f1add8hc57c529ahafad4f0eh61df2fe7h6fee8f4bhb268e189h934fb1ffh6d1a4dbdh653f474cha532e475h35f915fdhd7ed6f65h7244990dh0ce57debh9f2cc070ha53c9272h0e067f88h636e8b65h4a7edc9ah0ba89b96he0612696h6d10c22ch1bcc35f0h2e6a5e8dhf71ebe09h4ea812f7h9a7c347eh8506ed54hc23e83b9hc17ce4f2hb267b716hf652d1e1h487d0ccfh380c30c6h0b4df765h1b1a836eh47a6a8d7hd1086a98h2235e1a0hc2718cceh044e9e36hdb8987a0h4a22de62h35681036h204b2b12h2511a83bh3d033625h77a3b01bha59fde98h26c0e830h520f3969ha74273cfhc2e48647h6c144108ha3854511h09f97d11hf640e598h38dcaf41h3baa7af7h12409f77hcad7f0a6h90fece65h803faac8he8465b04h5c0689a6hf0ad6aceh461efd26h27b95fbfh5ef9576fhfb9d5633h60e4e415h9795611eh9d687590h99f8eed4h4195fb8dh064771a1h1200747dh834aa77bh01a89dcdh6921726dh3e0f6a81hbcf0fd1dh8da951ach8876aeebhe7576a17hdc1c35d9h9ccc75bch6d79a13ch4afd9db1hd9429537hae3022cah7081a93dh8d2b900fh46181b2dh5b77bdc5hdeabcb8fh47488521hf8cfe176h8d08586ah693a400fha2875344h59a94040h3fec5ae7hc9ace212h81b56020h36519736h32b37c90h29f18fech5261b8dehd271790ehb1542f2bhbdf65304hbece721ahe328c618heef24026h290f9813h662d3c41h695404cbh3afcd975hd0f779a7h2d0b0b94h2d622b51h49d00757hb1168782hf96fe09ehabaafd78hdb859436ha96c1b16haf083f8fh1dc4cbf4hbffef1b2h0395ff50h49f7859ah6aba22c4hb5d7bf1eh2f28f38ch4c7a5582h18dc739ehc8101603hd0ed4d4dh514d55f7he8c0f0a4h44262d43h6e0b0826h188ae81ch7b7396d3h5782d109h33ab9611h9d2c6bb9he423fef8hd6cf0336ha943ae81he3545e5bhb557c1a4h2125cae0hb976a02bh3cf692c9h1525f90dhebf6c033h4a36d6f6h3f0c13b7h3519680bh68e97f36h41e06a55had9eab3ch6a7102f6h8c66bce4h6756e675he02e8e20hf4e0f7cdh90278f0bh5dd0c44ah18061b59hf7012081h49584a8eh7c01fb37h147e6a31h68839f1fhace86ceehb7f958d3ha960f412h69e76036hdbc2253ah12ade612h2d0a581bh0aebe6feh9e7d5638h55d03efah4978076ch5f7ed4a0h8c4abe12h5ac36d4ahfdec90f5h0c3dce8bh2407a312h8be322a5h5cd3cec6had33428bh2e8eba7dh1357a5aeh0507ae7bhb35df7bbhf4cfbcb2hcddbcbb5h376a003ahbd847383ntemporal_index™R‚x2025-08-15T21:38:32.549669h36f5017c‚x2025-08-15T21:38:33.856007h9a0d07a4‚x2025-08-15T21:38:36.491837h572f6068‚x2025-08-15T21:38:37.704271h5e32af2e‚x2025-08-15T21:38:38.884966h5d0f7310‚x2025-08-15T21:38:40.952643h70d5fb14‚x2025-08-15T21:55:32.422687he528b903‚x2025-08-15T21:55:32.425727h20d0d6cc‚x2025-08-15T21:55:32.425818hac2db510‚x2025-08-15T21:55:32.425853h570c3fc2‚x2025-08-15T21:55:32.425883h47bf5016‚x2025-08-15T21:55:32.425925h9102595f‚x2025-08-15T21:55:32.426026hff0583d9‚x2025-08-15T21:55:32.426137he0004a75‚x2025-08-15T21:55:32.426268ha2eb1487‚x2025-08-15T21:55:32.426407hef25baac‚x2025-08-15T21:55:32.529530h2d15e853‚x2025-08-15T21:55:32.529675h11918eb5‚x2025-08-15T21:55:32.529732h84c99158‚x2025-08-15T21:55:32.529776he4408f88‚x2025-08-15T21:55:32.529815hd38c7c8e‚x2025-08-15T21:55:32.529857h0e1334f3‚x2025-08-15T21:55:32.529895ha76329a2‚x2025-08-15T21:55:32.529942h21284da8‚x2025-08-15T21:55:32.529987h68cfa4ce‚x2025-08-15T21:55:32.530028hb97130c8‚x2025-08-15T21:55:32.633273ha5dcfc32‚x2025-08-15T21:55:32.633460h7a9ba002‚x2025-08-15T21:55:32.633524h872244f6‚x2025-08-15T21:55:32.633568h5c8955e3‚x2025-08-15T21:55:32.633626hd362dcfc‚x2025-08-15T21:55:32.633675h08f7415d‚x2025-08-15T21:55:32.633727h35cc20aa‚x2025-08-15T21:55:32.633779ha6369815‚x2025-08-15T21:55:32.633824hceb03d78‚x2025-08-15T21:55:32.633866hd0737d10‚x2025-08-15T21:55:32.736800h30410f6f‚x2025-08-15T21:55:32.736919h9eef4a92‚x2025-08-15T21:55:32.736977h6ce47fe9‚x2025-08-15T21:55:32.737030h0228d0d4‚x2025-08-15T21:55:32.737076h0b49794a‚x2025-08-15T21:55:32.737123hb811269b‚x2025-08-15T21:55:32.737170h5804ed0e‚x2025-08-15T21:55:32.737216hb1eecbe8‚x2025-08-15T21:55:32.737255h6cb689a8‚x2025-08-15T21:55:32.737295h0608e8cc‚x2025-08-15T21:55:32.839946ha818d166‚x2025-08-15T21:55:32.840107h1d948ac3‚x2025-08-15T21:55:32.840176h53defd32‚x2025-08-15T21:55:32.840230h9cd5598a‚x2025-08-15T21:55:32.840293hbec78dc8‚x2025-08-15T21:55:32.840384h53a48f2d‚x2025-08-15T21:55:32.840450hef93d97a‚x2025-08-15T21:55:32.840506h6af99423‚x2025-08-15T21:55:32.840560hed828c86‚x2025-08-15T21:55:32.840614h70efa5ee‚x2025-08-15T21:55:32.943640h429081f3‚x2025-08-15T21:55:32.943756h8e64aec7‚x2025-08-15T21:55:32.943807h18754c90‚x2025-08-15T21:55:32.943867h094ee57a‚x2025-08-15T21:55:32.943909h7ee52082‚x2025-08-15T21:55:32.943950haaec0264‚x2025-08-15T21:55:32.943993h4fe6fb2c‚x2025-08-15T21:55:32.944030hace5f85d‚x2025-08-15T21:55:32.944066h3ae67ca5‚x2025-08-15T21:55:32.944101hff679b88‚x2025-08-15T21:55:33.048070h937c762a‚x2025-08-15T21:55:33.048136ha2945ff8‚x2025-08-15T21:55:33.048175hc36a700f‚x2025-08-15T21:55:33.048217h572eee85‚x2025-08-15T21:55:33.048268h4503a937‚x2025-08-15T21:55:33.048386hc3d64e60‚x2025-08-15T21:55:33.048497h953a478b‚x2025-08-15T21:55:33.048564hd464b970‚x2025-08-15T21:55:33.048625h551dbba9‚x2025-08-15T21:55:33.051408h547f0bc2‚x2025-08-15T21:55:33.154239h811e7cc0‚x2025-08-15T21:55:33.154454h846c0e19‚x2025-08-15T21:55:33.154528h0dca6eb0‚x2025-08-15T21:55:33.154583hf46debcf‚x2025-08-15T21:55:33.154647h2dde2f9b‚x2025-08-15T21:55:33.154697h75ec0eef‚x2025-08-15T21:55:33.154749h8d972cdf‚x2025-08-15T21:55:33.154800heb3655fd‚x2025-08-15T21:55:33.154852h6135cf38‚x2025-08-15T21:55:33.154894hf64cde61‚x2025-08-15T21:55:33.260700hb7ab5ec0‚x2025-08-15T21:55:33.260858h76a052f5‚x2025-08-15T21:55:33.260915h84607bbe‚x2025-08-15T21:55:33.260955h83561e8f‚x2025-08-15T21:55:33.261001h311eae89‚x2025-08-15T21:55:33.261040ha6c2b9f3‚x2025-08-15T21:55:33.261083h242dc6f3‚x2025-08-15T21:55:33.261122h941740fb‚x2025-08-15T21:55:33.261165h2f5d9f15‚x2025-08-15T21:55:33.261202h69c22d60‚x2025-08-15T21:55:33.362094h5a2fb7f4‚x2025-08-15T21:55:33.362166h12c87a15‚x2025-08-15T21:55:33.362187he679c479‚x2025-08-15T21:55:33.362218hc8711489‚x2025-08-15T21:55:33.362243h1a1fd62e‚x2025-08-15T21:55:33.362269h7d9c5f3e‚x2025-08-15T21:55:33.362291hd9e66584‚x2025-08-15T21:55:33.362313h0897cfb4‚x2025-08-15T21:55:33.362337hbf0ee540‚x2025-08-15T21:55:33.362380h9e81a91a‚x2025-08-15T21:55:33.468803he78456ba‚x2025-08-15T21:55:33.468931ha07dba80‚x2025-08-15T21:55:33.468977hc9c6ab13‚x2025-08-15T21:55:33.469024h138979b7‚x2025-08-15T21:55:33.469073h543f78f9‚x2025-08-15T21:55:33.469121hb488b85e‚x2025-08-15T21:55:33.469165h80d0a668‚x2025-08-15T21:55:33.469207hfeba4b20‚x2025-08-15T21:55:33.469249ha43a059d‚x2025-08-15T21:55:33.471643hb92df15f‚x2025-08-15T21:55:33.574376h845921f7‚x2025-08-15T21:55:33.574577h7df462e1‚x2025-08-15T21:55:33.574646hb0e6d6e0‚x2025-08-15T21:55:33.574698h28baa057‚x2025-08-15T21:55:33.574745h41149f6d‚x2025-08-15T21:55:33.574793h5ab528f7‚x2025-08-15T21:55:33.574839h0e0f3465‚x2025-08-15T21:55:33.574880he3f92e2c‚x2025-08-15T21:55:33.574965h57777f5d‚x2025-08-15T21:55:33.575024h2aec61dc‚x2025-08-15T21:55:33.676775hd6ab5c52‚x2025-08-15T21:55:33.676854h67cbafb7‚x2025-08-15T21:55:33.676908hd6fc2ecf‚x2025-08-15T21:55:33.676953h553f70f8‚x2025-08-15T21:55:33.676996h632c222d‚x2025-08-15T21:55:33.677036h38979a72‚x2025-08-15T21:55:33.677083h44b2da3b‚x2025-08-15T21:55:33.677126h4b775ee3‚x2025-08-15T21:55:33.677170h70f4914e‚x2025-08-15T21:55:33.677216hcc1b15ab‚x2025-08-15T21:55:33.782450he2681871‚x2025-08-15T21:55:33.782619hc77d31be‚x2025-08-15T21:55:33.782733h1ad60057‚x2025-08-15T21:55:33.782845hddeda6ab‚x2025-08-15T21:55:33.782959hf2a565eb‚x2025-08-15T21:55:33.783077h9bdb8e9b‚x2025-08-15T21:55:33.783186h51a5f919‚x2025-08-15T21:55:33.783303hfab40a8a‚x2025-08-15T21:55:33.789819hbc046db5‚x2025-08-15T21:55:33.790004h4e023d28‚x2025-08-15T21:55:33.898282h45dc8e99‚x2025-08-15T21:55:33.898447h4d9ab500‚x2025-08-15T21:55:33.898499hbd89ae96‚x2025-08-15T21:55:33.898568h144f7ac8‚x2025-08-15T21:55:33.898615h0464ce6d‚x2025-08-15T21:55:33.898651hb7f2c348‚x2025-08-15T21:55:33.898691h60f36d6d‚x2025-08-15T21:55:33.898726h73cf116c‚x2025-08-15T21:55:33.898765hb42571ff‚x2025-08-15T21:55:33.898807h94ca8f53‚x2025-08-15T21:55:34.002011h9d389ec7‚x2025-08-15T21:55:34.002120hcbf21f4c‚x2025-08-15T21:55:34.002163hb2e4e5f9‚x2025-08-15T21:55:34.002207hc42be446‚x2025-08-15T21:55:34.002249ha1c5ca6c‚x2025-08-15T21:55:34.002292h1ec78c6a‚x2025-08-15T21:55:34.002327h40815685‚x2025-08-15T21:55:34.002390h71f2e6de‚x2025-08-15T21:55:34.002433h622487a0‚x2025-08-15T21:55:34.002468hfd474bd2‚x2025-08-15T21:55:34.115166hd8f968a2‚x2025-08-15T21:55:34.115259h77786f3b‚x2025-08-15T21:55:34.115293hbcb4303e‚x2025-08-15T21:55:34.115326hdc039dbe‚x2025-08-15T21:55:34.115371h89c8eaea‚x2025-08-15T21:55:34.115406h82cb45fc‚x2025-08-15T21:55:34.115440h7a5491d8‚x2025-08-15T21:55:34.115487hc925abd5‚x2025-08-15T21:55:34.115537hcaf13d96‚x2025-08-15T21:55:34.115571h9707f8f4‚x2025-08-15T21:55:34.218713hc5e21db4‚x2025-08-15T21:55:34.218865h54502b71‚x2025-08-15T21:55:34.218924h2f5c3ee9‚x2025-08-15T21:55:34.218986h69fc5dfb‚x2025-08-15T21:55:34.219038h19497b53‚x2025-08-15T21:55:34.219084h6a92e88b‚x2025-08-15T21:55:34.219135h4a3c6d24‚x2025-08-15T21:55:34.219222hcb10ed70‚x2025-08-15T21:55:34.219288hb96b555b‚x2025-08-15T21:55:34.219335h6e881959‚x2025-08-15T21:55:34.325767h5ab3e26e‚x2025-08-15T21:55:34.325923h0217fa7b‚x2025-08-15T21:55:34.326050hc408864b‚x2025-08-15T21:55:34.326160hfbf19d8f‚x2025-08-15T21:55:34.326261h460616fc‚x2025-08-15T21:55:34.326380ha5668207‚x2025-08-15T21:55:34.326486h445770f1‚x2025-08-15T21:55:34.326588h005e4514‚x2025-08-15T21:55:34.326701h45caf1c0‚x2025-08-15T21:55:34.326811h52f57127‚x2025-08-15T21:55:34.429727h4564ec63‚x2025-08-15T21:55:34.429874hfcac6b1f‚x2025-08-15T21:55:34.430001ha204ca92‚x2025-08-15T21:55:34.430119hc3493209‚x2025-08-15T21:55:34.430257h058f4c06‚x2025-08-15T21:55:34.430395h87509a8c‚x2025-08-15T21:55:34.430509h5632b601‚x2025-08-15T21:55:34.430621h6f68f931‚x2025-08-15T21:55:34.430745hbe614b59‚x2025-08-15T21:55:34.430863hfda07d94‚x2025-08-15T21:55:36.050710h5a250bed‚x2025-08-15T21:55:36.050784h104058b5‚x2025-08-15T21:55:36.050824h688bc2a5‚x2025-08-15T21:55:36.050863h03d837a4‚x2025-08-15T21:55:36.050901h23d857a2‚x2025-08-15T21:55:36.050944he39d4a52‚x2025-08-15T21:55:36.050989hc545c5b0‚x2025-08-15T21:55:36.051030h78c5bdd1‚x2025-08-15T21:55:36.051071h776ed066‚x2025-08-15T21:55:36.051114h7522f8fe‚x2025-08-15T21:55:36.153107had5fc5db‚x2025-08-15T21:55:36.153222h5e99e55e‚x2025-08-15T21:55:36.153301h1229c143‚x2025-08-15T21:55:36.153373h2b4a2e5b‚x2025-08-15T21:55:36.153429h76f8d631‚x2025-08-15T21:55:36.153476h9473ed57‚x2025-08-15T21:55:36.153522h2c3347ae‚x2025-08-15T21:55:36.153564h87c92936‚x2025-08-15T21:55:36.153606h08d26242‚x2025-08-15T21:55:36.153650h5830cfbf‚x2025-08-15T21:55:36.258259hf7cf0021‚x2025-08-15T21:55:36.258405h0387de65‚x2025-08-15T21:55:36.258458hf5f0da0a‚x2025-08-15T21:55:36.258493h63b602c8‚x2025-08-15T21:55:36.258528hf616d23a‚x2025-08-15T21:55:36.258565h8ced6f8e‚x2025-08-15T21:55:36.258601he3822d2d‚x2025-08-15T21:55:36.258636hedbbaf10‚x2025-08-15T21:55:36.258670h47b9bf81‚x2025-08-15T21:55:36.258703hbc95c155‚x2025-08-15T21:55:36.363957hc543ebb6‚x2025-08-15T21:55:36.364024h7e01e3d6‚x2025-08-15T21:55:36.364062h6e9c1a11‚x2025-08-15T21:55:36.364099h5381564a‚x2025-08-15T21:55:36.364136h80cc2b3d‚x2025-08-15T21:55:36.364172hb0c83df7‚x2025-08-15T21:55:36.364208heb16dc84‚x2025-08-15T21:55:36.364240h65968ff7‚x2025-08-15T21:55:36.364274he345beb1‚x2025-08-15T21:55:36.364306hedd594fc‚x2025-08-15T21:55:36.465131h5bec037d‚x2025-08-15T21:55:36.465199hb9ed2af6‚x2025-08-15T21:55:36.465244hd58e7953‚x2025-08-15T21:55:36.465284h77018a2c‚x2025-08-15T21:55:36.465324h9ea9d418‚x2025-08-15T21:55:36.465378hb989dc99‚x2025-08-15T21:55:36.465420he6d0fcdf‚x2025-08-15T21:55:36.465460h155f91c9‚x2025-08-15T21:55:36.465501h180a8164‚x2025-08-15T21:55:36.465538hac091cf4‚x2025-08-15T21:55:36.573782h7138d467‚x2025-08-15T21:55:36.573923hf88f906d‚x2025-08-15T21:55:36.573975hfbcf10e8‚x2025-08-15T21:55:36.574025hf93b7a12‚x2025-08-15T21:55:36.574068ha6e14bca‚x2025-08-15T21:55:36.574123h7f6d9e3d‚x2025-08-15T21:55:36.574171hca313264‚x2025-08-15T21:55:36.574211hf23cd5ba‚x2025-08-15T21:55:36.574252h3f8b898e‚x2025-08-15T21:55:36.574296h6fa91bd4‚x2025-08-15T21:55:36.697649h4fec510e‚x2025-08-15T21:55:36.697842h4847e629‚x2025-08-15T21:55:36.697974h5631daeb‚x2025-08-15T21:55:36.698097h41243da0‚x2025-08-15T21:55:36.698222h1acac75d‚x2025-08-15T21:55:36.698335h15b77e70‚x2025-08-15T21:55:36.701604h67788c15‚x2025-08-15T21:55:36.701821h310ad79a‚x2025-08-15T21:55:36.701953h5e2697cd‚x2025-08-15T21:55:36.702082h4ef08a9a‚x2025-08-15T21:55:36.817517h8eb081c9‚x2025-08-15T21:55:36.817646h785033ab‚x2025-08-15T21:55:36.817702he3b62cb9‚x2025-08-15T21:55:36.817747hf313f65c‚x2025-08-15T21:55:36.817793he032ce52‚x2025-08-15T21:55:36.817836h8cd56a71‚x2025-08-15T21:55:36.817895h75339728‚x2025-08-15T21:55:36.817947h95eca6fe‚x2025-08-15T21:55:36.817989h4f89fbac‚x2025-08-15T21:55:36.818036h71c016a8‚x2025-08-15T21:55:36.935440h9aa87b8a‚x2025-08-15T21:55:36.944754h876c5d32‚x2025-08-15T21:55:36.944907h7d2b5a5a‚x2025-08-15T21:55:36.944972hf29693ed‚x2025-08-15T21:55:36.945025h3e8cc974‚x2025-08-15T21:55:36.945078heb516220‚x2025-08-15T21:55:36.945129haa6a2b2c‚x2025-08-15T21:55:36.945185h06326daf‚x2025-08-15T21:55:36.945236h488cd5be‚x2025-08-15T21:55:36.945285h574ed44a‚x2025-08-15T21:55:37.046567hdbabf7c6‚x2025-08-15T21:55:37.046695h26fe2f1a‚x2025-08-15T21:55:37.046749h45c966d8‚x2025-08-15T21:55:37.046799hb7364051‚x2025-08-15T21:55:37.046842hcce1e6a2‚x2025-08-15T21:55:37.046884h15f0a598‚x2025-08-15T21:55:37.046925h9028d18b‚x2025-08-15T21:55:37.046964h314b6ac4‚x2025-08-15T21:55:37.047004h10fe73b0‚x2025-08-15T21:55:37.047041h7dc22252‚x2025-08-15T21:55:37.158089h7d9c3e79‚x2025-08-15T21:55:37.158277h20537f67‚x2025-08-15T21:55:37.164555h966a8901‚x2025-08-15T21:55:37.164763h1b742532‚x2025-08-15T21:55:37.164919h84d3f354‚x2025-08-15T21:55:37.165052h6ceb1c5f‚x2025-08-15T21:55:37.165184h3994476e‚x2025-08-15T21:55:37.165314hde62a5d4‚x2025-08-15T21:55:37.165478hfcc0ebdd‚x2025-08-15T21:55:37.165614h9e3033c5‚x2025-08-15T21:55:37.275285h668ab307‚x2025-08-15T21:55:37.275510h7f7d9b3d‚x2025-08-15T21:55:37.275663h31a09252‚x2025-08-15T21:55:37.275806hed162c94‚x2025-08-15T21:55:37.275951h0cbb3e27‚x2025-08-15T21:55:37.276092h94b1153e‚x2025-08-15T21:55:37.276231hedd58062‚x2025-08-15T21:55:37.282466h6458097a‚x2025-08-15T21:55:37.282761ha7985dfe‚x2025-08-15T21:55:37.282862hcf772aed‚x2025-08-15T21:55:37.400983ha504c19d‚x2025-08-15T21:55:37.401191h064c8628‚x2025-08-15T21:55:37.401343h496e0fa6‚x2025-08-15T21:55:37.401530hd48632d5‚x2025-08-15T21:55:37.401691hcde139e2‚x2025-08-15T21:55:37.401859h27aaa621‚x2025-08-15T21:55:37.402008hc06fc9ae‚x2025-08-15T21:55:37.402180hf3133057‚x2025-08-15T21:55:37.402344hb9a334f9‚x2025-08-15T21:55:37.410675h3b377d4b‚x2025-08-15T21:55:37.515620he8ed9565‚x2025-08-15T21:55:37.515768ha5a799f1‚x2025-08-15T21:55:37.515829hddc2c9d3‚x2025-08-15T21:55:37.515886h374b0a3c‚x2025-08-15T21:55:37.515942he5cfebbe‚x2025-08-15T21:55:37.515994h8eab0a49‚x2025-08-15T21:55:37.516044h297fab3d‚x2025-08-15T21:55:37.516092h97a242d1‚x2025-08-15T21:55:37.516147hbdb64a5d‚x2025-08-15T21:55:37.516231h1a9f17e3‚x2025-08-15T21:55:37.624014hf40c4a59‚x2025-08-15T21:55:37.624183hfae60fd7‚x2025-08-15T21:55:37.624309h481a1889‚x2025-08-15T21:55:37.629727h6df7397d‚x2025-08-15T21:55:37.629936h2f26811f‚x2025-08-15T21:55:37.630106h91b1bf4c‚x2025-08-15T21:55:37.630249hc9df71c2‚x2025-08-15T21:55:37.630414h998aa459‚x2025-08-15T21:55:37.630554h17d89d0e‚x2025-08-15T21:55:37.630688h81acaa5e‚x2025-08-15T21:55:37.734988h5ad3b99d‚x2025-08-15T21:55:37.735194ha2147f50‚x2025-08-15T21:55:37.735339hb4d6371c‚x2025-08-15T21:55:37.748599hba081fae‚x2025-08-15T21:55:37.748776h9ecfc69a‚x2025-08-15T21:55:37.748926h1d26fa0f‚x2025-08-15T21:55:37.749058h4be7fe14‚x2025-08-15T21:55:37.749796hf7db4692‚x2025-08-15T21:55:37.749994hde787fb5‚x2025-08-15T21:55:37.750132hd957db6c‚x2025-08-15T21:55:37.867889ha6f03cf4‚x2025-08-15T21:55:37.868076h56e7a304‚x2025-08-15T21:55:37.868200h405e1f43‚x2025-08-15T21:55:37.868317hceb538a7‚x2025-08-15T21:55:37.868469hd00c6c06‚x2025-08-15T21:55:37.868600h99278293‚x2025-08-15T21:55:37.868784hf454f646‚x2025-08-15T21:55:37.868946h3cb610f1‚x2025-08-15T21:55:37.869094he58987a7‚x2025-08-15T21:55:37.869233hd571d119‚x2025-08-15T21:55:37.987992h9667f2ad‚x2025-08-15T21:55:37.988170hf1d2edba‚x2025-08-15T21:55:37.988298h11444271‚x2025-08-15T21:55:37.988470hfce0b53d‚x2025-08-15T21:55:37.988615hbeb6c08f‚x2025-08-15T21:55:37.988743h61e0f998‚x2025-08-15T21:55:37.988897h1d0af46d‚x2025-08-15T21:55:37.989023h4760a4ee‚x2025-08-15T21:55:37.989180hd233a8f6‚x2025-08-15T21:55:37.989326h0896ac85‚x2025-08-15T21:55:38.106254h39bb7efe‚x2025-08-15T21:55:38.106482h904ae4dc‚x2025-08-15T21:55:38.106966h92296ac4‚x2025-08-15T21:55:38.107497h9f3d2ac9‚x2025-08-15T21:55:38.107990hce707dee‚x2025-08-15T21:55:38.108159hea0709a8‚x2025-08-15T21:55:38.110081h516bd362‚x2025-08-15T21:55:38.112508h41fbbbc4‚x2025-08-15T21:55:38.112849h88e7ad4e‚x2025-08-15T21:55:38.113039h20ab45d6‚x2025-08-15T21:55:38.224240h37d66c68‚x2025-08-15T21:55:38.225309h48fa3a06‚x2025-08-15T21:55:38.227169hbf271d4a‚x2025-08-15T21:55:38.227455h857db328‚x2025-08-15T21:55:38.227588he5e81e7e‚x2025-08-15T21:55:38.227725hfd284f3b‚x2025-08-15T21:55:38.227852h4dc0aa75‚x2025-08-15T21:55:38.227970h2a56f557‚x2025-08-15T21:55:38.228094h94314bde‚x2025-08-15T21:55:38.228219h201f0f76‚x2025-08-15T21:55:39.882139hecdf6f24‚x2025-08-15T21:55:39.882262h3dda55e4‚x2025-08-15T21:55:39.882314hedf71064‚x2025-08-15T21:55:39.884518hb9abd98e‚x2025-08-15T21:55:39.884763hff9d9a24‚x2025-08-15T21:55:39.884840hffa35be9‚x2025-08-15T21:55:39.884924h8142dae1‚x2025-08-15T21:55:39.884977h421d4fbc‚x2025-08-15T21:55:39.885025h15e88642‚x2025-08-15T21:55:39.885073h82fa52f0‚x2025-08-15T21:55:39.989267hc9a7bd23‚x2025-08-15T21:55:39.990293hddd7bd54‚x2025-08-15T21:55:39.990420h8cb2f5cd‚x2025-08-15T21:55:39.990473h71515dff‚x2025-08-15T21:55:39.990518h17c7da1e‚x2025-08-15T21:55:39.990561h41984468‚x2025-08-15T21:55:39.990603h35d2849f‚x2025-08-15T21:55:39.990646he318e3be‚x2025-08-15T21:55:39.990683h68f0e412‚x2025-08-15T21:55:39.990719hced811f9‚x2025-08-15T21:55:40.096312hdd36bcd9‚x2025-08-15T21:55:40.096495h1784fe1c‚x2025-08-15T21:55:40.096547he7d70338‚x2025-08-15T21:55:40.096595h97568b1f‚x2025-08-15T21:55:40.096641hf1d363c7‚x2025-08-15T21:55:40.096687he99af4af‚x2025-08-15T21:55:40.096733h3b2412a1‚x2025-08-15T21:55:40.096777h090f567b‚x2025-08-15T21:55:40.096823h6a1a18b1‚x2025-08-15T21:55:40.096871h16f07c79‚x2025-08-15T21:55:40.198969h8aca9772‚x2025-08-15T21:55:40.199074haa412ff8‚x2025-08-15T21:55:40.199122h332fc516‚x2025-08-15T21:55:40.199165haad9d6d6‚x2025-08-15T21:55:40.199201h036e77b2‚x2025-08-15T21:55:40.199240ha2a1ba29‚x2025-08-15T21:55:40.199282hc0c5c072‚x2025-08-15T21:55:40.199327hc3882ffb‚x2025-08-15T21:55:40.199391h8740558c‚x2025-08-15T21:55:40.199439h563924f7‚x2025-08-15T21:55:40.305574h57a9c3c5‚x2025-08-15T21:55:40.305703h469db2d1‚x2025-08-15T21:55:40.305765h6a974e29‚x2025-08-15T21:55:40.305820h1deb672c‚x2025-08-15T21:55:40.305870h08ec95e1‚x2025-08-15T21:55:40.305931h605c1c93‚x2025-08-15T21:55:40.305984hb4fe84dc‚x2025-08-15T21:55:40.306033h299b6ece‚x2025-08-15T21:55:40.306086h60596bff‚x2025-08-15T21:55:40.306139ha2b6286c‚x2025-08-15T21:55:40.410491h1f2ef777‚x2025-08-15T21:55:40.410654h5fc03a18‚x2025-08-15T21:55:40.410729h586724cc‚x2025-08-15T21:55:40.410789h5e15e07b‚x2025-08-15T21:55:40.410848h125914b3‚x2025-08-15T21:55:40.410909h50c0f97d‚x2025-08-15T21:55:40.410965ha59d8ab1‚x2025-08-15T21:55:40.411024h2cff742a‚x2025-08-15T21:55:40.411081h7ae8fdfd‚x2025-08-15T21:55:40.411137h8384241a‚x2025-08-15T21:55:40.521335he537fce3‚x2025-08-15T21:55:40.521463h13af69a1‚x2025-08-15T21:55:40.521510h0d2a69b6‚x2025-08-15T21:55:40.521553h8e1ef5ba‚x2025-08-15T21:55:40.521596h41699431‚x2025-08-15T21:55:40.521642hdbe4c983‚x2025-08-15T21:55:40.521682h48f22c40‚x2025-08-15T21:55:40.521728hae572c52‚x2025-08-15T21:55:40.521770h3aefe0bd‚x2025-08-15T21:55:40.521825h4897fa83‚x2025-08-15T21:55:40.624889h952ff51a‚x2025-08-15T21:55:40.624960h4c5f2b0d‚x2025-08-15T21:55:40.624994h1c093cd5‚x2025-08-15T21:55:40.625026hd3eb2346‚x2025-08-15T21:55:40.625057h3ed922b6‚x2025-08-15T21:55:40.625094hff76aa33‚x2025-08-15T21:55:40.625129h1f534770‚x2025-08-15T21:55:40.625187he18480f4‚x2025-08-15T21:55:40.625230h658da9c0‚x2025-08-15T21:55:40.625265h3e03a29c‚x2025-08-15T21:55:40.726270h1a15c2a9‚x2025-08-15T21:55:40.726335ha4814b0e‚x2025-08-15T21:55:40.726413h83839ed4‚x2025-08-15T21:55:40.726454he78695e3‚x2025-08-15T21:55:40.726497h373fcf47‚x2025-08-15T21:55:40.726531h2961d877‚x2025-08-15T21:55:40.726564hbbc469da‚x2025-08-15T21:55:40.726599hb4896b1c‚x2025-08-15T21:55:40.726632ha3766507‚x2025-08-15T21:55:40.726666h3417650e‚x2025-08-15T21:55:40.829859h096c6475‚x2025-08-15T21:55:40.829961h74c3c087‚x2025-08-15T21:55:40.829998hb2cfceb5‚x2025-08-15T21:55:40.830039hb6e98338‚x2025-08-15T21:55:40.830078h4dd8e026‚x2025-08-15T21:55:40.830116h0a0ab5f8‚x2025-08-15T21:55:40.830154h58459f53‚x2025-08-15T21:55:40.830194h0366f433‚x2025-08-15T21:55:40.830235h9d6e4d3f‚x2025-08-15T21:55:40.830273h1e6d4f36‚x2025-08-15T21:55:40.935278hba35e905‚x2025-08-15T21:55:40.935455h66b20bf7‚x2025-08-15T21:55:40.935513h19623c84‚x2025-08-15T21:55:40.935569hbe96116c‚x2025-08-15T21:55:40.935613hf9c3b942‚x2025-08-15T21:55:40.935654h73dac9ce‚x2025-08-15T21:55:40.935698hc9b62fd4‚x2025-08-15T21:55:40.935739h9f87a6a7‚x2025-08-15T21:55:40.935778h051c5196‚x2025-08-15T21:55:40.935823hd9fd83f5‚x2025-08-15T21:55:41.036942h185b7e8f‚x2025-08-15T21:55:41.037043ha7d81de2‚x2025-08-15T21:55:41.037087he8eaf6e0‚x2025-08-15T21:55:41.037161h7af46ca2‚x2025-08-15T21:55:41.037203h10ef2481‚x2025-08-15T21:55:41.037239ha6ba231f‚x2025-08-15T21:55:41.037274hd941efcd‚x2025-08-15T21:55:41.037309h077686be‚x2025-08-15T21:55:41.037342h209c6844‚x2025-08-15T21:55:41.037397h3aaa000c‚x2025-08-15T21:55:41.139650he0f10bbc‚x2025-08-15T21:55:41.139759hb1269843‚x2025-08-15T21:55:41.139802hf72db686‚x2025-08-15T21:55:41.139842hd1fe87f1‚x2025-08-15T21:55:41.139889h1e918336‚x2025-08-15T21:55:41.139927h63a8d03a‚x2025-08-15T21:55:41.139963h55118728‚x2025-08-15T21:55:41.140038hf5eb5dfb‚x2025-08-15T21:55:41.140085h9713be18‚x2025-08-15T21:55:41.140127hb2d8bb8d‚x2025-08-15T21:55:41.241625hff1e0d59‚x2025-08-15T21:55:41.241782h9c194a4f‚x2025-08-15T21:55:41.241843h9e10b0e9‚x2025-08-15T21:55:41.241890hfb5fec64‚x2025-08-15T21:55:41.241942h14b11fff‚x2025-08-15T21:55:41.241990hd05313d8‚x2025-08-15T21:55:41.242040h28b7f96c‚x2025-08-15T21:55:41.242089heb1b1693‚x2025-08-15T21:55:41.242134hc1bf9ad8‚x2025-08-15T21:55:41.242182h0c7936e5‚x2025-08-15T21:55:41.344976h96074302‚x2025-08-15T21:55:41.345047h1f4ee799‚x2025-08-15T21:55:41.345092h07ddab86‚x2025-08-15T21:55:41.345133h30b1dd87‚x2025-08-15T21:55:41.345190h24895fbc‚x2025-08-15T21:55:41.345235h8870bde8‚x2025-08-15T21:55:41.345274h348cd205‚x2025-08-15T21:55:41.345320hacfab479‚x2025-08-15T21:55:41.345374h6ff58116‚x2025-08-15T21:55:41.345436hb4b98270‚x2025-08-15T21:55:41.449078h77970e96‚x2025-08-15T21:55:41.449197hd3bf4c39‚x2025-08-15T21:55:41.449256h6b8f0f82‚x2025-08-15T21:55:41.449305h45072803‚x2025-08-15T21:55:41.449839h3b7d9709‚x2025-08-15T21:55:41.449988h5d2c835f‚x2025-08-15T21:55:41.450039h7ff0afac‚x2025-08-15T21:55:41.450083h8f208656‚x2025-08-15T21:55:41.450124hde82cb77‚x2025-08-15T21:55:41.450162h8ce79c68‚x2025-08-15T21:55:41.555456h79fe24d3‚x2025-08-15T21:55:41.555518h73599210‚x2025-08-15T21:55:41.555558h17e78bfe‚x2025-08-15T21:55:41.555597h167d3d44‚x2025-08-15T21:55:41.555633h09cc9d07‚x2025-08-15T21:55:41.555666hdf97e1c1‚x2025-08-15T21:55:41.555699h4ea7d7b6‚x2025-08-15T21:55:41.555732h313ce712‚x2025-08-15T21:55:41.555766h40fb462e‚x2025-08-15T21:55:41.555799h1e494408‚x2025-08-15T21:55:41.657776hd2db35d2‚x2025-08-15T21:55:41.657841hc7aae0ca‚x2025-08-15T21:55:41.657880h3e7ec7f2‚x2025-08-15T21:55:41.657921h1c6e6b6f‚x2025-08-15T21:55:41.657959h4fb3fa7a‚x2025-08-15T21:55:41.658001h609a1242‚x2025-08-15T21:55:41.658042h2ca98e94‚x2025-08-15T21:55:41.658082ha1e30736‚x2025-08-15T21:55:41.658118hb545a808‚x2025-08-15T21:55:41.658152hcd150ff3‚x2025-08-15T21:55:41.760059h68bafc1a‚x2025-08-15T21:55:41.760197he2825bce‚x2025-08-15T21:55:41.760258hb9ec6244‚x2025-08-15T21:55:41.760298h8a1dce17‚x2025-08-15T21:55:41.760337h01ec9078‚x2025-08-15T21:55:41.760411hd0b6a1bf‚x2025-08-15T21:55:41.760452he32c0560‚x2025-08-15T21:55:41.760490h65ad67aa‚x2025-08-15T21:55:41.760530h5fb240e9‚x2025-08-15T21:55:41.760573hb82ab873‚x2025-08-15T21:55:41.861230hf6b643d7‚x2025-08-15T21:55:41.861297ha0269c02‚x2025-08-15T21:55:41.861320ha92ed5a9‚x2025-08-15T21:55:41.861342h987bc975‚x2025-08-15T21:55:41.861390h43d95ca0‚x2025-08-15T21:55:41.861414h5c7332bd‚x2025-08-15T21:55:41.861435hb1e34163‚x2025-08-15T21:55:41.861457h25068f32‚x2025-08-15T21:55:41.861478h4505b518‚x2025-08-15T21:55:41.861531h405abeec‚x2025-08-15T21:55:43.456409h39d64629‚x2025-08-15T21:55:43.456533h636ec8ea‚x2025-08-15T21:55:43.456576hcaa92def‚x2025-08-15T21:55:43.456613hf1eba0c7‚x2025-08-15T21:55:43.456651h04fead55‚x2025-08-15T21:55:43.456686ha38b0821‚x2025-08-15T21:55:43.456722h79ff4954‚x2025-08-15T21:55:43.456756hcec30013‚x2025-08-15T21:55:43.456789h5763284e‚x2025-08-15T21:55:43.456826ha096586e‚x2025-08-15T21:55:43.578531h404e0887‚x2025-08-15T21:55:43.580814h08ba6e08‚x2025-08-15T21:55:43.581114h5b87554b‚x2025-08-15T21:55:43.581288hba46228e‚x2025-08-15T21:55:43.581506hc0177ce3‚x2025-08-15T21:55:43.581693h37279354‚x2025-08-15T21:55:43.581871h2dff8b9a‚x2025-08-15T21:55:43.582036hb5451537‚x2025-08-15T21:55:43.588402hc77a4337‚x2025-08-15T21:55:43.588770h137bc357‚x2025-08-15T21:55:43.708346h67a2ddc2‚x2025-08-15T21:55:43.708566h10f0102e‚x2025-08-15T21:55:43.708708hcb1eb5ae‚x2025-08-15T21:55:43.709015he3df7224‚x2025-08-15T21:55:43.709171ha3344536‚x2025-08-15T21:55:43.709376hfeb92a0f‚x2025-08-15T21:55:43.709552hac9d3813‚x2025-08-15T21:55:43.709708h2ef27601‚x2025-08-15T21:55:43.709866h9881b0bb‚x2025-08-15T21:55:43.709994h895ef5cf‚x2025-08-15T21:55:43.812404h138c7df0‚x2025-08-15T21:55:43.812497h50ef48e7‚x2025-08-15T21:55:43.812543haccc5cc4‚x2025-08-15T21:55:43.812584h03e66adb‚x2025-08-15T21:55:43.812625hb8691112‚x2025-08-15T21:55:43.812666h04b84b05‚x2025-08-15T21:55:43.812707h5acb2fb8‚x2025-08-15T21:55:43.812758h5804b202‚x2025-08-15T21:55:43.812830h27f2c100‚x2025-08-15T21:55:43.812993h3f2b5ea5‚x2025-08-15T21:55:43.914449ha1c210b8‚x2025-08-15T21:55:43.914566he085ea17‚x2025-08-15T21:55:43.914621he6ec73be‚x2025-08-15T21:55:43.914671hd81c4c9c‚x2025-08-15T21:55:43.914720hff6c21e0‚x2025-08-15T21:55:43.914775h26372c99‚x2025-08-15T21:55:43.914821he16d5707‚x2025-08-15T21:55:43.914884h670054b1‚x2025-08-15T21:55:43.914930h6dc0ca79‚x2025-08-15T21:55:43.914983hba4be1f8‚x2025-08-15T21:55:44.016180h813ef624‚x2025-08-15T21:55:44.016337h957b1318‚x2025-08-15T21:55:44.016430h0c65aa21‚x2025-08-15T21:55:44.016501h599b2b7d‚x2025-08-15T21:55:44.016554h9608fb72‚x2025-08-15T21:55:44.016612h67d77336‚x2025-08-15T21:55:44.016668h4521b73e‚x2025-08-15T21:55:44.016722he29bd079‚x2025-08-15T21:55:44.016772hd974f750‚x2025-08-15T21:55:44.016825h39eed378‚x2025-08-15T21:55:44.118372h8d43542f‚x2025-08-15T21:55:44.118552h53b4e7d6‚x2025-08-15T21:55:44.118613hdea3041a‚x2025-08-15T21:55:44.118662h90b00d04‚x2025-08-15T21:55:44.118708h3d99813e‚x2025-08-15T21:55:44.118756h495a3476‚x2025-08-15T21:55:44.118802he3df0a8d‚x2025-08-15T21:55:44.118839he39de6ff‚x2025-08-15T21:55:44.118878hd34d2c69‚x2025-08-15T21:55:44.118915h598bbf5b‚x2025-08-15T21:55:44.222628h491b79e6‚x2025-08-15T21:55:44.222813h58b9e1a4‚x2025-08-15T21:55:44.222888h89dcd735‚x2025-08-15T21:55:44.222955hf4fdcb8c‚x2025-08-15T21:55:44.223014hf0e8c2ff‚x2025-08-15T21:55:44.223074h88b73f2a‚x2025-08-15T21:55:44.223131hc46fd598‚x2025-08-15T21:55:44.223216h2df21faf‚x2025-08-15T21:55:44.223261hc4f8ff60‚x2025-08-15T21:55:44.223318hfd43ee8a‚x2025-08-15T21:55:44.324397h5d88992a‚x2025-08-15T21:55:44.324618h5ec9d5a1‚x2025-08-15T21:55:44.324700hdfbb42de‚x2025-08-15T21:55:44.324766hc6c60fc5‚x2025-08-15T21:55:44.324823hbf949688‚x2025-08-15T21:55:44.324880hf74efb5c‚x2025-08-15T21:55:44.324935h6a33c0d4‚x2025-08-15T21:55:44.325004h823e913d‚x2025-08-15T21:55:44.325069h27dcbbac‚x2025-08-15T21:55:44.325177hfba7760b‚x2025-08-15T21:55:44.427858hcbdccf2e‚x2025-08-15T21:55:44.428012h9c95a279‚x2025-08-15T21:55:44.428081h0303792e‚x2025-08-15T21:55:44.428152h6a7db2ac‚x2025-08-15T21:55:44.428201h6840b07d‚x2025-08-15T21:55:44.428373ha3fa6f57‚x2025-08-15T21:55:44.428502h30012412‚x2025-08-15T21:55:44.428551ha3525187‚x2025-08-15T21:55:44.428651h46e1cf2a‚x2025-08-15T21:55:44.428703hb1a27f25‚x2025-08-15T21:55:44.531748hc8278d88‚x2025-08-15T21:55:44.532033h7dabf220‚x2025-08-15T21:55:44.532117h03f0f7c6‚x2025-08-15T21:55:44.532169h21c7ff29‚x2025-08-15T21:55:44.532272h1df3e070‚x2025-08-15T21:55:44.532678h17042795‚x2025-08-15T21:55:44.532845hf9cc1b14‚x2025-08-15T21:55:44.532912h7a1a5748‚x2025-08-15T21:55:44.532958h3df81c63‚x2025-08-15T21:55:44.533020h9bed2fed‚x2025-08-15T21:55:44.634326h1da9cc09‚x2025-08-15T21:55:44.634482h883b1fcb‚x2025-08-15T21:55:44.634534h28b0c149‚x2025-08-15T21:55:44.634591hb5a69d43‚x2025-08-15T21:55:44.634660hff54ec01‚x2025-08-15T21:55:44.634737h6289c6f2‚x2025-08-15T21:55:44.634805h783419a0‚x2025-08-15T21:55:44.634863h58c55881‚x2025-08-15T21:55:44.634929h4d8e67dc‚x2025-08-15T21:55:44.634977hadbfd692‚x2025-08-15T21:55:44.736559h2a6036ae‚x2025-08-15T21:55:44.736728h14d28a92‚x2025-08-15T21:55:44.736792h94b99976‚x2025-08-15T21:55:44.736846h1bfcc978‚x2025-08-15T21:55:44.736896hffe40b12‚x2025-08-15T21:55:44.736953h91a52382‚x2025-08-15T21:55:44.737002ha7966fab‚x2025-08-15T21:55:44.737053ha365195b‚x2025-08-15T21:55:44.737109ha0d7cf9f‚x2025-08-15T21:55:44.737206hf8aac569‚x2025-08-15T21:55:44.839714h4d8b84e7‚x2025-08-15T21:55:44.839890h161aa9c8‚x2025-08-15T21:55:44.839988h481971cb‚x2025-08-15T21:55:44.840063h9c0a1b96‚x2025-08-15T21:55:44.840115h55f2bd1f‚x2025-08-15T21:55:44.840172h3e01540b‚x2025-08-15T21:55:44.842572h0d989f8a‚x2025-08-15T21:55:44.842869h3f343af2‚x2025-08-15T21:55:44.842977hc835a751‚x2025-08-15T21:55:44.843040h13b4c3d9‚x2025-08-15T21:55:44.944903hcce9fa28‚x2025-08-15T21:55:44.945014h0cb81937‚x2025-08-15T21:55:44.945070h0363b32e‚x2025-08-15T21:55:44.945116h049c9279‚x2025-08-15T21:55:44.945178h944d5cfd‚x2025-08-15T21:55:44.945234ha7c32103‚x2025-08-15T21:55:44.945281h6aa6692e‚x2025-08-15T21:55:44.945324hf19650cc‚x2025-08-15T21:55:44.945375he3738f37‚x2025-08-15T21:55:44.945425hd5811d80‚x2025-08-15T21:55:45.046692h8714cafc‚x2025-08-15T21:55:45.046820hb860326a‚x2025-08-15T21:55:45.046881hbbd09f76‚x2025-08-15T21:55:45.046913hb4af3b05‚x2025-08-15T21:55:45.046938hd650c64a‚x2025-08-15T21:55:45.046981h146b755d‚x2025-08-15T21:55:45.047008hd7060508‚x2025-08-15T21:55:45.047035hefe838ff‚x2025-08-15T21:55:45.047072h0efb8905‚x2025-08-15T21:55:45.047110h3746d02b‚x2025-08-15T21:55:45.148428hdaa79d1b‚x2025-08-15T21:55:45.148495h59c212c3‚x2025-08-15T21:55:45.148541heccb9458‚x2025-08-15T21:55:45.148582haa8957e9‚x2025-08-15T21:55:45.148620hd8dcf7ff‚x2025-08-15T21:55:45.148661h0532ccfc‚x2025-08-15T21:55:45.148702hd4a6deb5‚x2025-08-15T21:55:45.148738h80e1cd3a‚x2025-08-15T21:55:45.148772hf96d5ed1‚x2025-08-15T21:55:45.148831h0e4328c6‚x2025-08-15T21:55:45.253983hcb186307‚x2025-08-15T21:55:45.254270h107f7f5f‚x2025-08-15T21:55:45.254490hac4ef7d5‚x2025-08-15T21:55:45.254695h817bb5b8‚x2025-08-15T21:55:45.254868ha4eaee2a‚x2025-08-15T21:55:45.255020h17c88d25‚x2025-08-15T21:55:45.255232h153ee5a5‚x2025-08-15T21:55:45.255665hb012e7d4‚x2025-08-15T21:55:45.255862h5abb8e9c‚x2025-08-15T21:55:45.256008h35d66639‚x2025-08-15T21:55:45.364308h7f5c75f4‚x2025-08-15T21:55:45.364551h89a6fcca‚x2025-08-15T21:55:45.364722hef73584b‚x2025-08-15T21:55:45.364885hfbe1b1b9‚x2025-08-15T21:55:45.365070h75a77d7a‚x2025-08-15T21:55:45.365256h53a64e89‚x2025-08-15T21:55:45.365459hcbfe6a59‚x2025-08-15T21:55:45.365640h01bb9a3c‚x2025-08-15T21:55:45.365815hfb13c213‚x2025-08-15T21:55:45.365997h97d491a7‚x2025-08-15T21:55:45.467184h13df8833‚x2025-08-15T21:55:45.467280h17700baf‚x2025-08-15T21:55:45.467330h58b8dbb5‚x2025-08-15T21:55:45.467374h7229dcd3‚x2025-08-15T21:55:45.467406h7a647264‚x2025-08-15T21:55:45.467428hbc0aac03‚x2025-08-15T21:55:45.467458h1b7d167d‚x2025-08-15T21:55:45.467489h209646ea‚x2025-08-15T21:55:45.467515h80712b6d‚x2025-08-15T21:55:45.467553h1e056049‚x2025-08-15T21:55:46.952814h0ff940f5‚x2025-08-15T21:55:46.952985h0e4965c4‚x2025-08-15T21:55:46.953061h4e417c0a‚x2025-08-15T21:55:46.953127h94cdee28‚x2025-08-15T21:55:46.953191h10e1ddc2‚x2025-08-15T21:55:46.953251hf1f0e02f‚x2025-08-15T21:55:46.953317hc84562c6‚x2025-08-15T21:55:46.953398hd5026261‚x2025-08-15T21:55:46.953463h09dc1198‚x2025-08-15T21:55:46.953519h959dee39‚x2025-08-15T21:55:47.054538ha0f9103c‚x2025-08-15T21:55:47.054672h115e6e95‚x2025-08-15T21:55:47.054741h8b5be2f8‚x2025-08-15T21:55:47.054804h4ef744e5‚x2025-08-15T21:55:47.054859h369f54fe‚x2025-08-15T21:55:47.054914h2db17d28‚x2025-08-15T21:55:47.054964haa960a61‚x2025-08-15T21:55:47.055013he839445a‚x2025-08-15T21:55:47.055064h073ed21a‚x2025-08-15T21:55:47.055111hb0686c06‚x2025-08-15T21:55:47.156018he4806c24‚x2025-08-15T21:55:47.156118h8bc059e6‚x2025-08-15T21:55:47.156166h058dd09b‚x2025-08-15T21:55:47.156220h881c2414‚x2025-08-15T21:55:47.156266h43b833a6‚x2025-08-15T21:55:47.156311h211ba7ff‚x2025-08-15T21:55:47.156370h66dfc872‚x2025-08-15T21:55:47.156417h62f644a4‚x2025-08-15T21:55:47.156458h8b8b2629‚x2025-08-15T21:55:47.156502ha3a464d3‚x2025-08-15T21:55:47.257934h6b095595‚x2025-08-15T21:55:47.258036h7b012ced‚x2025-08-15T21:55:47.258079hf3035305‚x2025-08-15T21:55:47.258118ha038b940‚x2025-08-15T21:55:47.258166h41e06c33‚x2025-08-15T21:55:47.258205h7d177519‚x2025-08-15T21:55:47.258241hdf372fca‚x2025-08-15T21:55:47.258271he585acd5‚x2025-08-15T21:55:47.258300hde16f7dc‚x2025-08-15T21:55:47.258344h7cc60f7b‚x2025-08-15T21:55:47.361184hdefbc6b3‚x2025-08-15T21:55:47.361345h530f5861‚x2025-08-15T21:55:47.361435h835d1e43‚x2025-08-15T21:55:47.361492hfb98d47f‚x2025-08-15T21:55:47.361553h938f3369‚x2025-08-15T21:55:47.361609hcde5c869‚x2025-08-15T21:55:47.361665heb969e14‚x2025-08-15T21:55:47.361727h2d50dba7‚x2025-08-15T21:55:47.361778h67fb7906‚x2025-08-15T21:55:47.361832h340027bf‚x2025-08-15T21:55:47.463544h60f33b12‚x2025-08-15T21:55:47.463696hc91d291d‚x2025-08-15T21:55:47.463767h3618c9f3‚x2025-08-15T21:55:47.463827h486f7bc4‚x2025-08-15T21:55:47.463877hbf6d7d26‚x2025-08-15T21:55:47.463967hb9b57cfd‚x2025-08-15T21:55:47.464027hed037549‚x2025-08-15T21:55:47.464092hd26f4438‚x2025-08-15T21:55:47.464146h54b1a15e‚x2025-08-15T21:55:47.464196h8e846fd9‚x2025-08-15T21:55:47.564814h496cffb1‚x2025-08-15T21:55:47.564898h01dbdef3‚x2025-08-15T21:55:47.564938h9e430b70‚x2025-08-15T21:55:47.564975he7144f12‚x2025-08-15T21:55:47.565013h4a06e728‚x2025-08-15T21:55:47.565050ha0816546‚x2025-08-15T21:55:47.565109ha487515a‚x2025-08-15T21:55:47.565146hf05d621c‚x2025-08-15T21:55:47.565182h9286e22c‚x2025-08-15T21:55:47.565220h8b4cb08b‚x2025-08-15T21:55:47.667195hb8692206‚x2025-08-15T21:55:47.667299h4fdaebff‚x2025-08-15T21:55:47.667376h109f38ef‚x2025-08-15T21:55:47.667432h5383ed50‚x2025-08-15T21:55:47.667484h8f35967b‚x2025-08-15T21:55:47.667549h9c333356‚x2025-08-15T21:55:47.667605he1bf3449‚x2025-08-15T21:55:47.667653h2b426f10‚x2025-08-15T21:55:47.667699h91bd4f14‚x2025-08-15T21:55:47.667743hc645ae53‚x2025-08-15T21:55:47.774106hf535cddd‚x2025-08-15T21:55:47.774444ha25b7f7c‚x2025-08-15T21:55:47.774665h10410773‚x2025-08-15T21:55:47.774835h51085d16‚x2025-08-15T21:55:47.774976h06f608c3‚x2025-08-15T21:55:47.775107h6561bee7‚x2025-08-15T21:55:47.775250hbafe38d5‚x2025-08-15T21:55:47.775421heedb5711‚x2025-08-15T21:55:47.775587h425abf85‚x2025-08-15T21:55:47.775732hce5a83e2‚x2025-08-15T21:55:47.876704h72598145‚x2025-08-15T21:55:47.876793h3762ca53‚x2025-08-15T21:55:47.876834h068a260b‚x2025-08-15T21:55:47.876910h4be70d71‚x2025-08-15T21:55:47.876954hb3f1add8‚x2025-08-15T21:55:47.876994hc57c529a‚x2025-08-15T21:55:47.877033hafad4f0e‚x2025-08-15T21:55:47.877072h61df2fe7‚x2025-08-15T21:55:47.877114h6fee8f4b‚x2025-08-15T21:55:47.877156hb268e189‚x2025-08-15T21:55:49.622456h934fb1ff‚x2025-08-15T21:55:49.622574h6d1a4dbd‚x2025-08-15T21:55:49.622617h653f474c‚x2025-08-15T21:55:49.622666ha532e475‚x2025-08-15T21:55:49.622721h35f915fd‚x2025-08-15T21:55:49.622798hd7ed6f65‚x2025-08-15T21:55:49.622840h7244990d‚x2025-08-15T21:55:49.622888h0ce57deb‚x2025-08-15T21:55:49.622973h9f2cc070‚x2025-08-15T21:55:49.623034ha53c9272‚x2025-08-15T21:55:49.724201h0e067f88‚x2025-08-15T21:55:49.724347h636e8b65‚x2025-08-15T21:55:49.724437h4a7edc9a‚x2025-08-15T21:55:49.724491h0ba89b96‚x2025-08-15T21:55:49.724550he0612696‚x2025-08-15T21:55:49.724593h6d10c22c‚x2025-08-15T21:55:49.724638h1bcc35f0‚x2025-08-15T21:55:49.724683h2e6a5e8d‚x2025-08-15T21:55:49.724767hf71ebe09‚x2025-08-15T21:55:49.724830h4ea812f7‚x2025-08-15T21:55:49.826611h9a7c347e‚x2025-08-15T21:55:49.826659h8506ed54‚x2025-08-15T21:55:49.826684hc23e83b9‚x2025-08-15T21:55:49.826710hc17ce4f2‚x2025-08-15T21:55:49.826737hb267b716‚x2025-08-15T21:55:49.826765hf652d1e1‚x2025-08-15T21:55:49.826789h487d0ccf‚x2025-08-15T21:55:49.826814h380c30c6‚x2025-08-15T21:55:49.826836h0b4df765‚x2025-08-15T21:55:49.826859h1b1a836e‚x2025-08-15T21:55:49.928561h47a6a8d7‚x2025-08-15T21:55:49.928663hd1086a98‚x2025-08-15T21:55:49.928711h2235e1a0‚x2025-08-15T21:55:49.928757hc2718cce‚x2025-08-15T21:55:49.928806h044e9e36‚x2025-08-15T21:55:49.928850hdb8987a0‚x2025-08-15T21:55:49.928893h4a22de62‚x2025-08-15T21:55:49.928936h35681036‚x2025-08-15T21:55:49.928980h204b2b12‚x2025-08-15T21:55:49.929023h2511a83b‚x2025-08-15T21:55:50.032738h3d033625‚x2025-08-15T21:55:50.032886h77a3b01b‚x2025-08-15T21:55:50.032960ha59fde98‚x2025-08-15T21:55:50.033017h26c0e830‚x2025-08-15T21:55:50.033071h520f3969‚x2025-08-15T21:55:50.033128ha74273cf‚x2025-08-15T21:55:50.033182hc2e48647‚x2025-08-15T21:55:50.033235h6c144108‚x2025-08-15T21:55:50.033291ha3854511‚x2025-08-15T21:55:50.033344h09f97d11‚x2025-08-15T21:55:50.136698hf640e598‚x2025-08-15T21:55:50.136778h38dcaf41‚x2025-08-15T21:55:50.136812h3baa7af7‚x2025-08-15T21:55:50.136843h12409f77‚x2025-08-15T21:55:50.136873hcad7f0a6‚x2025-08-15T21:55:50.136903h90fece65‚x2025-08-15T21:55:50.136932h803faac8‚x2025-08-15T21:55:50.136960he8465b04‚x2025-08-15T21:55:50.136987h5c0689a6‚x2025-08-15T21:55:50.137014hf0ad6ace‚x2025-08-15T21:55:50.237915h461efd26‚x2025-08-15T21:55:50.238007h27b95fbf‚x2025-08-15T21:55:50.238054h5ef9576f‚x2025-08-15T21:55:50.238088hfb9d5633‚x2025-08-15T21:55:50.238124h60e4e415‚x2025-08-15T21:55:50.238160h9795611e‚x2025-08-15T21:55:50.238196h9d687590‚x2025-08-15T21:55:50.238219h99f8eed4‚x2025-08-15T21:55:50.238238h4195fb8d‚x2025-08-15T21:55:50.238269h064771a1‚x2025-08-15T21:55:50.339088h1200747d‚x2025-08-15T21:55:50.339178h834aa77b‚x2025-08-15T21:55:50.339209h01a89dcd‚x2025-08-15T21:55:50.339234h6921726d‚x2025-08-15T21:55:50.339262h3e0f6a81‚x2025-08-15T21:55:50.339299hbcf0fd1d‚x2025-08-15T21:55:50.339331h8da951ac‚x2025-08-15T21:55:50.339371h8876aeeb‚x2025-08-15T21:55:50.339409he7576a17‚x2025-08-15T21:55:50.339435hdc1c35d9‚x2025-08-15T21:55:50.440651h9ccc75bc‚x2025-08-15T21:55:50.440734h6d79a13c‚x2025-08-15T21:55:50.440778h4afd9db1‚x2025-08-15T21:55:50.440824hd9429537‚x2025-08-15T21:55:50.440853hae3022ca‚x2025-08-15T21:55:50.440876h7081a93d‚x2025-08-15T21:55:50.440897h8d2b900f‚x2025-08-15T21:55:50.440934h46181b2d‚x2025-08-15T21:55:50.440957h5b77bdc5‚x2025-08-15T21:55:50.440978hdeabcb8f‚x2025-08-15T21:55:50.545044h47488521‚x2025-08-15T21:55:50.545170hf8cfe176‚x2025-08-15T21:55:50.545209h8d08586a‚x2025-08-15T21:55:50.545246h693a400f‚x2025-08-15T21:55:50.545277ha2875344‚x2025-08-15T21:55:50.545308h59a94040‚x2025-08-15T21:55:50.545375h3fec5ae7‚x2025-08-15T21:55:50.545442hc9ace212‚x2025-08-15T21:55:50.545501h81b56020‚x2025-08-15T21:55:50.545553h36519736‚x2025-08-15T21:55:50.647306h32b37c90‚x2025-08-15T21:55:50.647391h29f18fec‚x2025-08-15T21:55:50.647436h5261b8de‚x2025-08-15T21:55:50.647482hd271790e‚x2025-08-15T21:55:50.647522hb1542f2b‚x2025-08-15T21:55:50.647557hbdf65304‚x2025-08-15T21:55:50.647592hbece721a‚x2025-08-15T21:55:50.647630he328c618‚x2025-08-15T21:55:50.647716heef24026‚x2025-08-15T21:55:50.647844h290f9813‚x2025-08-15T21:55:50.749753h662d3c41‚x2025-08-15T21:55:50.749914h695404cb‚x2025-08-15T21:55:50.749952h3afcd975‚x2025-08-15T21:55:50.749991hd0f779a7‚x2025-08-15T21:55:50.750023h2d0b0b94‚x2025-08-15T21:55:50.750050h2d622b51‚x2025-08-15T21:55:50.750075h49d00757‚x2025-08-15T21:55:50.750105hb1168782‚x2025-08-15T21:55:50.750132hf96fe09e‚x2025-08-15T21:55:50.750162habaafd78‚x2025-08-15T21:55:50.851918hdb859436‚x2025-08-15T21:55:50.852046ha96c1b16‚x2025-08-15T21:55:50.852114haf083f8f‚x2025-08-15T21:55:50.852170h1dc4cbf4‚x2025-08-15T21:55:50.852220hbffef1b2‚x2025-08-15T21:55:50.852272h0395ff50‚x2025-08-15T21:55:50.852323h49f7859a‚x2025-08-15T21:55:50.852400h6aba22c4‚x2025-08-15T21:55:50.852636hb5d7bf1e‚x2025-08-15T21:55:50.852694h2f28f38c‚x2025-08-15T21:55:50.955220h4c7a5582‚x2025-08-15T21:55:50.955382h18dc739e‚x2025-08-15T21:55:50.955463hc8101603‚x2025-08-15T21:55:50.955495hd0ed4d4d‚x2025-08-15T21:55:50.955521h514d55f7‚x2025-08-15T21:55:50.955550he8c0f0a4‚x2025-08-15T21:55:50.955579h44262d43‚x2025-08-15T21:55:50.955607h6e0b0826‚x2025-08-15T21:55:50.955635h188ae81c‚x2025-08-15T21:55:50.955660h7b7396d3‚x2025-08-15T21:55:51.057567h5782d109‚x2025-08-15T21:55:51.057750h33ab9611‚x2025-08-15T21:55:51.057819h9d2c6bb9‚x2025-08-15T21:55:51.057875he423fef8‚x2025-08-15T21:55:51.057930hd6cf0336‚x2025-08-15T21:55:51.057991ha943ae81‚x2025-08-15T21:55:51.058058he3545e5b‚x2025-08-15T21:55:51.058117hb557c1a4‚x2025-08-15T21:55:51.058179h2125cae0‚x2025-08-15T21:55:51.058247hb976a02b‚x2025-08-15T21:55:51.159236h3cf692c9‚x2025-08-15T21:55:51.159433h1525f90d‚x2025-08-15T21:55:51.159507hebf6c033‚x2025-08-15T21:55:51.159562h4a36d6f6‚x2025-08-15T21:55:51.159617h3f0c13b7‚x2025-08-15T21:55:51.159721h3519680b‚x2025-08-15T21:55:51.159791h68e97f36‚x2025-08-15T21:55:51.159838h41e06a55‚x2025-08-15T21:55:51.159878had9eab3c‚x2025-08-15T21:55:51.159941h6a7102f6‚x2025-08-15T21:55:51.261385h8c66bce4‚x2025-08-15T21:55:51.261483h6756e675‚x2025-08-15T21:55:51.261533he02e8e20‚x2025-08-15T21:55:51.261599hf4e0f7cd‚x2025-08-15T21:55:51.261646h90278f0b‚x2025-08-15T21:55:51.261687h5dd0c44a‚x2025-08-15T21:55:51.261727h18061b59‚x2025-08-15T21:55:51.261766hf7012081‚x2025-08-15T21:55:51.261806h49584a8e‚x2025-08-15T21:55:51.261860h7c01fb37‚x2025-08-15T21:55:51.363243h147e6a31‚x2025-08-15T21:55:51.363410h68839f1f‚x2025-08-15T21:55:51.363482hace86cee‚x2025-08-15T21:55:51.363531hb7f958d3‚x2025-08-15T21:55:51.363589ha960f412‚x2025-08-15T21:55:51.363651h69e76036‚x2025-08-15T21:55:51.363699hdbc2253a‚x2025-08-15T21:55:51.363745h12ade612‚x2025-08-15T21:55:51.363787h2d0a581b‚x2025-08-15T21:55:51.363833h0aebe6fe‚x2025-08-15T21:55:51.465244h9e7d5638‚x2025-08-15T21:55:51.465412h55d03efa‚x2025-08-15T21:55:51.465459h4978076c‚x2025-08-15T21:55:51.465492h5f7ed4a0‚x2025-08-15T21:55:51.465519h8c4abe12‚x2025-08-15T21:55:51.465547h5ac36d4a‚x2025-08-15T21:55:51.465622hfdec90f5‚x2025-08-15T21:55:51.465677h0c3dce8b‚x2025-08-15T21:55:51.465704h2407a312‚x2025-08-15T21:55:51.465732h8be322a5‚x2025-08-15T21:55:51.568087h5cd3cec6‚x2025-08-15T21:55:51.568220had33428b‚x2025-08-15T21:55:51.568276h2e8eba7d‚x2025-08-15T21:55:51.568325h1357a5ae‚x2025-08-15T21:55:51.568385h0507ae7b‚x2025-08-15T21:55:51.568437hb35df7bb‚x2025-08-15T21:55:51.568475hf4cfbcb2‚x2025-08-15T21:55:51.568514hcddbcbb5‚x2025-08-15T21:55:51.568552h376a003a‚x2025-08-15T21:55:51.568591hbd847383hmetadata£ktotal_atomsRpexport_timestampûAÚ(	ó‘;gversione0.1.0