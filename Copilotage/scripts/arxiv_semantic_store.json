{
  "collection_metadata": {
    "collector_agent": {
      "id": "arxiv_collector_v1",
      "type": "machine",
      "name": "PaniniFS arXiv Semantic Collector",
      "version": "1.0.0",
      "bias_profile": {
        "source": "arxiv_papers",
        "language": "english",
        "cultural_context": "academic_scientific",
        "extraction_method": "heuristic_nlp_patterns",
        "domain_focus": "computer_science_ai"
      }
    },
    "collection_date": "2025-08-15T21:55:52.669073",
    "total_atoms": 1100,
    "source_domain": "ai_computer_science",
    "source_type": "arxiv_papers",
    "papers_count": 110,
    "version": "0.2.0"
  },
  "source_papers": [
    {
      "id": "1909.03550v1",
      "title": "Lecture Notes: Optimization for Machine Learning",
      "authors": [
        "Elad Hazan"
      ],
      "abstract": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley.",
      "published": "2019-09-08T21:49:42Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1909.03550v1"
    },
    {
      "id": "1811.04422v1",
      "title": "An Optimal Control View of Adversarial Machine Learning",
      "authors": [
        "Xiaojin Zhu"
      ],
      "abstract": "I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning.",
      "published": "2018-11-11T14:28:34Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1811.04422v1"
    },
    {
      "id": "1707.04849v1",
      "title": "Minimax deviation strategies for machine learning and recognition with   short learning samples",
      "authors": [
        "Michail Schlesinger",
        "Evgeniy Vodolazskiy"
      ],
      "abstract": "The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws.",
      "published": "2017-07-16T09:15:08Z",
      "categories": [
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/1707.04849v1"
    },
    {
      "id": "1909.09246v1",
      "title": "Machine Learning for Clinical Predictive Analytics",
      "authors": [
        "Wei-Hung Weng"
      ],
      "abstract": "In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studies.",
      "published": "2019-09-19T22:02:00Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1909.09246v1"
    },
    {
      "id": "2301.09753v1",
      "title": "Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs",
      "authors": [
        "Samiyuru Menik",
        "Lakshmish Ramaswamy"
      ],
      "abstract": "Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands in the way of efficient and cost effective customized machine learning solution development. In this work we explore the benefits of modular machine learning solutions and discuss how modular machine learning solutions can overcome some of the major solution engineering limitations of monolithic machine learning solutions. We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image based. Our experimental results show that modular machine learning solutions have a promising potential to reap the solution engineering advantages of modularity while gaining performance and data advantages in a way the monolithic machine learning solutions do not permit.",
      "published": "2023-01-23T22:54:34Z",
      "categories": [
        "cs.LG",
        "cs.SE"
      ],
      "url": "https://arxiv.org/abs/2301.09753v1"
    },
    {
      "id": "0904.3664v1",
      "title": "Introduction to Machine Learning: Class Notes 67577",
      "authors": [
        "Amnon Shashua"
      ],
      "abstract": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem).",
      "published": "2009-04-23T11:40:57Z",
      "categories": [
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/0904.3664v1"
    },
    {
      "id": "2012.04105v1",
      "title": "The Tribes of Machine Learning and the Realm of Computer Architecture",
      "authors": [
        "Ayaz Akram",
        "Jason Lowe-Power"
      ],
      "abstract": "Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture.",
      "published": "2020-12-07T23:10:51Z",
      "categories": [
        "cs.LG",
        "cs.AR"
      ],
      "url": "https://arxiv.org/abs/2012.04105v1"
    },
    {
      "id": "2204.07492v2",
      "title": "A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning",
      "authors": [
        "Randy J. Chase",
        "David R. Harrison",
        "Amanda Burke",
        "Gary M. Lackmann",
        "Amy McGovern"
      ],
      "abstract": "Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods. A familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain language. The following machine learning methods are demonstrated: linear regression; logistic regression; decision trees; random forest; gradient boosted decision trees; naive Bayes; and support vector machines. Beyond discussing the different methods, the paper also contains discussions on the general machine learning process as well as best practices to enable readers to apply machine learning to their own datasets. Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorology.",
      "published": "2022-04-15T14:48:04Z",
      "categories": [
        "physics.ao-ph",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/2204.07492v2"
    },
    {
      "id": "1909.01866v1",
      "title": "Understanding Bias in Machine Learning",
      "authors": [
        "Jindong Gu",
        "Daniela Oelke"
      ],
      "abstract": "Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate the impact that biased data can have on a machine learning model. To reach such a goal, we develop interactive plots to visualizing the bias learned from synthetic data.",
      "published": "2019-09-02T20:36:19Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1909.01866v1"
    },
    {
      "id": "1911.06612v1",
      "title": "Position Paper: Towards Transparent Machine Learning",
      "authors": [
        "Dustin Juliano"
      ],
      "abstract": "Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward.",
      "published": "2019-11-12T10:49:55Z",
      "categories": [
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/1911.06612v1"
    },
    {
      "id": "1903.08801v1",
      "title": "A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain",
      "authors": [
        "Tao Wang"
      ],
      "abstract": "Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between machine learning technology and blockchain technology. Previously, machine learning and blockchain have been considered two independent technologies without an obvious link. Second, it proposes a unified analytical framework for trustable machine learning by using blockchain technology. This unified framework solves both the trustability and automation issues in machine learning. Third, it enables a computer to translate core machine learning implementation from a single thread on a single machine to multiple threads on multiple machines running with blockchain by using a unified approach. The paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisis.",
      "published": "2019-03-21T02:17:08Z",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "url": "https://arxiv.org/abs/1903.08801v1"
    },
    {
      "id": "1707.09562v3",
      "title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?",
      "authors": [
        "Yu Liu",
        "Hantian Zhang",
        "Luyuan Zeng",
        "Wentao Wu",
        "Ce Zhang"
      ],
      "abstract": "We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is possible. How good, then, are current machine learning clouds on real-world machine learning workloads?   We study this question with a focus on binary classication problems. We present mlbench, a novel benchmark constructed by harvesting datasets from Kaggle competitions. We then compare the performance of the top winning code available from Kaggle with that of running machine learning clouds from both Azure and Amazon on mlbench. Our comparative study reveals the strength and weakness of existing machine learning clouds and points out potential future directions for improvement.",
      "published": "2017-07-29T21:59:18Z",
      "categories": [
        "cs.DC",
        "cs.LG",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1707.09562v3"
    },
    {
      "id": "2108.07915v1",
      "title": "Data Pricing in Machine Learning Pipelines",
      "authors": [
        "Zicun Cong",
        "Xuan Luo",
        "Pei Jian",
        "Feida Zhu",
        "Yong Zhang"
      ],
      "abstract": "Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many parties. In this article, we survey the principles and the latest research development of data pricing in machine learning pipelines. We start with a brief review of data marketplaces and pricing desiderata. Then, we focus on pricing in three important steps in machine learning pipelines. To understand pricing in the step of training data collection, we review pricing raw data sets and data labels. We also investigate pricing in the step of collaborative training of machine learning models, and overview pricing machine learning models for end users in the step of machine learning deployment. We also discuss a series of possible future directions.",
      "published": "2021-08-18T00:57:06Z",
      "categories": [
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/2108.07915v1"
    },
    {
      "id": "1907.08908v1",
      "title": "Techniques for Automated Machine Learning",
      "authors": [
        "Yi-Wei Chen",
        "Qingquan Song",
        "Xia Hu"
      ],
      "abstract": "Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL). State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.",
      "published": "2019-07-21T04:03:36Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1907.08908v1"
    },
    {
      "id": "2312.03120v1",
      "title": "The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning",
      "authors": [
        "Omer Subasi",
        "Oceane Bel",
        "Joseph Manzano",
        "Kevin Barker"
      ],
      "abstract": "With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning, deep learning as well as federated learning. As a result, our work serves as an introductory text to the vast field of modern machine learning.",
      "published": "2023-12-05T20:40:05Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "url": "https://arxiv.org/abs/2312.03120v1"
    },
    {
      "id": "2206.07090v2",
      "title": "Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark",
      "authors": [
        "Jiajun Shen"
      ],
      "abstract": "With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and efficiency of traditional machine learning algorithms with parallelized machine learning algorithms respectively on the single machine and Spark platform. The research results have shown significant improvement in runtime and efficiency of parallelized machine learning algorithms.",
      "published": "2022-05-08T03:47:30Z",
      "categories": [
        "cs.DC"
      ],
      "url": "https://arxiv.org/abs/2206.07090v2"
    },
    {
      "id": "1507.02188v1",
      "title": "AutoCompete: A Framework for Machine Learning Competition",
      "authors": [
        "Abhishek Thakur",
        "Artus Krohn-Grimberghe"
      ],
      "abstract": "In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data types, choosing a machine learn- ing model, tuning hyper-parameters, avoiding over-fitting and optimization for a provided evaluation metric. We also observe that the proposed system produces better (or comparable) results with less runtime as compared to other approaches.",
      "published": "2015-07-08T15:07:39Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/1507.02188v1"
    },
    {
      "id": "1212.2686v1",
      "title": "Joint Training of Deep Boltzmann Machines",
      "authors": [
        "Ian Goodfellow",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.",
      "published": "2012-12-12T01:59:27Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/1212.2686v1"
    },
    {
      "id": "1607.02450v2",
      "title": "Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications",
      "authors": [
        "Kush R. Varshney"
      ],
      "abstract": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York.",
      "published": "2016-07-08T16:55:31Z",
      "categories": [
        "stat.ML",
        "cs.CY",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/1607.02450v2"
    },
    {
      "id": "2007.01503v1",
      "title": "Mathematical Perspective of Machine Learning",
      "authors": [
        "Yarema Boryshchak"
      ],
      "abstract": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective.",
      "published": "2020-07-03T05:26:02Z",
      "categories": [
        "cs.LG",
        "stat.ML",
        "68T07"
      ],
      "url": "https://arxiv.org/abs/2007.01503v1"
    },
    {
      "id": "1805.08355v1",
      "title": "Opening the black box of deep learning",
      "authors": [
        "Dian Lei",
        "Xiaoxiao Chen",
        "Jianfei Zhao"
      ],
      "abstract": "The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physical system, examines deep learning from three different perspectives: microscopic, macroscopic, and physical world views, answers multiple theoretical puzzles in deep learning by using physics principles. For example, from the perspective of quantum mechanics and statistical physics, this dissertation presents the calculation methods for convolution calculation, pooling, normalization, and Restricted Boltzmann Machine, as well as the selection of cost functions, explains why deep learning must be deep, what characteristics are learned in deep learning, why Convolutional Neural Networks do not have to be trained layer by layer, and the limitations of deep learning, etc., and proposes the theoretical direction and basis for the further development of deep learning now and in the future. The brilliance of physics flashes in deep learning, we try to establish the deep learning technology based on the scientific theory of physics.",
      "published": "2018-05-22T02:12:33Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1805.08355v1"
    },
    {
      "id": "1908.02130v1",
      "title": "Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning",
      "authors": [
        "Aras R. Dargazany"
      ],
      "abstract": "The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately.",
      "published": "2019-07-30T16:57:38Z",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/1908.02130v1"
    },
    {
      "id": "1806.01756v1",
      "title": "Concept-Oriented Deep Learning",
      "authors": [
        "Daniel T Chang"
      ],
      "abstract": "Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, concept representations, concept exemplars, and concept representation learning systems supporting incremental and continual learning.",
      "published": "2018-06-05T15:50:30Z",
      "categories": [
        "cs.AI"
      ],
      "url": "https://arxiv.org/abs/1806.01756v1"
    },
    {
      "id": "1812.05448v4",
      "title": "A First Look at Deep Learning Apps on Smartphones",
      "authors": [
        "Mengwei Xu",
        "Jiawei Liu",
        "Yuanqiang Liu",
        "Felix Xiaozhu Lin",
        "Yunxin Liu",
        "Xuanzhe Liu"
      ],
      "abstract": "We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do their deep learning models look like. Our study has strong implications for app developers, smartphone vendors, and deep learning R\\&D. On one hand, our findings paint a promising picture of deep learning for smartphones, showing the prosperity of mobile deep learning frameworks as well as the prosperity of apps building their cores atop deep learning. On the other hand, our findings urge optimizations on deep learning models deployed on smartphones, the protection of these models, and validation of research ideas on these models.",
      "published": "2018-11-08T07:59:23Z",
      "categories": [
        "cs.LG",
        "cs.CY"
      ],
      "url": "https://arxiv.org/abs/1812.05448v4"
    },
    {
      "id": "1901.02354v2",
      "title": "Geometrization of deep networks for the interpretability of deep   learning systems",
      "authors": [
        "Xiao Dong",
        "Ling Zhou"
      ],
      "abstract": "How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it may also help to solve the interpretability problem of deep learning systems.",
      "published": "2019-01-06T14:32:45Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1901.02354v2"
    },
    {
      "id": "1705.03921v1",
      "title": "Why & When Deep Learning Works: Looking Inside Deep Learnings",
      "authors": [
        "Ronny Ronen"
      ],
      "abstract": "The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output of this challenge resulted in five papers that address different facets of deep learning. These different facets include a high-level understating of why and when deep networks work (and do not work), the impact of geometry on the expressiveness of deep networks, and making deep networks interpretable.",
      "published": "2017-05-10T18:52:26Z",
      "categories": [
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/1705.03921v1"
    },
    {
      "id": "2010.05125v2",
      "title": "Learning Task-aware Robust Deep Learning Systems",
      "authors": [
        "Keji Han",
        "Yun Li",
        "Xianzhong Long",
        "Yao Ge"
      ],
      "abstract": "Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep learning system. Our method can be viewed as improving the robustness of deep learning systems from both the learning task and deep model. Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracy.",
      "published": "2020-10-11T01:06:49Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/2010.05125v2"
    },
    {
      "id": "1805.04825v1",
      "title": "Deep Learning in Software Engineering",
      "authors": [
        "Xiaochen Li",
        "He Jiang",
        "Zhilei Ren",
        "Ge Li",
        "Jingxuan Zhang"
      ],
      "abstract": "Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in SE that use deep learning techniques. We find that 41 SE tasks in all SE phases have been facilitated by deep learning integrated solutions. In which, 84.7% papers only use standard deep learning models and their variants to solve SE problems. The practicability becomes a concern in utilizing deep learning techniques. How to improve the effectiveness, efficiency, understandability, and testability of deep learning based solutions may attract more SE researchers in the future.",
      "published": "2018-05-13T06:01:39Z",
      "categories": [
        "cs.SE"
      ],
      "url": "https://arxiv.org/abs/1805.04825v1"
    },
    {
      "id": "1901.09388v2",
      "title": "Moving Deep Learning into Web Browser: How Far Can We Go?",
      "authors": [
        "Yun Ma",
        "Dongwei Xiang",
        "Shuyu Zheng",
        "Deyu Tian",
        "Xuanzhe Liu"
      ],
      "abstract": "Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been supported in browsers so far. Then we measure the performance of different frameworks when running different deep learning tasks. Finally, we dig out the performance gap between deep learning in browsers and on native platforms by comparing the performance of TensorFlow.js and TensorFlow in Python. Our findings could help application developers, deep-learning framework vendors and browser vendors to improve the efficiency of deep learning in browsers.",
      "published": "2019-01-27T14:54:51Z",
      "categories": [
        "cs.SE"
      ],
      "url": "https://arxiv.org/abs/1901.09388v2"
    },
    {
      "id": "2108.01468v1",
      "title": "Quantum Neural Networks: Concepts, Applications, and Challenges",
      "authors": [
        "Yunseok Kwak",
        "Won Joon Yun",
        "Soyi Jung",
        "Joongheon Kim"
      ],
      "abstract": "Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, this paper discusses the challenges of quantum deep learning research in multiple perspectives. Lastly, this paper presents various future research directions and application fields of quantum deep learning.",
      "published": "2021-08-02T04:32:15Z",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/2108.01468v1"
    },
    {
      "id": "2306.13586v1",
      "title": "NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders   of Deep Giants",
      "authors": [
        "Zhongzhi Yu",
        "Yonggan Fu",
        "Jiayi Yuan",
        "Haoran You",
        "Yingyan Lin"
      ],
      "abstract": "Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the architectures of TNNs via an expansion-then-contraction strategy. Extensive experiments show that NetBooster consistently outperforms state-of-the-art tiny deep learning solutions.",
      "published": "2023-06-23T16:14:25Z",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "url": "https://arxiv.org/abs/2306.13586v1"
    },
    {
      "id": "1602.00203v1",
      "title": "Greedy Deep Dictionary Learning",
      "authors": [
        "Snigdha Tariyal",
        "Angshul Majumdar",
        "Richa Singh",
        "Mayank Vatsa"
      ],
      "abstract": "In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD. Our method yields better results than all.",
      "published": "2016-01-31T06:12:58Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1602.00203v1"
    },
    {
      "id": "2108.11510v1",
      "title": "Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey",
      "authors": [
        "Ngan Le",
        "Vidhiwar Singh Rathour",
        "Kashu Yamazaki",
        "Khoa Luu",
        "Marios Savvides"
      ],
      "abstract": "Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision",
      "published": "2021-08-25T23:01:48Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "url": "https://arxiv.org/abs/2108.11510v1"
    },
    {
      "id": "2106.00120v3",
      "title": "Probabilistic Deep Learning with Probabilistic Neural Networks and Deep   Probabilistic Models",
      "authors": [
        "Daniel T. Chang"
      ],
      "abstract": "Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neural network components which capture complex non-linear stochastic relationships between the random variables. We discuss some major examples of each approach including Bayesian neural networks and mixture density networks (for probabilistic neural networks), and variational autoencoders, deep Gaussian processes and deep mixed effects models (for deep probabilistic models). TensorFlow Probability is a library for probabilistic modeling and inference which can be used for both approaches of probabilistic deep learning. We include its code examples for illustration.",
      "published": "2021-05-31T22:13:21Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/2106.00120v3"
    },
    {
      "id": "2303.01980v1",
      "title": "Towards energy-efficient Deep Learning: An overview of energy-efficient   approaches along the Deep Learning Lifecycle",
      "authors": [
        "Vanessa Mehlin",
        "Sigurd Schacht",
        "Carsten Lanquillon"
      ],
      "abstract": "Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which points along the lifecycle of Deep Learning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation) it is possible to reduce energy consumption.",
      "published": "2023-02-05T11:36:51Z",
      "categories": [
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/2303.01980v1"
    },
    {
      "id": "1805.03551v2",
      "title": "A Unified Framework of Deep Neural Networks by Capsules",
      "authors": [
        "Yujian Li",
        "Chuanhui Shan"
      ],
      "abstract": "With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learning.",
      "published": "2018-05-09T14:23:17Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1805.03551v2"
    },
    {
      "id": "1901.04195v1",
      "title": "Integrating Learning and Reasoning with Deep Logic Models",
      "authors": [
        "Giuseppe Marra",
        "Francesco Giannini",
        "Michelangelo Diligenti",
        "Marco Gori"
      ],
      "abstract": "Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, which are deep graphical models integrating deep learning and logic reasoning both for learning and inference. Deep Logic Models create an end-to-end differentiable architecture, where deep learners are embedded into a network implementing a continuous relaxation of the logic knowledge. The learning process allows to jointly learn the weights of the deep learners and the meta-parameters controlling the high-level reasoning. The experimental results show that the proposed methodology overtakes the limitations of the other approaches that have been proposed to bridge deep learning and reasoning.",
      "published": "2019-01-14T09:06:28Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1901.04195v1"
    },
    {
      "id": "2303.02715v1",
      "title": "Deep Learning in the Field of Biometric Template Protection: An Overview",
      "authors": [
        "Christian Rathgeb",
        "Jascha Kolberg",
        "Andreas Uhl",
        "Christoph Busch"
      ],
      "abstract": "Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fairness, vulnerability to attacks, or template protection. Technologies of biometric template protection are designed to enable a secure and privacy-preserving deployment of biometrics. In the recent past, deep learning techniques have been frequently applied in biometric template protection systems for various purposes. This work provides an overview of how advances in deep learning take influence on the field of biometric template protection. The interrelation between improved biometric performance rates and security in biometric template protection is elaborated. Further, the use of deep learning for obtaining feature representations that are suitable for biometric template protection is discussed. Novel methods that apply deep learning to achieve various goals of biometric template protection are surveyed along with deep learning-based attacks.",
      "published": "2023-03-05T17:06:40Z",
      "categories": [
        "cs.CV"
      ],
      "url": "https://arxiv.org/abs/2303.02715v1"
    },
    {
      "id": "2401.02349v2",
      "title": "A Survey Analyzing Generalization in Deep Reinforcement Learning",
      "authors": [
        "Ezgi Korkmaz"
      ],
      "abstract": "Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formalize and analyze generalization in deep reinforcement learning. We will explain the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their generalization capabilities. Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policies. From exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth view. We believe our study can provide a compact guideline for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with higher generalization skills.",
      "published": "2024-01-04T16:45:01Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/2401.02349v2"
    },
    {
      "id": "1711.03577v1",
      "title": "What Really is Deep Learning Doing?",
      "authors": [
        "Chuyu Xiong"
      ],
      "abstract": "Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective of mechanical learning and learning machine (see [1], [2]). From this particular angle, we can see deep learning much better and answer with confidence: What deep learning is really doing? why it works well, how it works, and how much data is necessary for learning. We also will discuss advantages and disadvantages of deep learning at the end of this work.",
      "published": "2017-11-06T23:00:13Z",
      "categories": [
        "cs.LG",
        "cs.NE"
      ],
      "url": "https://arxiv.org/abs/1711.03577v1"
    },
    {
      "id": "2304.05133v2",
      "title": "Lecture Notes: Neural Network Architectures",
      "authors": [
        "Evelyn Herberg"
      ],
      "abstract": "These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network.",
      "published": "2023-04-11T10:54:36Z",
      "categories": [
        "cs.LG",
        "math.OC",
        "68T07"
      ],
      "url": "https://arxiv.org/abs/2304.05133v2"
    },
    {
      "id": "0504056v1",
      "title": "Self-Organizing Multilayered Neural Networks of Optimal Complexity",
      "authors": [
        "V. Schetinin"
      ],
      "abstract": "The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics.",
      "published": "2005-04-13T13:59:55Z",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "url": "https://arxiv.org/abs/0504056v1"
    },
    {
      "id": "1911.05640v2",
      "title": "Neural Network Processing Neural Networks: An efficient way to learn   higher order functions",
      "authors": [
        "Firat Tuna"
      ],
      "abstract": "Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures.",
      "published": "2019-11-06T19:15:34Z",
      "categories": [
        "cs.LG",
        "cs.NE"
      ],
      "url": "https://arxiv.org/abs/1911.05640v2"
    },
    {
      "id": "2304.13812v1",
      "title": "Guaranteed Quantization Error Computation for Neural Network Model   Compression",
      "authors": [
        "Wesley Cooke",
        "Zihao Mo",
        "Weiming Xiang"
      ],
      "abstract": "Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods are applied to the merged neural network to compute the guaranteed quantization error. Finally, a numerical example is proposed to validate the applicability and effectiveness of the proposed approach.",
      "published": "2023-04-26T20:21:54Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "url": "https://arxiv.org/abs/2304.13812v1"
    },
    {
      "id": "2007.06559v2",
      "title": "Graph Structure of Neural Networks",
      "authors": [
        "Jiaxuan You",
        "Jure Leskovec",
        "Kaiming He",
        "Saining Xie"
      ],
      "abstract": "Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that: (1) a \"sweet spot\" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.",
      "published": "2020-07-13T17:59:31Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.SI",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/2007.06559v2"
    },
    {
      "id": "2408.04747v1",
      "title": "Hybrid Quantum-Classical Neural Networks for Downlink Beamforming   Optimization",
      "authors": [
        "Juping Zhang",
        "Gan Zheng",
        "Toshiaki Koike-Akino",
        "Kai-Kit Wong",
        "Fraser Burton"
      ],
      "abstract": "This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a classical convolutional neural network. The classical neural network can be jointly trained with the quantum neural network or pre-trained leading to a fine-tuning transfer learning method. The second one designs a quantum convolutional neural network to better extract features followed by a classical deep neural network. Our results demonstrate the feasibility of the proposed hybrid neural networks, and reveal that the first method can achieve similar sum rate performance compared to a benchmark classical neural network with significantly less training parameters; while the second method can achieve higher sum rate especially in presence of many users still with less training parameters. The robustness of the proposed methods is verified using both software simulators and hardware emulators considering noisy intermediate-scale quantum devices.",
      "published": "2024-08-08T20:14:39Z",
      "categories": [
        "cs.IT",
        "math.IT"
      ],
      "url": "https://arxiv.org/abs/2408.04747v1"
    },
    {
      "id": "2409.13654v2",
      "title": "A Novel Neural Filter to Improve Accuracy of Neural Network Models of   Dynamic Systems",
      "authors": [
        "Parham Oveissi",
        "Turibius Rozario",
        "Ankit Goel"
      ],
      "abstract": "The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the neural filter combines the neural network state predictions with the measurements from the physical system to improve the estimated state's accuracy. The neural filter's improvements in prediction accuracy are demonstrated through applications to four nonlinear dynamical systems. Numerical experiments show that the neural filter significantly improves prediction accuracy and bounds the state estimate covariance, outperforming the neural network predictions. Furthermore, it is also shown that the accuracy of a poorly trained neural network model can be improved to the same level as that of an adequately trained neural network model, potentially decreasing the training cost and required data to train a neural network.",
      "published": "2024-09-20T17:03:04Z",
      "categories": [
        "cs.LG",
        "math.DS"
      ],
      "url": "https://arxiv.org/abs/2409.13654v2"
    },
    {
      "id": "1804.03313v1",
      "title": "Cortex Neural Network: learning with Neural Network groups",
      "authors": [
        "Liyao Gao"
      ],
      "abstract": "Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current artificial neural network. In this paper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is an upper architecture of neural networks which motivated from cerebral cortex in the brain to handle different tasks in the same learning system. It is able to identify different tasks and solve them with different methods. In our implementation, the Cortex Neural Network is able to process different cognitive tasks and perform reflection to get a higher accuracy. We provide a series of experiments to examine the capability of the cortex architecture on traditional neural networks. Our experiments proved its ability on the Cortex Neural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the same time, which can promisingly reduce the loss by 40%.",
      "published": "2018-04-10T02:33:47Z",
      "categories": [
        "cs.NE",
        "cs.CV",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/1804.03313v1"
    },
    {
      "id": "0608073v1",
      "title": "Parametrical Neural Networks and Some Other Similar Architectures",
      "authors": [
        "Leonid B. Litinskii"
      ],
      "abstract": "A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated.",
      "published": "2006-08-18T08:28:23Z",
      "categories": [
        "cs.CV",
        "cs.NE"
      ],
      "url": "https://arxiv.org/abs/0608073v1"
    },
    {
      "id": "2006.02909v1",
      "title": "Assessing Intelligence in Artificial Neural Networks",
      "authors": [
        "Nicholas J. Schaub",
        "Nathan Hotaling"
      ],
      "abstract": "The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet-300-100) and a convolutional neural network (LeNet-5). The LeNet-5 network with the highest aIQ was 2.32% less accurate but contained 30,912 times fewer parameters than the highest accuracy network. Both batch normalization and dropout layers were found to increase neural efficiency. Finally, high aIQ networks are shown to be memorization and overtraining resistant, capable of learning proper digit classification with an accuracy of 92.51% even when 75% of the class labels are randomized. These results demonstrate the utility of aIQ and neural efficiency as metrics for balancing network performance and size.",
      "published": "2020-06-03T16:45:42Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "url": "https://arxiv.org/abs/2006.02909v1"
    },
    {
      "id": "2307.06287v1",
      "title": "Rational Neural Network Controllers",
      "authors": [
        "Matthew Newton",
        "Antonis Papachristodoulou"
      ],
      "abstract": "Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properly, they are not an ideal candidate for controllers due to issues with robustness and uncertainty, which are pivotal aspects of control systems. There has been initial work on robustness to both analyse and design dynamical systems with neural network controllers. However, one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning tasks. These structures may not be appropriate for neural network controllers and it is important to consider alternative architectures. This paper considers rational neural networks and presents novel rational activation functions, which can be used effectively in robustness problems for neural feedback loops. Rational activation functions are replaced by a general rational neural network structure, which is convex in the neural network's parameters. A method is proposed to recover a stabilising controller from a Sum of Squares feasibility test. This approach is then applied to a refined rational neural network which is more compatible with Sum of Squares programming. Numerical examples show that this method can successfully recover stabilising rational neural network controllers for neural feedback loops with non-linear plants with noise and parametric uncertainty.",
      "published": "2023-07-12T16:35:41Z",
      "categories": [
        "eess.SY",
        "cs.LG",
        "cs.SY"
      ],
      "url": "https://arxiv.org/abs/2307.06287v1"
    },
    {
      "id": "2011.01218v1",
      "title": "Asymptotic Theory of Expectile Neural Networks",
      "authors": [
        "Jinghang Lin",
        "Xiaoxi Shen",
        "Qing Lu"
      ],
      "abstract": "Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework.",
      "published": "2020-10-31T01:14:04Z",
      "categories": [
        "math.ST",
        "stat.TH"
      ],
      "url": "https://arxiv.org/abs/2011.01218v1"
    },
    {
      "id": "1605.07333v1",
      "title": "Combining Recurrent and Convolutional Neural Networks for Relation   Classification",
      "authors": [
        "Ngoc Thang Vu",
        "Heike Adel",
        "Pankaj Gupta",
        "Hinrich Schtze"
      ],
      "abstract": "This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task.",
      "published": "2016-05-24T08:20:12Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/1605.07333v1"
    },
    {
      "id": "2303.10780v2",
      "title": "A Comprehensive Review of Spiking Neural Networks: Interpretation,   Optimization, Efficiency, and Best Practices",
      "authors": [
        "Kai Malcolm",
        "Josue Casco-Rodriguez"
      ],
      "abstract": "Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neural networks. Key contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitioners.",
      "published": "2023-03-19T22:07:27Z",
      "categories": [
        "cs.NE",
        "cs.LG",
        "eess.IV"
      ],
      "url": "https://arxiv.org/abs/2303.10780v2"
    },
    {
      "id": "2401.10289v1",
      "title": "Design and development of opto-neural processors for simulation of   neural networks trained in image detection for potential implementation in   hybrid robotics",
      "authors": [
        "Sanjana Shetty"
      ],
      "abstract": "Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms.",
      "published": "2024-01-17T04:42:49Z",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "url": "https://arxiv.org/abs/2401.10289v1"
    },
    {
      "id": "1911.07626v1",
      "title": "Convex Formulation of Overparameterized Deep Neural Networks",
      "authors": [
        "Cong Fang",
        "Yihong Gu",
        "Weizhong Zhang",
        "Tong Zhang"
      ],
      "abstract": "Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundamental problem by investigating such overparameterizeddeep neural networks when fully trained. We generalize a new technique called neural feature repopulation, originally introduced in (Fang et al., 2019a) for two-level neural networks, to analyze deep neural networks. It is shown that under suitable representations, overparameterized deep neural networks are inherently convex, and when optimized, the system can learn effective features suitable for the underlying learning task under mild conditions. This new analysis is consistent with empirical observations that deep neural networks are capable of learning efficient feature representations. Therefore, the highly unexpected result of this paper can satisfactorily explain the practical success of deep neural networks. Empirical studies confirm that predictions of our theory are consistent with results observed in practice.",
      "published": "2019-11-18T13:42:04Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1911.07626v1"
    },
    {
      "id": "2202.01214v1",
      "title": "Approximate Bisimulation Relations for Neural Networks and Application   to Assured Neural Network Compression",
      "authors": [
        "Weiming Xiang",
        "Zhongzhu Shao"
      ],
      "abstract": "In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate bisimulation relation results to perform neural networks model reduction and compute the compression precision, i.e., assured neural networks compression. At last, using the assured neural network compression, we accelerate the verification processes of ACAS Xu neural networks to illustrate the effectiveness and advantages of our proposed approximate bisimulation approach.",
      "published": "2022-02-02T16:21:19Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "url": "https://arxiv.org/abs/2202.01214v1"
    },
    {
      "id": "2304.01561v3",
      "title": "Optimal rates of approximation by shallow ReLU$^k$ neural networks and   applications to nonparametric regression",
      "authors": [
        "Yunfei Yang",
        "Ding-Xuan Zhou"
      ],
      "abstract": "We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\\\"older functions, which complements recent results for deep neural networks. It is also proven that over-parameterized (deep or shallow) neural networks can achieve nearly optimal rates for nonparametric regression.",
      "published": "2023-04-04T06:35:02Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/2304.01561v3"
    },
    {
      "id": "2309.07716v2",
      "title": "Understanding Vector-Valued Neural Networks and Their Relationship with   Real and Hypercomplex-Valued Neural Networks",
      "authors": [
        "Marcos Eduardo Valle"
      ],
      "abstract": "Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrelation between feature channels. Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks. This paper aims to present a broad framework for vector-valued neural networks, referred to as V-nets. In this context, hypercomplex-valued neural networks are regarded as vector-valued models with additional algebraic properties. Furthermore, this paper explains the relationship between vector-valued and traditional neural networks. Precisely, a vector-valued neural network can be obtained by placing restrictions on a real-valued model to consider the intercorrelation between feature channels. Finally, we show how V-nets, including hypercomplex-valued neural networks, can be implemented in current deep-learning libraries as real-valued networks.",
      "published": "2023-09-14T13:48:16Z",
      "categories": [
        "cs.LG",
        "cs.NE"
      ],
      "url": "https://arxiv.org/abs/2309.07716v2"
    },
    {
      "id": "1404.5997v2",
      "title": "One weird trick for parallelizing convolutional neural networks",
      "authors": [
        "Alex Krizhevsky"
      ],
      "abstract": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.",
      "published": "2014-04-23T22:37:56Z",
      "categories": [
        "cs.NE",
        "cs.DC",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/1404.5997v2"
    },
    {
      "id": "1608.04434v1",
      "title": "Natural Language Processing using Hadoop and KOSHIK",
      "authors": [
        "Emre Erturk",
        "Hong Shi"
      ],
      "abstract": "Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains language processing components such as Stanford CoreNLP and OpenNLP. This study describes how to build a KOSHIK platform with the relevant tools, and provides the steps to analyze wiki data. Finally, it evaluates and discusses the advantages and disadvantages of the KOSHIK architecture, and gives recommendations on improving the processing performance.",
      "published": "2016-08-15T23:09:21Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/1608.04434v1"
    },
    {
      "id": "2503.02435v1",
      "title": "NLI4DB: A Systematic Review of Natural Language Interfaces for Databases",
      "authors": [
        "Mengyi Liu",
        "Jianqiu Xu"
      ],
      "abstract": "As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation process from natural language to executable database language. The translation process is divided into three stages: (i) natural language preprocessing, (ii) natural language understanding, and (iii) natural language translation. Traditional and data-driven methods are utilized in the preprocessing stage. Traditional approaches rely on predefined rules and grammars, and involve techniques such as regular expressions, dependency parsing and named entity recognition. Data-driven approaches depend on large-scale data and machine learning models, using techniques including word embedding and pattern linking. Natural language understanding methods are classified into three categories: (i) rule-based, (ii) machine learning-based, and (iii) hybrid. We then describe a general construction process for executable languages over relational and spatio-temporal databases. Subsequently, common benchmarks and evaluation metrics for transforming natural language into executable language are presented, and methods for generating new benchmarks are explored. Finally, we summarize the classification, development, and enhancement of NLIDB systems, and discuss deep language understanding and database interaction techniques related to NLIDB, including (i) using LLM for Text2SQL tasks, (ii) generating natural language interpretations from SQL, and (iii) transforming speech queries into SQL.",
      "published": "2025-03-04T09:22:50Z",
      "categories": [
        "cs.DB"
      ],
      "url": "https://arxiv.org/abs/2503.02435v1"
    },
    {
      "id": "2202.07138v2",
      "title": "Integrating AI Planning with Natural Language Processing: A Combination   of Explicit and Tacit Knowledge",
      "authors": [
        "Kebing Jin",
        "Hankz Hankui Zhuo"
      ],
      "abstract": "Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning. Automated planning (AI planning) focuses on building symbolic domain models and synthesizing plans to transit initial states to goals based on domain models. Recently, there have been plenty of works related to these two fields, which have the abilities to generate explicit knowledge, e.g., preconditions and effects of action models, and learn from tacit knowledge, e.g., neural models, respectively. Integrating AI planning and natural language processing effectively improves the communication between human and intelligent agents. This paper outlines the commons and relations between AI planning and natural language processing, argues that each of them can effectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based natural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and (5) applications. We also explore some potential future issues between AI planning and natural language processing. To the best of our knowledge, this survey is the first work that addresses the deep connections between AI planning and Natural language processing.",
      "published": "2022-02-15T02:19:09Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/2202.07138v2"
    },
    {
      "id": "1906.11608v2",
      "title": "Simple Natural Language Processing Tools for Danish",
      "authors": [
        "Leon Derczynski"
      ],
      "abstract": "This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available.",
      "published": "2019-06-27T13:15:12Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/1906.11608v2"
    },
    {
      "id": "2503.16728v2",
      "title": "Natural Language Generation",
      "authors": [
        "Emiel van Miltenburg",
        "Chenghua Lin"
      ],
      "abstract": "This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Language Processing, NLG is closely related to other sub-disciplines such as Machine Translation (MT) and Dialog Systems. Some NLG researchers exclude MT from their definition of the field, since there is no content selection involved where the system has to determine what to say. Conversely, dialog systems do not typically fall under the header of Natural Language Generation since NLG is just one component of dialog systems (the others being Natural Language Understanding and Dialog Management). However, with the rise of Large Language Models (LLMs), different subfields of Natural Language Processing have converged on similar methodologies for the production of natural language and the evaluation of automatically generated text.",
      "published": "2025-03-20T22:12:08Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/2503.16728v2"
    },
    {
      "id": "2006.16212v1",
      "title": "Towards the Study of Morphological Processing of the Tangkhul Language",
      "authors": [
        "Mirinso Shadang",
        "Navanath Saharia",
        "Thoudam Doren Singh"
      ],
      "abstract": "There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus.",
      "published": "2020-06-29T17:24:09Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/2006.16212v1"
    },
    {
      "id": "1510.00726v1",
      "title": "A Primer on Neural Network Models for Natural Language Processing",
      "authors": [
        "Yoav Goldberg"
      ],
      "abstract": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.",
      "published": "2015-10-02T20:17:33Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/1510.00726v1"
    },
    {
      "id": "1511.07916v1",
      "title": "Natural Language Understanding with Distributed Representation",
      "authors": [
        "Kyunghyun Cho"
      ],
      "abstract": "This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introduced. On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding.",
      "published": "2015-11-24T23:23:13Z",
      "categories": [
        "cs.CL",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1511.07916v1"
    },
    {
      "id": "1908.08971v2",
      "title": "Deploying Technology to Save Endangered Languages",
      "authors": [
        "Hilaria Cruz",
        "Joseph Waring"
      ],
      "abstract": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages.",
      "published": "2019-08-23T18:31:35Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/1908.08971v2"
    },
    {
      "id": "2111.09791v1",
      "title": "Supporting Undotted Arabic with Pre-trained Language Models",
      "authors": [
        "Aviad Rom",
        "Kfir Bar"
      ],
      "abstract": "We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted texts with pre-trained models, without additional training, and measure their performance on two Arabic natural-language-processing downstream tasks. The results are encouraging; in one of the tasks our method shows nearly perfect performance.",
      "published": "2021-11-18T16:47:56Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/2111.09791v1"
    },
    {
      "id": "2112.01705v1",
      "title": "Multilingual Text Classification for Dravidian Languages",
      "authors": [
        "Xiaotian Lin",
        "Nankai Lin",
        "Kanoksak Wattanachote",
        "Shengyi Jiang",
        "Lianxi Wang"
      ],
      "abstract": "As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to address these problems, we proposed a multilingual text classification framework for the Dravidian languages. On the one hand, the framework used the LaBSE pre-trained model as the base model. Aiming at the problem of text information bias in multi-task learning, we propose to use the MLM strategy to select language-specific words, and used adversarial training to perturb them. On the other hand, in view of the problem that the model cannot well recognize and utilize the correlation among languages, we further proposed a language-specific representation module to enrich semantic information for the model. The experimental results demonstrated that the framework we proposed has a significant performance in multilingual text classification tasks with each strategy achieving certain improvements.",
      "published": "2021-12-03T04:26:49Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/2112.01705v1"
    },
    {
      "id": "2205.07634v1",
      "title": "A Precis of Language Models are not Models of Language",
      "authors": [
        "Csaba Veres"
      ],
      "abstract": "Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition.",
      "published": "2022-05-16T12:50:58Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/2205.07634v1"
    },
    {
      "id": "1107.4687v2",
      "title": "Fence - An Efficient Parser with Ambiguity Support for Model-Driven   Language Specification",
      "authors": [
        "Luis Quesada",
        "Fernando Berzal",
        "Francisco J. Cortijo"
      ],
      "abstract": "Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguities. In this paper, we propose Fence, an efficient bottom-up parsing algorithm with lexical and syntactic ambiguity support that enables the use of model-based language specification in practice.",
      "published": "2011-07-23T12:56:02Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/1107.4687v2"
    },
    {
      "id": "2210.04451v1",
      "title": "Self-move and Other-move: Quantum Categorical Foundations of Japanese",
      "authors": [
        "Ryder Dale Walton"
      ],
      "abstract": "The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological principles of these diagrams and many potential avenues for further research are proposed. Why is this endeavor important? Hundreds of languages have developed over the course of millennia coinciding with the evolution of human interaction across time and geographic location. These languages are foundational to human survival, experience, flourishing, and living the good life. They are also, however, the strongest barrier between people groups. Over the last several decades, advancements in Natural Language Processing (NLP) have made it easier to bridge the gap between individuals who do not share a common language or culture. Tools like Google Translate and DeepL make it easier than ever before to share our experiences with people globally. Nevertheless, these tools are still inadequate as they fail to convey our ideas across the language barrier fluently, leaving people feeling anxious and embarrassed. This is particularly true of languages born out of substantially different cultures, such as English and Japanese. Quantum computers offer the best chance to achieve translation fluency in that they are better suited to simulating the natural world and natural phenomenon such as natural speech.   Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English grammar, translation, topology, Quantum Natural Language Processing, Natural Language Processing",
      "published": "2022-10-10T06:26:59Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/2210.04451v1"
    },
    {
      "id": "2105.05222v2",
      "title": "Including Signed Languages in Natural Language Processing",
      "authors": [
        "Kayo Yin",
        "Amit Moryossef",
        "Julie Hochgesang",
        "Yoav Goldberg",
        "Malihe Alikhani"
      ],
      "abstract": "Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.",
      "published": "2021-05-11T17:37:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/2105.05222v2"
    },
    {
      "id": "2104.09712v1",
      "title": "Problems and Countermeasures in Natural Language Processing Evaluation",
      "authors": [
        "Qingxiu Dong",
        "Zhifang Sui",
        "Weidong Zhan",
        "Baobao Chang"
      ],
      "abstract": "Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteristics of mainstream natural language evaluation, and then summarizes the problems and causes of natural language pro-cessing evaluation. Finally, this article refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability evaluation, and proposes a series of basic principles and implementation ideas for hu-man-like machine language ability evaluation from the three aspects of reliability, difficulty and validity.",
      "published": "2021-04-20T01:35:16Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/2104.09712v1"
    },
    {
      "id": "9507009v1",
      "title": "Specifying Logic Programs in Controlled Natural Language",
      "authors": [
        "Norbert E. Fuchs",
        "Rolf Schwitter"
      ],
      "abstract": "Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-specialists. Specifications in controlled natural language are automatically translated into Prolog clauses, hence become formal and executable. The translation uses a definite clause grammar (DCG) enhanced by feature structures. Inter-text references of the specification, e.g. anaphora, are resolved with the help of discourse representation theory (DRT). The generated Prolog clauses are added to a knowledge base. We have implemented a prototypical specification system that successfully processes the specification of a simple automated teller machine.",
      "published": "1995-07-21T17:44:05Z",
      "categories": [
        "cmp-lg",
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/9507009v1"
    },
    {
      "id": "2312.15713v1",
      "title": "PersianLLaMA: Towards Building First Persian Large Language Model",
      "authors": [
        "Mohammad Amin Abbasi",
        "Arash Ghafouri",
        "Mahdi Firouzmandi",
        "Hassan Naderi",
        "Behrouz Minaei Bidgoli"
      ],
      "abstract": "Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. This paper introduces the first large Persian language model, named PersianLLaMA, trained on a collection of Persian texts and datasets. This foundational model comes in two versions, with 7 and 13 billion parameters, trained on formal and colloquial Persian texts using two different approaches. PersianLLaMA has been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, and for natural language understanding tasks based on automated machine metrics. The results indicate that PersianLLaMA significantly outperforms its competitors in both understanding and generating Persian text. PersianLLaMA marks an important step in the development of Persian natural language processing and can be a valuable resource for the Persian-speaking community. This large language model can be used for various natural language processing tasks, especially text generation like chatbots, question-answering, machine translation, and text summarization",
      "published": "2023-12-25T12:48:55Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "url": "https://arxiv.org/abs/2312.15713v1"
    },
    {
      "id": "1612.03231v1",
      "title": "A natural language interface to a graph-based bibliographic information   retrieval system",
      "authors": [
        "Yongjun Zhu",
        "Erjia Yan",
        "Il-Yeol Song"
      ],
      "abstract": "With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics for interpreting and analyzing natural language bibliographic queries. NLI-GIBIR allows users to search for a variety of bibliographic data through natural language. A series of text- and linguistic-based techniques are used to analyze and answer natural language queries, including tokenization, named entity recognition, and syntactic analysis. We find that our framework can effectively represents and addresses complex bibliographic information needs. Thus, the contributions of this paper are as follows: First, to our knowledge, it is the first attempt to propose a natural language interface to graph-based bibliographic information retrieval. Second, we propose a novel customized natural language processing framework that integrates a few original algorithms/heuristics for interpreting and analyzing natural language bibliographic queries. Third, we show that the proposed framework and natural language interface provide a practical solution in building real-world natural language interface-based bibliographic information retrieval systems. Our experimental results show that the presented system can correctly answer 39 out of 40 example natural language queries with varying lengths and complexities.",
      "published": "2016-12-10T00:32:28Z",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/1612.03231v1"
    },
    {
      "id": "1612.07486v2",
      "title": "Continuous multilinguality with language vectors",
      "authors": [
        "Robert stling",
        "Jrg Tiedemann"
      ],
      "abstract": "Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages.",
      "published": "2016-12-22T08:29:25Z",
      "categories": [
        "cs.CL"
      ],
      "url": "https://arxiv.org/abs/1612.07486v2"
    },
    {
      "id": "1905.07844v1",
      "title": "Implications of Computer Vision Driven Assistive Technologies Towards   Individuals with Visual Impairment",
      "authors": [
        "Linda Wang",
        "Alexander Wong"
      ],
      "abstract": "Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potential usefulness arise. This paper addresses the positive and negative implications computer vision based assistive technologies have on individuals with visual impairment, as well as considerations for computer vision researchers and developers in order to mitigate the amount of negative implications.",
      "published": "2019-05-20T02:00:56Z",
      "categories": [
        "cs.CV",
        "cs.CY"
      ],
      "url": "https://arxiv.org/abs/1905.07844v1"
    },
    {
      "id": "1310.0319v3",
      "title": "Second Croatian Computer Vision Workshop (CCVW 2013)",
      "authors": [
        "Sven Lonari",
        "Sinia egvi"
      ],
      "abstract": "Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb.",
      "published": "2013-10-01T14:26:29Z",
      "categories": [
        "cs.CV"
      ],
      "url": "https://arxiv.org/abs/1310.0319v3"
    },
    {
      "id": "1707.03720v1",
      "title": "Multiband NFC for High-Throughput Wireless Computer Vision Sensor   Network",
      "authors": [
        "F. Li",
        "J. Du"
      ],
      "abstract": "Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput.",
      "published": "2017-05-28T06:43:29Z",
      "categories": [
        "cs.NI",
        "cs.CV"
      ],
      "url": "https://arxiv.org/abs/1707.03720v1"
    },
    {
      "id": "1910.13796v1",
      "title": "Deep Learning vs. Traditional Computer Vision",
      "authors": [
        "Niall O' Mahony",
        "Sean Campbell",
        "Anderson Carvalho",
        "Suman Harapanahalli",
        "Gustavo Velasco-Hernandez",
        "Lenka Krpalkova",
        "Daniel Riordan",
        "Joseph Walsh"
      ],
      "abstract": "Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will also explore how the two sides of computer vision can be combined. Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning. For example, combining traditional computer vision techniques with Deep Learning has been popular in emerging domains such as Panoramic Vision and 3D vision for which Deep Learning models have not yet been fully optimised",
      "published": "2019-10-30T12:25:10Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/1910.13796v1"
    },
    {
      "id": "1808.03998v1",
      "title": "Enhancing camera surveillance using computer vision: a research note",
      "authors": [
        "Haroon Idrees",
        "Mubarak Shah",
        "Ray Surette"
      ],
      "abstract": "$\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus on computer vision as a solution for two common surveillance camera tasks (live monitoring of multiple surveillance cameras and summarizing archived video files). Three unaddressed research questions (can specialized computer vision applications for law enforcement be developed at this time, how will computer vision be utilized within existing public safety camera monitoring rooms, and what are the system-wide impacts of a computer vision capability on local criminal justice systems) are considered.   $\\mathbf{Findings}$ - Despite computer vision becoming accessible to law enforcement agencies the impact of computer vision has not been discussed or adequately researched. There is little knowledge of computer vision or its potential in the field.   $\\mathbf{Originality/value}$ - This paper introduces and discusses computer vision from a law enforcement perspective and will be valuable to police personnel tasked with monitoring large camera networks and considering computer vision as a system upgrade.",
      "published": "2018-08-12T20:01:37Z",
      "categories": [
        "cs.CY"
      ],
      "url": "https://arxiv.org/abs/1808.03998v1"
    },
    {
      "id": "1809.04659v2",
      "title": "Are object detection assessment criteria ready for maritime computer   vision?",
      "authors": [
        "Dilip K. Prasad",
        "Huixu Dong",
        "Deepu Rajan",
        "Chai Quek"
      ],
      "abstract": "Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime setting. Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight. We discuss the problem of defining assessment metrics suitable for maritime computer vision. We consider new bottom edge proximity metrics as assessment metrics for maritime computer vision. These metrics indicate that existing computer vision approaches are indeed promising for maritime computer vision and can play a foundational role in the emerging field of maritime computer vision.",
      "published": "2018-09-12T20:18:04Z",
      "categories": [
        "cs.CV"
      ],
      "url": "https://arxiv.org/abs/1809.04659v2"
    },
    {
      "id": "1909.07245v1",
      "title": "BMVC 2019: Workshop on Interpretable and Explainable Machine Vision",
      "authors": [
        "Alun Preece"
      ],
      "abstract": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019.",
      "published": "2019-09-16T14:44:19Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/1909.07245v1"
    },
    {
      "id": "2203.15269v1",
      "title": "Vision Transformers in Medical Computer Vision -- A Contemplative   Retrospection",
      "authors": [
        "Arshi Parvaiz",
        "Muhammad Anwaar Khalid",
        "Rukhsana Zafar",
        "Huma Ameer",
        "Muhammad Ali",
        "Muhammad Moazam Fraz"
      ],
      "abstract": "Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are immensely utilized by a plenty of researchers to perform new as well as former experiments. Here, in this article we investigate the intersection of Vision Transformers and Medical images and proffered an overview of various ViTs based frameworks that are being used by different researchers in order to decipher the obstacles in Medical Computer Vision. We surveyed the application of Vision transformers in different areas of medical computer vision such as image-based disease classification, anatomical structure segmentation, registration, region-based lesion Detection, captioning, report generation, reconstruction using multiple medical imaging modalities that greatly assist in medical diagnosis and hence treatment process. Along with this, we also demystify several imaging modalities used in Medical Computer Vision. Moreover, to get more insight and deeper understanding, self-attention mechanism of transformers is also explained briefly. Conclusively, we also put some light on available data sets, adopted methodology, their performance measures, challenges and their solutions in form of discussion. We hope that this review article will open future directions for researchers in medical computer vision.",
      "published": "2022-03-29T06:32:43Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/2203.15269v1"
    },
    {
      "id": "2408.12114v3",
      "title": "SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for   Large-scale Vision-Language Models",
      "authors": [
        "Youngjoon Yu",
        "Sangyun Chung",
        "Byung-Kwan Lee",
        "Yong Man Ro"
      ],
      "abstract": "Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteristics of multi-vision sensors. They fail to convey the fundamental multi-vision sensor information from the dataset and the corresponding contextual knowledge properly. Consequently, alignment between the information from the actual physical environment and the text is not achieved correctly, making it difficult to answer complex sensor-related questions that consider the physical environment. In this paper, we aim to establish a multi-vision Sensor Perception And Reasoning benchmarK called SPARK that can reduce the fundamental multi-vision sensor information gap between images and multi-vision sensors. We generated 6,248 vision-language test samples to investigate multi-vision sensory perception and multi-vision sensory reasoning on physical sensor knowledge proficiency across different formats, covering different types of sensor-related questions. We utilized these samples to assess ten leading LVLMs. The results showed that most models displayed deficiencies in multi-vision sensory reasoning to varying extents. Codes and data are available at https://github.com/top-yun/SPARK",
      "published": "2024-08-22T03:59:48Z",
      "categories": [
        "cs.CV"
      ],
      "url": "https://arxiv.org/abs/2408.12114v3"
    },
    {
      "id": "1907.09233v1",
      "title": "Adapting Computer Vision Algorithms for Omnidirectional Video",
      "authors": [
        "Hannes Fassold"
      ],
      "abstract": "Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video.",
      "published": "2019-07-22T11:12:35Z",
      "categories": [
        "cs.CV"
      ],
      "url": "https://arxiv.org/abs/1907.09233v1"
    },
    {
      "id": "2001.09608v1",
      "title": "Some Insights into Lifelong Reinforcement Learning Systems",
      "authors": [
        "Changjian Li"
      ],
      "abstract": "A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system.",
      "published": "2020-01-27T07:26:12Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/2001.09608v1"
    },
    {
      "id": "2405.15430v1",
      "title": "Counterexample-Guided Repair of Reinforcement Learning Systems Using   Safety Critics",
      "authors": [
        "David Boetius",
        "Stefan Leue"
      ],
      "abstract": "Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation.",
      "published": "2024-05-24T10:56:51Z",
      "categories": [
        "cs.LG",
        "cs.LO"
      ],
      "url": "https://arxiv.org/abs/2405.15430v1"
    },
    {
      "id": "2108.11510v1",
      "title": "Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey",
      "authors": [
        "Ngan Le",
        "Vidhiwar Singh Rathour",
        "Kashu Yamazaki",
        "Khoa Luu",
        "Marios Savvides"
      ],
      "abstract": "Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision",
      "published": "2021-08-25T23:01:48Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "url": "https://arxiv.org/abs/2108.11510v1"
    },
    {
      "id": "2307.01452v2",
      "title": "Causal Reinforcement Learning: A Survey",
      "authors": [
        "Zhihong Deng",
        "Jing Jiang",
        "Guodong Long",
        "Chengqi Zhang"
      ],
      "abstract": "Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcement learning. We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learning. We categorize and systematically review existing causal reinforcement learning approaches based on their target problems and methodologies. Finally, we outline open issues and future directions in this emerging field.",
      "published": "2023-07-04T03:00:43Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "url": "https://arxiv.org/abs/2307.01452v2"
    },
    {
      "id": "2212.00253v1",
      "title": "Distributed Deep Reinforcement Learning: A Survey and A Multi-Player   Multi-Agent Learning Toolbox",
      "authors": [
        "Qiyue Yin",
        "Tongtong Yu",
        "Shengqi Shen",
        "Jun Yang",
        "Meijing Zhao",
        "Kaiqi Huang",
        "Bin Liang",
        "Liang Wang"
      ],
      "abstract": "With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, distributed deep reinforcement learning has shown its potential in various applications, such as human-computer gaming, and intelligent transportation. In this paper, we conclude the state of this exciting field, by comparing the classical distributed deep reinforcement learning methods, and studying important components to achieve efficient distributed learning, covering single player single agent distributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learning. Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modifications of their non-distributed versions. By analyzing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex games. Finally, we try to point out challenges and future trends, hoping this brief review can provide a guide or a spark for researchers who are interested in distributed deep reinforcement learning.",
      "published": "2022-12-01T03:39:24Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "url": "https://arxiv.org/abs/2212.00253v1"
    },
    {
      "id": "2009.07888v7",
      "title": "Transfer Learning in Deep Reinforcement Learning: A Survey",
      "authors": [
        "Zhuangdi Zhu",
        "Kaixiang Lin",
        "Anil K. Jain",
        "Jiayu Zhou"
      ],
      "abstract": "Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progress.",
      "published": "2020-09-16T18:38:54Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/2009.07888v7"
    },
    {
      "id": "2108.03258v2",
      "title": "Memory-two strategies forming symmetric mutual reinforcement learning   equilibrium in repeated prisoners' dilemma game",
      "authors": [
        "Masahiko Ueda"
      ],
      "abstract": "We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria formed by memory-two strategies are also mutual reinforcement learning equilibria when both players use reinforcement learning of memory-$n$ strategies with $n>2$.",
      "published": "2021-08-05T04:59:18Z",
      "categories": [
        "physics.soc-ph",
        "cs.GT"
      ],
      "url": "https://arxiv.org/abs/2108.03258v2"
    },
    {
      "id": "2204.05437v1",
      "title": "Implementing Online Reinforcement Learning with Temporal Neural Networks",
      "authors": [
        "James E. Smith"
      ],
      "abstract": "A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulation.",
      "published": "2022-04-11T23:10:42Z",
      "categories": [
        "cs.NE",
        "68T07",
        "I.2.6"
      ],
      "url": "https://arxiv.org/abs/2204.05437v1"
    },
    {
      "id": "1709.05067v1",
      "title": "Deep Reinforcement Learning for Conversational AI",
      "authors": [
        "Mahipal Jadeja",
        "Neelanshi Varia",
        "Agam Shah"
      ],
      "abstract": "Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement learning and supervised learning and models for implementation of reinforcement are discussed. Key challenges related to the implementation of reinforcement learning in conversational AI domain are identified as well as discussed in detail. Various conversational models which are based on deep reinforcement learning (as well as deep learning) are also discussed. In summary, this paper discusses key aspects of deep reinforcement learning which are crucial for designing an efficient conversational AI.",
      "published": "2017-09-15T06:18:33Z",
      "categories": [
        "cs.AI"
      ],
      "url": "https://arxiv.org/abs/1709.05067v1"
    },
    {
      "id": "2308.11336v1",
      "title": "On the Opportunities and Challenges of Offline Reinforcement Learning   for Recommender Systems",
      "authors": [
        "Xiaocong Chen",
        "Siyu Wang",
        "Julian McAuley",
        "Dietmar Jannach",
        "Lina Yao"
      ],
      "abstract": "Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain limited. This survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domain. Furthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field.",
      "published": "2023-08-22T10:28:02Z",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "url": "https://arxiv.org/abs/2308.11336v1"
    },
    {
      "id": "2401.02349v2",
      "title": "A Survey Analyzing Generalization in Deep Reinforcement Learning",
      "authors": [
        "Ezgi Korkmaz"
      ],
      "abstract": "Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formalize and analyze generalization in deep reinforcement learning. We will explain the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their generalization capabilities. Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policies. From exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth view. We believe our study can provide a compact guideline for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with higher generalization skills.",
      "published": "2024-01-04T16:45:01Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/2401.02349v2"
    },
    {
      "id": "2204.11897v3",
      "title": "Reinforcement Teaching",
      "authors": [
        "Calarina Muslimani",
        "Alex Lewandowski",
        "Dale Schuurmans",
        "Matthew E. Taylor",
        "Jun Luo"
      ],
      "abstract": "Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algorithm. Under Reinforcement Teaching, a teaching policy is learned, through reinforcement, to improve a student's learning algorithm. To learn an effective teaching policy, we introduce the parametric-behavior embedder that learns a representation of the student's learnable parameters from its input/output behavior. We further use learning progress to shape the teacher's reward, allowing it to more quickly maximize the student's performance. To demonstrate the generality of Reinforcement Teaching, we conduct experiments in which a teacher learns to significantly improve both reinforcement and supervised learning algorithms. Reinforcement Teaching outperforms previous work using heuristic reward functions and state representations, as well as other parameter representations.",
      "published": "2022-04-25T18:04:17Z",
      "categories": [
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/2204.11897v3"
    },
    {
      "id": "1606.03476v1",
      "title": "Generative Adversarial Imitation Learning",
      "authors": [
        "Jonathan Ho",
        "Stefano Ermon"
      ],
      "abstract": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.",
      "published": "2016-06-10T20:51:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "url": "https://arxiv.org/abs/1606.03476v1"
    },
    {
      "id": "2304.10098v2",
      "title": "Two-Memory Reinforcement Learning",
      "authors": [
        "Zhao Yang",
        "Thomas. M. Moerland",
        "Mike Preuss",
        "Aske Plaat"
      ],
      "abstract": "While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weaknesses. Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them. In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengths. The 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each other. Our experiments demonstrate that the 2M agent is more data efficient and outperforms both pure episodic memory and pure reinforcement learning, as well as a state-of-the-art memory-augmented RL agent. Moreover, the proposed approach provides a general framework that can be used to combine any episodic memory agent with other off-policy reinforcement learning algorithms.",
      "published": "2023-04-20T05:39:25Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "url": "https://arxiv.org/abs/2304.10098v2"
    },
    {
      "id": "1912.06310v1",
      "title": "Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning",
      "authors": [
        "Shuai L",
        "Shuai Han",
        "Wenbo Zhou",
        "Junwei Zhang"
      ],
      "abstract": "Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning, a scalable framework that combines advantages of the three methods mentioned above. The core of this framework is a dual-actors and single critic reinforcement learning agent. This agent can recruit high-fitness actors from the population of evolutionary algorithms, which instructs itself to learn from experience replay buffer. At the same time, low-fitness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and improve their adaptability. Reinforcement and imitation learners in this framework can be replaced with any off-policy actor-critic reinforcement learner or data-driven imitation learner. We evaluate RIM on a series of benchmarks for continuous control tasks in Mujoco. The experimental results show that RIM outperforms prior evolutionary or reinforcement learning methods. The performance of RIM's components is significantly better than components of previous evolutionary reinforcement learning algorithm, and the recruitment using soft update enables reinforcement learning agent to learn faster than that using hard update.",
      "published": "2019-12-13T03:26:14Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "url": "https://arxiv.org/abs/1912.06310v1"
    },
    {
      "id": "2210.00770v1",
      "title": "Accelerate Reinforcement Learning with PID Controllers in the Pendulum   Simulations",
      "authors": [
        "Liping Bai"
      ],
      "abstract": "We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL).",
      "published": "2022-10-03T08:59:12Z",
      "categories": [
        "eess.SY",
        "cs.LG",
        "cs.SY"
      ],
      "url": "https://arxiv.org/abs/2210.00770v1"
    },
    {
      "id": "2004.00993v2",
      "title": "Augmented Q Imitation Learning (AQIL)",
      "authors": [
        "Xiao Lei Zhang",
        "Anish Agarwal"
      ],
      "abstract": "The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning.",
      "published": "2020-03-31T18:08:23Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "url": "https://arxiv.org/abs/2004.00993v2"
    },
    {
      "id": "1706.05749v1",
      "title": "Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning",
      "authors": [
        "Nick Erickson",
        "Qi Zhao"
      ],
      "abstract": "This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments. We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention.",
      "published": "2017-06-19T00:16:24Z",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "url": "https://arxiv.org/abs/1706.05749v1"
    },
    {
      "id": "1809.06995v1",
      "title": "Interpretable Reinforcement Learning with Ensemble Methods",
      "authors": [
        "Alexander Brown",
        "Marek Petrik"
      ],
      "abstract": "We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods.",
      "published": "2018-09-19T03:23:35Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1809.06995v1"
    },
    {
      "id": "1806.04640v3",
      "title": "Unsupervised Meta-Learning for Reinforcement Learning",
      "authors": [
        "Abhishek Gupta",
        "Benjamin Eysenbach",
        "Chelsea Finn",
        "Sergey Levine"
      ],
      "abstract": "Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach. Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners. Our experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratch.",
      "published": "2018-06-12T16:48:52Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "url": "https://arxiv.org/abs/1806.04640v3"
    }
  ],
  "semantic_atoms": [
    {
      "id": "e528b903",
      "concept": "buenos",
      "definition": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley",
      "context": "Paper: Lecture Notes: Optimization for Machine Learning\nAuthors: Elad Hazan\nCategories: cs.LG, stat.ML\nAbstract: Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.422687",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.03550v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.03550v1"
        ]
      }
    },
    {
      "id": "20d0d6cc",
      "concept": "well",
      "definition": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley",
      "context": "Paper: Lecture Notes: Optimization for Machine Learning\nAuthors: Elad Hazan\nCategories: cs.LG, stat.ML\nAbstract: Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.425727",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.03550v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.03550v1"
        ]
      }
    },
    {
      "id": "ac2db510",
      "concept": "given",
      "definition": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley",
      "context": "Paper: Lecture Notes: Optimization for Machine Learning\nAuthors: Elad Hazan\nCategories: cs.LG, stat.ML\nAbstract: Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.425818",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.03550v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.03550v1"
        ]
      }
    },
    {
      "id": "570c3fc2",
      "concept": "machine",
      "definition": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley",
      "context": "Paper: Lecture Notes: Optimization for Machine Learning\nAuthors: Elad Hazan\nCategories: cs.LG, stat.ML\nAbstract: Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.425853",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.03550v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1909.03550v1"
        ]
      }
    },
    {
      "id": "47bf5016",
      "concept": "princeton",
      "definition": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley",
      "context": "Paper: Lecture Notes: Optimization for Machine Learning\nAuthors: Elad Hazan\nCategories: cs.LG, stat.ML\nAbstract: Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.425883",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.03550v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.03550v1"
        ]
      }
    },
    {
      "id": "9102595f",
      "concept": "course at",
      "definition": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley",
      "context": "Paper: Lecture Notes: Optimization for Machine Learning\nAuthors: Elad Hazan\nCategories: cs.LG, stat.ML\nAbstract: Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.425925",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.03550v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.03550v1"
        ]
      }
    },
    {
      "id": "ff0583d9",
      "concept": "lecture",
      "definition": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley",
      "context": "Paper: Lecture Notes: Optimization for Machine Learning\nAuthors: Elad Hazan\nCategories: cs.LG, stat.ML\nAbstract: Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.426026",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.03550v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1909.03550v1"
        ]
      }
    },
    {
      "id": "e0004a75",
      "concept": "derived",
      "definition": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley",
      "context": "Paper: Lecture Notes: Optimization for Machine Learning\nAuthors: Elad Hazan\nCategories: cs.LG, stat.ML\nAbstract: Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.426137",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.03550v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.03550v1"
        ]
      }
    },
    {
      "id": "a2eb1487",
      "concept": "learning",
      "definition": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley",
      "context": "Paper: Lecture Notes: Optimization for Machine Learning\nAuthors: Elad Hazan\nCategories: cs.LG, stat.ML\nAbstract: Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.426268",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.03550v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1909.03550v1"
        ]
      }
    },
    {
      "id": "ef25baac",
      "concept": "buenos aires",
      "definition": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley",
      "context": "Paper: Lecture Notes: Optimization for Machine Learning\nAuthors: Elad Hazan\nCategories: cs.LG, stat.ML\nAbstract: Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.426407",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.03550v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.03550v1"
        ]
      }
    },
    {
      "id": "2d15e853",
      "concept": "adversarial machine",
      "definition": "I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect",
      "context": "Paper: An Optimal Control View of Adversarial Machine Learning\nAuthors: Xiaojin Zhu\nCategories: cs.LG, stat.ML\nAbstract: I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.529530",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1811.04422v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1811.04422v1"
        ]
      }
    },
    {
      "id": "11918eb5",
      "concept": "including test",
      "definition": "This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping",
      "context": "Paper: An Optimal Control View of Adversarial Machine Learning\nAuthors: Xiaojin Zhu\nCategories: cs.LG, stat.ML\nAbstract: I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.529675",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1811.04422v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1811.04422v1"
        ]
      }
    },
    {
      "id": "84c99158",
      "concept": "machine",
      "definition": "I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect",
      "context": "Paper: An Optimal Control View of Adversarial Machine Learning\nAuthors: Xiaojin Zhu\nCategories: cs.LG, stat.ML\nAbstract: I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.529732",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1811.04422v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1811.04422v1"
        ]
      }
    },
    {
      "id": "e4408f88",
      "concept": "do harm",
      "definition": "I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect",
      "context": "Paper: An Optimal Control View of Adversarial Machine Learning\nAuthors: Xiaojin Zhu\nCategories: cs.LG, stat.ML\nAbstract: I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.529776",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1811.04422v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1811.04422v1"
        ]
      }
    },
    {
      "id": "d38c7c8e",
      "concept": "machine learner",
      "definition": "I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect",
      "context": "Paper: An Optimal Control View of Adversarial Machine Learning\nAuthors: Xiaojin Zhu\nCategories: cs.LG, stat.ML\nAbstract: I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.529815",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1811.04422v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1811.04422v1"
        ]
      }
    },
    {
      "id": "0e1334f3",
      "concept": "the view",
      "definition": "The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning",
      "context": "Paper: An Optimal Control View of Adversarial Machine Learning\nAuthors: Xiaojin Zhu\nCategories: cs.LG, stat.ML\nAbstract: I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.529857",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1811.04422v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1811.04422v1"
        ]
      }
    },
    {
      "id": "a76329a2",
      "concept": "item",
      "definition": "This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping",
      "context": "Paper: An Optimal Control View of Adversarial Machine Learning\nAuthors: Xiaojin Zhu\nCategories: cs.LG, stat.ML\nAbstract: I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.529895",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1811.04422v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1811.04422v1"
        ]
      }
    },
    {
      "id": "21284da8",
      "concept": "encourages",
      "definition": "The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning",
      "context": "Paper: An Optimal Control View of Adversarial Machine Learning\nAuthors: Xiaojin Zhu\nCategories: cs.LG, stat.ML\nAbstract: I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.529942",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1811.04422v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1811.04422v1"
        ]
      }
    },
    {
      "id": "68cfa4ce",
      "concept": "types of",
      "definition": "This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping",
      "context": "Paper: An Optimal Control View of Adversarial Machine Learning\nAuthors: Xiaojin Zhu\nCategories: cs.LG, stat.ML\nAbstract: I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.529987",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1811.04422v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1811.04422v1"
        ]
      }
    },
    {
      "id": "b97130c8",
      "concept": "system",
      "definition": "I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect",
      "context": "Paper: An Optimal Control View of Adversarial Machine Learning\nAuthors: Xiaojin Zhu\nCategories: cs.LG, stat.ML\nAbstract: I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.530028",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1811.04422v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1811.04422v1"
        ]
      }
    },
    {
      "id": "a5dcfc32",
      "concept": "to the",
      "definition": "The article is devoted to the problem of small learning samples in machine learning",
      "context": "Paper: Minimax deviation strategies for machine learning and recognition with   short learning samples\nAuthors: Michail Schlesinger, Evgeniy Vodolazskiy\nCategories: cs.LG\nAbstract: The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.633273",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.04849v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.04849v1"
        ]
      }
    },
    {
      "id": "7a9ba002",
      "concept": "and recognition",
      "definition": "The article is devoted to the problem of small learning samples in machine learning",
      "context": "Paper: Minimax deviation strategies for machine learning and recognition with   short learning samples\nAuthors: Michail Schlesinger, Evgeniy Vodolazskiy\nCategories: cs.LG\nAbstract: The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.633460",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.04849v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1707.04849v1"
        ]
      }
    },
    {
      "id": "872244f6",
      "concept": "learning is",
      "definition": "The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws",
      "context": "Paper: Minimax deviation strategies for machine learning and recognition with   short learning samples\nAuthors: Michail Schlesinger, Evgeniy Vodolazskiy\nCategories: cs.LG\nAbstract: The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.633524",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.04849v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.04849v1"
        ]
      }
    },
    {
      "id": "5c8955e3",
      "concept": "devoted",
      "definition": "The article is devoted to the problem of small learning samples in machine learning",
      "context": "Paper: Minimax deviation strategies for machine learning and recognition with   short learning samples\nAuthors: Michail Schlesinger, Evgeniy Vodolazskiy\nCategories: cs.LG\nAbstract: The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.633568",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.04849v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.04849v1"
        ]
      }
    },
    {
      "id": "d362dcfc",
      "concept": "and minimax",
      "definition": "The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws",
      "context": "Paper: Minimax deviation strategies for machine learning and recognition with   short learning samples\nAuthors: Michail Schlesinger, Evgeniy Vodolazskiy\nCategories: cs.LG\nAbstract: The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.633626",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.04849v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.04849v1"
        ]
      }
    },
    {
      "id": "08f7415d",
      "concept": "small learning",
      "definition": "The article is devoted to the problem of small learning samples in machine learning",
      "context": "Paper: Minimax deviation strategies for machine learning and recognition with   short learning samples\nAuthors: Michail Schlesinger, Evgeniy Vodolazskiy\nCategories: cs.LG\nAbstract: The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.633675",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.04849v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.04849v1"
        ]
      }
    },
    {
      "id": "35cc20aa",
      "concept": "recognition",
      "definition": "The article is devoted to the problem of small learning samples in machine learning",
      "context": "Paper: Minimax deviation strategies for machine learning and recognition with   short learning samples\nAuthors: Michail Schlesinger, Evgeniy Vodolazskiy\nCategories: cs.LG\nAbstract: The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.633727",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.04849v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1707.04849v1"
        ]
      }
    },
    {
      "id": "a6369815",
      "concept": "machine",
      "definition": "The article is devoted to the problem of small learning samples in machine learning",
      "context": "Paper: Minimax deviation strategies for machine learning and recognition with   short learning samples\nAuthors: Michail Schlesinger, Evgeniy Vodolazskiy\nCategories: cs.LG\nAbstract: The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.633779",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.04849v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1707.04849v1"
        ]
      }
    },
    {
      "id": "ceb03d78",
      "concept": "looked",
      "definition": "The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws",
      "context": "Paper: Minimax deviation strategies for machine learning and recognition with   short learning samples\nAuthors: Michail Schlesinger, Evgeniy Vodolazskiy\nCategories: cs.LG\nAbstract: The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.633824",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.04849v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.04849v1"
        ]
      }
    },
    {
      "id": "d0737d10",
      "concept": "is devoted",
      "definition": "The article is devoted to the problem of small learning samples in machine learning",
      "context": "Paper: Minimax deviation strategies for machine learning and recognition with   short learning samples\nAuthors: Michail Schlesinger, Evgeniy Vodolazskiy\nCategories: cs.LG\nAbstract: The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.633866",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.04849v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.04849v1"
        ]
      }
    },
    {
      "id": "30410f6f",
      "concept": "available datasets",
      "definition": "The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studies",
      "context": "Paper: Machine Learning for Clinical Predictive Analytics\nAuthors: Wei-Hung Weng\nCategories: cs.LG, stat.ML\nAbstract: In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.736800",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.09246v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.09246v1"
        ]
      }
    },
    {
      "id": "9eef4a92",
      "concept": "machine",
      "definition": "In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks",
      "context": "Paper: Machine Learning for Clinical Predictive Analytics\nAuthors: Wei-Hung Weng\nCategories: cs.LG, stat.ML\nAbstract: In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.736919",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.09246v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1909.09246v1"
        ]
      }
    },
    {
      "id": "6ce47fe9",
      "concept": "overview",
      "definition": "In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks",
      "context": "Paper: Machine Learning for Clinical Predictive Analytics\nAuthors: Wei-Hung Weng\nCategories: cs.LG, stat.ML\nAbstract: In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.736977",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.09246v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.09246v1"
        ]
      }
    },
    {
      "id": "0228d0d4",
      "concept": "they",
      "definition": "The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studies",
      "context": "Paper: Machine Learning for Clinical Predictive Analytics\nAuthors: Wei-Hung Weng\nCategories: cs.LG, stat.ML\nAbstract: In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.737030",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.09246v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.09246v1"
        ]
      }
    },
    {
      "id": "0b49794a",
      "concept": "provide",
      "definition": "In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks",
      "context": "Paper: Machine Learning for Clinical Predictive Analytics\nAuthors: Wei-Hung Weng\nCategories: cs.LG, stat.ML\nAbstract: In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.737076",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.09246v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.09246v1"
        ]
      }
    },
    {
      "id": "b811269b",
      "concept": "some",
      "definition": "We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms",
      "context": "Paper: Machine Learning for Clinical Predictive Analytics\nAuthors: Wei-Hung Weng\nCategories: cs.LG, stat.ML\nAbstract: In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.737123",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.09246v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.09246v1"
        ]
      }
    },
    {
      "id": "5804ed0e",
      "concept": "tasks",
      "definition": "In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks",
      "context": "Paper: Machine Learning for Clinical Predictive Analytics\nAuthors: Wei-Hung Weng\nCategories: cs.LG, stat.ML\nAbstract: In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.737170",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.09246v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.09246v1"
        ]
      }
    },
    {
      "id": "b1eecbe8",
      "concept": "vector",
      "definition": "The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studies",
      "context": "Paper: Machine Learning for Clinical Predictive Analytics\nAuthors: Wei-Hung Weng\nCategories: cs.LG, stat.ML\nAbstract: In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.737216",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.09246v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.09246v1"
        ]
      }
    },
    {
      "id": "6cb689a8",
      "concept": "to apply",
      "definition": "Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks",
      "context": "Paper: Machine Learning for Clinical Predictive Analytics\nAuthors: Wei-Hung Weng\nCategories: cs.LG, stat.ML\nAbstract: In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.737255",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.09246v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.09246v1"
        ]
      }
    },
    {
      "id": "0608e8cc",
      "concept": "how to",
      "definition": "Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks",
      "context": "Paper: Machine Learning for Clinical Predictive Analytics\nAuthors: Wei-Hung Weng\nCategories: cs.LG, stat.ML\nAbstract: In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.737295",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.09246v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.09246v1"
        ]
      }
    },
    {
      "id": "a818d166",
      "concept": "benefits and",
      "definition": "Machine learning technologies have demonstrated immense capabilities in various domains",
      "context": "Paper: Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs\nAuthors: Samiyuru Menik, Lakshmish Ramaswamy\nCategories: cs.LG, cs.SE\nAbstract: Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands i...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.839946",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2301.09753v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2301.09753v1"
        ]
      }
    },
    {
      "id": "1d948ac3",
      "concept": "analyze",
      "definition": "We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image based",
      "context": "Paper: Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs\nAuthors: Samiyuru Menik, Lakshmish Ramaswamy\nCategories: cs.LG, cs.SE\nAbstract: Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands i...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.840107",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2301.09753v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2301.09753v1"
        ]
      }
    },
    {
      "id": "53defd32",
      "concept": "adoption",
      "definition": "However, adoption of machine learning technologies has a lot of untouched potential",
      "context": "Paper: Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs\nAuthors: Samiyuru Menik, Lakshmish Ramaswamy\nCategories: cs.LG, cs.SE\nAbstract: Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands i...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.840176",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2301.09753v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2301.09753v1"
        ]
      }
    },
    {
      "id": "9cd5598a",
      "concept": "machine",
      "definition": "Machine learning technologies have demonstrated immense capabilities in various domains",
      "context": "Paper: Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs\nAuthors: Samiyuru Menik, Lakshmish Ramaswamy\nCategories: cs.LG, cs.SE\nAbstract: Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands i...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.840230",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2301.09753v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2301.09753v1"
        ]
      }
    },
    {
      "id": "bec78dc8",
      "concept": "modular machine",
      "definition": "In this work we explore the benefits of modular machine learning solutions and discuss how modular machine learning solutions can overcome some of the major solution engineering limitations of monolithic machine learning solutions",
      "context": "Paper: Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs\nAuthors: Samiyuru Menik, Lakshmish Ramaswamy\nCategories: cs.LG, cs.SE\nAbstract: Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands i...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.840293",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2301.09753v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2301.09753v1"
        ]
      }
    },
    {
      "id": "53a48f2d",
      "concept": "based and",
      "definition": "We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image based",
      "context": "Paper: Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs\nAuthors: Samiyuru Menik, Lakshmish Ramaswamy\nCategories: cs.LG, cs.SE\nAbstract: Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands i...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.840384",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2301.09753v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2301.09753v1"
        ]
      }
    },
    {
      "id": "ef93d97a",
      "concept": "image based",
      "definition": "We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image based",
      "context": "Paper: Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs\nAuthors: Samiyuru Menik, Lakshmish Ramaswamy\nCategories: cs.LG, cs.SE\nAbstract: Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands i...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.840450",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2301.09753v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2301.09753v1"
        ]
      }
    },
    {
      "id": "6af99423",
      "concept": "they",
      "definition": "They play a key role in the success of modern businesses",
      "context": "Paper: Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs\nAuthors: Samiyuru Menik, Lakshmish Ramaswamy\nCategories: cs.LG, cs.SE\nAbstract: Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands i...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.840506",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2301.09753v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2301.09753v1"
        ]
      }
    },
    {
      "id": "ed828c86",
      "concept": "today",
      "definition": "We recognize that the monolithic nature prevalent in today's machine learning applications stands in the way of efficient and cost effective customized machine learning solution development",
      "context": "Paper: Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs\nAuthors: Samiyuru Menik, Lakshmish Ramaswamy\nCategories: cs.LG, cs.SE\nAbstract: Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands i...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.840560",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2301.09753v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2301.09753v1"
        ]
      }
    },
    {
      "id": "70efa5ee",
      "concept": "solution development",
      "definition": "We recognize that the monolithic nature prevalent in today's machine learning applications stands in the way of efficient and cost effective customized machine learning solution development",
      "context": "Paper: Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs\nAuthors: Samiyuru Menik, Lakshmish Ramaswamy\nCategories: cs.LG, cs.SE\nAbstract: Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands i...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.840614",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2301.09753v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2301.09753v1"
        ]
      }
    },
    {
      "id": "429081f3",
      "concept": "formal",
      "definition": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)",
      "context": "Paper: Introduction to Machine Learning: Class Notes 67577\nAuthors: Amnon Shashua\nCategories: cs.LG\nAbstract: Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.943640",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0904.3664v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0904.3664v1"
        ]
      }
    },
    {
      "id": "8e64aec7",
      "concept": "maxent duality",
      "definition": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)",
      "context": "Paper: Introduction to Machine Learning: Class Notes 67577\nAuthors: Amnon Shashua\nCategories: cs.LG\nAbstract: Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.943756",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0904.3664v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0904.3664v1"
        ]
      }
    },
    {
      "id": "18754c90",
      "concept": "dimension",
      "definition": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)",
      "context": "Paper: Introduction to Machine Learning: Class Notes 67577\nAuthors: Amnon Shashua\nCategories: cs.LG\nAbstract: Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.943807",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0904.3664v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0904.3664v1"
        ]
      }
    },
    {
      "id": "094ee57a",
      "concept": "and pac",
      "definition": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)",
      "context": "Paper: Introduction to Machine Learning: Class Notes 67577\nAuthors: Amnon Shashua\nCategories: cs.LG\nAbstract: Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.943867",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0904.3664v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0904.3664v1"
        ]
      }
    },
    {
      "id": "7ee52082",
      "concept": "machine",
      "definition": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)",
      "context": "Paper: Introduction to Machine Learning: Class Notes 67577\nAuthors: Amnon Shashua\nCategories: cs.LG\nAbstract: Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.943909",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0904.3664v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/0904.3664v1"
        ]
      }
    },
    {
      "id": "aaec0264",
      "concept": "clustering",
      "definition": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)",
      "context": "Paper: Introduction to Machine Learning: Class Notes 67577\nAuthors: Amnon Shashua\nCategories: cs.LG\nAbstract: Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.943950",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0904.3664v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0904.3664v1"
        ]
      }
    },
    {
      "id": "4fe6fb2c",
      "concept": "vc dimension",
      "definition": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)",
      "context": "Paper: Introduction to Machine Learning: Class Notes 67577\nAuthors: Amnon Shashua\nCategories: cs.LG\nAbstract: Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.943993",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0904.3664v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0904.3664v1"
        ]
      }
    },
    {
      "id": "ace5f85d",
      "concept": "maxent",
      "definition": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)",
      "context": "Paper: Introduction to Machine Learning: Class Notes 67577\nAuthors: Amnon Shashua\nCategories: cs.LG\nAbstract: Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.944030",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0904.3664v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0904.3664v1"
        ]
      }
    },
    {
      "id": "3ae67ca5",
      "concept": "inference",
      "definition": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)",
      "context": "Paper: Introduction to Machine Learning: Class Notes 67577\nAuthors: Amnon Shashua\nCategories: cs.LG\nAbstract: Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.944066",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0904.3664v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0904.3664v1"
        ]
      }
    },
    {
      "id": "ff679b88",
      "concept": "learning",
      "definition": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)",
      "context": "Paper: Introduction to Machine Learning: Class Notes 67577\nAuthors: Amnon Shashua\nCategories: cs.LG\nAbstract: Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:32.944101",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0904.3664v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/0904.3664v1"
        ]
      }
    },
    {
      "id": "937c762a",
      "concept": "influenced",
      "definition": "Machine learning techniques have influenced the field of computer architecture like many other fields",
      "context": "Paper: The Tribes of Machine Learning and the Realm of Computer Architecture\nAuthors: Ayaz Akram, Jason Lowe-Power\nCategories: cs.LG, cs.AR\nAbstract: Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for comput...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.048070",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2012.04105v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2012.04105v1"
        ]
      }
    },
    {
      "id": "a2945ff8",
      "concept": "research",
      "definition": "We also provide a detailed survey of computer architecture research that employs different machine learning methods",
      "context": "Paper: The Tribes of Machine Learning and the Realm of Computer Architecture\nAuthors: Ayaz Akram, Jason Lowe-Power\nCategories: cs.LG, cs.AR\nAbstract: Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for comput...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.048136",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2012.04105v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2012.04105v1"
        ]
      }
    },
    {
      "id": "c36a700f",
      "concept": "the outstanding",
      "definition": "Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture",
      "context": "Paper: The Tribes of Machine Learning and the Realm of Computer Architecture\nAuthors: Ayaz Akram, Jason Lowe-Power\nCategories: cs.LG, cs.AR\nAbstract: Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for comput...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.048175",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2012.04105v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2012.04105v1"
        ]
      }
    },
    {
      "id": "572eee85",
      "concept": "learning and",
      "definition": "Machine learning techniques have influenced the field of computer architecture like many other fields",
      "context": "Paper: The Tribes of Machine Learning and the Realm of Computer Architecture\nAuthors: Ayaz Akram, Jason Lowe-Power\nCategories: cs.LG, cs.AR\nAbstract: Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for comput...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.048217",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2012.04105v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2012.04105v1"
        ]
      }
    },
    {
      "id": "4503a937",
      "concept": "field of",
      "definition": "Machine learning techniques have influenced the field of computer architecture like many other fields",
      "context": "Paper: The Tribes of Machine Learning and the Realm of Computer Architecture\nAuthors: Ayaz Akram, Jason Lowe-Power\nCategories: cs.LG, cs.AR\nAbstract: Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for comput...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.048268",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2012.04105v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2012.04105v1"
        ]
      }
    },
    {
      "id": "c3d64e60",
      "concept": "be applied",
      "definition": "This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems",
      "context": "Paper: The Tribes of Machine Learning and the Realm of Computer Architecture\nAuthors: Ayaz Akram, Jason Lowe-Power\nCategories: cs.LG, cs.AR\nAbstract: Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for comput...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.048386",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2012.04105v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2012.04105v1"
        ]
      }
    },
    {
      "id": "953a478b",
      "concept": "to exploit",
      "definition": "Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture",
      "context": "Paper: The Tribes of Machine Learning and the Realm of Computer Architecture\nAuthors: Ayaz Akram, Jason Lowe-Power\nCategories: cs.LG, cs.AR\nAbstract: Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for comput...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.048497",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2012.04105v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2012.04105v1"
        ]
      }
    },
    {
      "id": "d464b970",
      "concept": "computer",
      "definition": "Machine learning techniques have influenced the field of computer architecture like many other fields",
      "context": "Paper: The Tribes of Machine Learning and the Realm of Computer Architecture\nAuthors: Ayaz Akram, Jason Lowe-Power\nCategories: cs.LG, cs.AR\nAbstract: Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for comput...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.048564",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2012.04105v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2012.04105v1"
        ]
      }
    },
    {
      "id": "551dbba9",
      "concept": "outstanding",
      "definition": "Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture",
      "context": "Paper: The Tribes of Machine Learning and the Realm of Computer Architecture\nAuthors: Ayaz Akram, Jason Lowe-Power\nCategories: cs.LG, cs.AR\nAbstract: Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for comput...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.048625",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2012.04105v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2012.04105v1"
        ]
      }
    },
    {
      "id": "547f0bc2",
      "concept": "techniques",
      "definition": "Machine learning techniques have influenced the field of computer architecture like many other fields",
      "context": "Paper: The Tribes of Machine Learning and the Realm of Computer Architecture\nAuthors: Ayaz Akram, Jason Lowe-Power\nCategories: cs.LG, cs.AR\nAbstract: Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for comput...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.051408",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2012.04105v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2012.04105v1"
        ]
      }
    },
    {
      "id": "811e7cc0",
      "concept": "required",
      "definition": "While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist",
      "context": "Paper: A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning\nAuthors: Randy J. Chase, David R. Harrison, Amanda Burke\nCategories: physics.ao-ph, cs.LG\nAbstract: Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of mach...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.154239",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.07492v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.07492v2"
        ]
      }
    },
    {
      "id": "846c0e19",
      "concept": "furthermore",
      "definition": "Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorology",
      "context": "Paper: A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning\nAuthors: Randy J. Chase, David R. Harrison, Amanda Burke\nCategories: physics.ao-ph, cs.LG\nAbstract: Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of mach...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.154454",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.07492v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.07492v2"
        ]
      }
    },
    {
      "id": "0dca6eb0",
      "concept": "machine",
      "definition": "Recently, the use of machine learning in meteorology has increased greatly",
      "context": "Paper: A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning\nAuthors: Randy J. Chase, David R. Harrison, Amanda Burke\nCategories: physics.ao-ph, cs.LG\nAbstract: Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of mach...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.154528",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.07492v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2204.07492v2"
        ]
      }
    },
    {
      "id": "f46debcf",
      "concept": "provided",
      "definition": "Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorology",
      "context": "Paper: A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning\nAuthors: Randy J. Chase, David R. Harrison, Amanda Burke\nCategories: physics.ao-ph, cs.LG\nAbstract: Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of mach...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.154583",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.07492v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.07492v2"
        ]
      }
    },
    {
      "id": "2dde2f9b",
      "concept": "learning methods",
      "definition": "While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist",
      "context": "Paper: A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning\nAuthors: Randy J. Chase, David R. Harrison, Amanda Burke\nCategories: physics.ao-ph, cs.LG\nAbstract: Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of mach...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.154647",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.07492v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.07492v2"
        ]
      }
    },
    {
      "id": "75ec0eef",
      "concept": "this paper",
      "definition": "To reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods",
      "context": "Paper: A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning\nAuthors: Randy J. Chase, David R. Harrison, Amanda Burke\nCategories: physics.ao-ph, cs.LG\nAbstract: Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of mach...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.154697",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.07492v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.07492v2"
        ]
      }
    },
    {
      "id": "8d972cdf",
      "concept": "language",
      "definition": "A familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain language",
      "context": "Paper: A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning\nAuthors: Randy J. Chase, David R. Harrison, Amanda Burke\nCategories: physics.ao-ph, cs.LG\nAbstract: Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of mach...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.154749",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.07492v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.07492v2"
        ]
      }
    },
    {
      "id": "eb3655fd",
      "concept": "and lower",
      "definition": "To reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods",
      "context": "Paper: A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning\nAuthors: Randy J. Chase, David R. Harrison, Amanda Burke\nCategories: physics.ao-ph, cs.LG\nAbstract: Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of mach...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.154800",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.07492v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.07492v2"
        ]
      }
    },
    {
      "id": "6135cf38",
      "concept": "some",
      "definition": "To reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods",
      "context": "Paper: A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning\nAuthors: Randy J. Chase, David R. Harrison, Amanda Burke\nCategories: physics.ao-ph, cs.LG\nAbstract: Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of mach...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.154852",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.07492v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.07492v2"
        ]
      }
    },
    {
      "id": "f64cde61",
      "concept": "demonstrated",
      "definition": "The following machine learning methods are demonstrated: linear regression; logistic regression; decision trees; random forest; gradient boosted decision trees; naive Bayes; and support vector machines",
      "context": "Paper: A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning\nAuthors: Randy J. Chase, David R. Harrison, Amanda Burke\nCategories: physics.ao-ph, cs.LG\nAbstract: Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of mach...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.154894",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.07492v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.07492v2"
        ]
      }
    },
    {
      "id": "b7ab5ec0",
      "concept": "can be",
      "definition": "At the same time, machine learning experts warn that machine learning models can be biased as well",
      "context": "Paper: Understanding Bias in Machine Learning\nAuthors: Jindong Gu, Daniela Oelke\nCategories: cs.LG, stat.ML\nAbstract: Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.260700",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.01866v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.01866v1"
        ]
      }
    },
    {
      "id": "76a052f5",
      "concept": "reach",
      "definition": "To reach such a goal, we develop interactive plots to visualizing the bias learned from synthetic data",
      "context": "Paper: Understanding Bias in Machine Learning\nAuthors: Jindong Gu, Daniela Oelke\nCategories: cs.LG, stat.ML\nAbstract: Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.260858",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.01866v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.01866v1"
        ]
      }
    },
    {
      "id": "84607bbe",
      "concept": "fair",
      "definition": "Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc",
      "context": "Paper: Understanding Bias in Machine Learning\nAuthors: Jindong Gu, Daniela Oelke\nCategories: cs.LG, stat.ML\nAbstract: Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.260915",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.01866v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.01866v1"
        ]
      }
    },
    {
      "id": "83561e8f",
      "concept": "machine",
      "definition": "Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem",
      "context": "Paper: Understanding Bias in Machine Learning\nAuthors: Jindong Gu, Daniela Oelke\nCategories: cs.LG, stat.ML\nAbstract: Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.260955",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.01866v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1909.01866v1"
        ]
      }
    },
    {
      "id": "311eae89",
      "concept": "even",
      "definition": "Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem",
      "context": "Paper: Understanding Bias in Machine Learning\nAuthors: Jindong Gu, Daniela Oelke\nCategories: cs.LG, stat.ML\nAbstract: Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.261001",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.01866v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.01866v1"
        ]
      }
    },
    {
      "id": "a6c2b9f3",
      "concept": "impediment to",
      "definition": "Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc",
      "context": "Paper: Understanding Bias in Machine Learning\nAuthors: Jindong Gu, Daniela Oelke\nCategories: cs.LG, stat.ML\nAbstract: Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.261040",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.01866v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.01866v1"
        ]
      }
    },
    {
      "id": "242dc6f3",
      "concept": "fair decisions",
      "definition": "Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc",
      "context": "Paper: Understanding Bias in Machine Learning\nAuthors: Jindong Gu, Daniela Oelke\nCategories: cs.LG, stat.ML\nAbstract: Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.261083",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.01866v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.01866v1"
        ]
      }
    },
    {
      "id": "941740fb",
      "concept": "methods for",
      "definition": "Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem",
      "context": "Paper: Understanding Bias in Machine Learning\nAuthors: Jindong Gu, Daniela Oelke\nCategories: cs.LG, stat.ML\nAbstract: Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.261122",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.01866v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.01866v1"
        ]
      }
    },
    {
      "id": "2f5d9f15",
      "concept": "hope has",
      "definition": "Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem",
      "context": "Paper: Understanding Bias in Machine Learning\nAuthors: Jindong Gu, Daniela Oelke\nCategories: cs.LG, stat.ML\nAbstract: Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.261165",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.01866v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.01866v1"
        ]
      }
    },
    {
      "id": "69c22d60",
      "concept": "at the",
      "definition": "Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem",
      "context": "Paper: Understanding Bias in Machine Learning\nAuthors: Jindong Gu, Daniela Oelke\nCategories: cs.LG, stat.ML\nAbstract: Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.261202",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.01866v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.01866v1"
        ]
      }
    },
    {
      "id": "5a2fb7f4",
      "concept": "systems",
      "definition": "If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward",
      "context": "Paper: Position Paper: Towards Transparent Machine Learning\nAuthors: Dustin Juliano\nCategories: cs.LG\nAbstract: Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.362094",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.06612v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.06612v1"
        ]
      }
    },
    {
      "id": "12c87a15",
      "concept": "the goal",
      "definition": "The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs",
      "context": "Paper: Position Paper: Towards Transparent Machine Learning\nAuthors: Dustin Juliano\nCategories: cs.LG\nAbstract: Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.362166",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.06612v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.06612v1"
        ]
      }
    },
    {
      "id": "e679c479",
      "concept": "machine",
      "definition": "Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form",
      "context": "Paper: Position Paper: Towards Transparent Machine Learning\nAuthors: Dustin Juliano\nCategories: cs.LG\nAbstract: Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.362187",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.06612v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1911.06612v1"
        ]
      }
    },
    {
      "id": "c8711489",
      "concept": "verify",
      "definition": "The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs",
      "context": "Paper: Position Paper: Towards Transparent Machine Learning\nAuthors: Dustin Juliano\nCategories: cs.LG\nAbstract: Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.362218",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.06612v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.06612v1"
        ]
      }
    },
    {
      "id": "1a1fd62e",
      "concept": "refine",
      "definition": "The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs",
      "context": "Paper: Position Paper: Towards Transparent Machine Learning\nAuthors: Dustin Juliano\nCategories: cs.LG\nAbstract: Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.362243",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.06612v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.06612v1"
        ]
      }
    },
    {
      "id": "7d9c5f3e",
      "concept": "could",
      "definition": "If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward",
      "context": "Paper: Position Paper: Towards Transparent Machine Learning\nAuthors: Dustin Juliano\nCategories: cs.LG\nAbstract: Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.362269",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.06612v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.06612v1"
        ]
      }
    },
    {
      "id": "d9e66584",
      "concept": "introduced as",
      "definition": "Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form",
      "context": "Paper: Position Paper: Towards Transparent Machine Learning\nAuthors: Dustin Juliano\nCategories: cs.LG\nAbstract: Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.362291",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.06612v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.06612v1"
        ]
      }
    },
    {
      "id": "0897cfb4",
      "concept": "system",
      "definition": "Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form",
      "context": "Paper: Position Paper: Towards Transparent Machine Learning\nAuthors: Dustin Juliano\nCategories: cs.LG\nAbstract: Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.362313",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.06612v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.06612v1"
        ]
      }
    },
    {
      "id": "bf0ee540",
      "concept": "transparent",
      "definition": "Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form",
      "context": "Paper: Position Paper: Towards Transparent Machine Learning\nAuthors: Dustin Juliano\nCategories: cs.LG\nAbstract: Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.362337",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.06612v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1911.06612v1"
        ]
      }
    },
    {
      "id": "9e81a91a",
      "concept": "to learn",
      "definition": "The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs",
      "context": "Paper: Position Paper: Towards Transparent Machine Learning\nAuthors: Dustin Juliano\nCategories: cs.LG\nAbstract: Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.362380",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.06612v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.06612v1"
        ]
      }
    },
    {
      "id": "e78456ba",
      "concept": "can be",
      "definition": "The paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisis",
      "context": "Paper: A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain\nAuthors: Tao Wang\nCategories: cs.LG, cs.CR\nAbstract: Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.468803",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1903.08801v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1903.08801v1"
        ]
      }
    },
    {
      "id": "a07dba80",
      "concept": "running with",
      "definition": "Third, it enables a computer to translate core machine learning implementation from a single thread on a single machine to multiple threads on multiple machines running with blockchain by using a unified approach",
      "context": "Paper: A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain\nAuthors: Tao Wang\nCategories: cs.LG, cs.CR\nAbstract: Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.468931",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1903.08801v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1903.08801v1"
        ]
      }
    },
    {
      "id": "c9c6ab13",
      "concept": "contributions",
      "definition": "This paper makes three contributions",
      "context": "Paper: A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain\nAuthors: Tao Wang\nCategories: cs.LG, cs.CR\nAbstract: Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.468977",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1903.08801v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1903.08801v1"
        ]
      }
    },
    {
      "id": "138979b7",
      "concept": "rule",
      "definition": "The paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisis",
      "context": "Paper: A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain\nAuthors: Tao Wang\nCategories: cs.LG, cs.CR\nAbstract: Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.469024",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1903.08801v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1903.08801v1"
        ]
      }
    },
    {
      "id": "543f78f9",
      "concept": "uses",
      "definition": "The paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisis",
      "context": "Paper: A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain\nAuthors: Tao Wang\nCategories: cs.LG, cs.CR\nAbstract: Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.469073",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1903.08801v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1903.08801v1"
        ]
      }
    },
    {
      "id": "b488b85e",
      "concept": "translate core",
      "definition": "Third, it enables a computer to translate core machine learning implementation from a single thread on a single machine to multiple threads on multiple machines running with blockchain by using a unified approach",
      "context": "Paper: A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain\nAuthors: Tao Wang\nCategories: cs.LG, cs.CR\nAbstract: Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.469121",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1903.08801v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1903.08801v1"
        ]
      }
    },
    {
      "id": "80d0a668",
      "concept": "without",
      "definition": "Previously, machine learning and blockchain have been considered two independent technologies without an obvious link",
      "context": "Paper: A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain\nAuthors: Tao Wang\nCategories: cs.LG, cs.CR\nAbstract: Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.469165",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1903.08801v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1903.08801v1"
        ]
      }
    },
    {
      "id": "feba4b20",
      "concept": "the data",
      "definition": "Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted",
      "context": "Paper: A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain\nAuthors: Tao Wang\nCategories: cs.LG, cs.CR\nAbstract: Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.469207",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1903.08801v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1903.08801v1"
        ]
      }
    },
    {
      "id": "a43a059d",
      "concept": "permanent",
      "definition": "This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way",
      "context": "Paper: A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain\nAuthors: Tao Wang\nCategories: cs.LG, cs.CR\nAbstract: Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.469249",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1903.08801v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1903.08801v1"
        ]
      }
    },
    {
      "id": "b92df15f",
      "concept": "analyze",
      "definition": "The paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisis",
      "context": "Paper: A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain\nAuthors: Tao Wang\nCategories: cs.LG, cs.CR\nAbstract: Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.471643",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1903.08801v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1903.08801v1"
        ]
      }
    },
    {
      "id": "845921f7",
      "concept": "structured data",
      "definition": "We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds",
      "context": "Paper: MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?\nAuthors: Yu Liu, Hantian Zhang, Luyuan Zeng\nCategories: cs.DC, cs.LG, stat.ML\nAbstract: We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is po...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.574376",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.09562v3",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1707.09562v3"
        ]
      }
    },
    {
      "id": "7df462e1",
      "concept": "to run",
      "definition": "Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest",
      "context": "Paper: MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?\nAuthors: Yu Liu, Hantian Zhang, Luyuan Zeng\nCategories: cs.DC, cs.LG, stat.ML\nAbstract: We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is po...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.574577",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.09562v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1707.09562v3"
        ]
      }
    },
    {
      "id": "b0e6d6e0",
      "concept": "winning code",
      "definition": "We then compare the performance of the top winning code available from Kaggle with that of running machine learning clouds from both Azure and Amazon on mlbench",
      "context": "Paper: MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?\nAuthors: Yu Liu, Hantian Zhang, Luyuan Zeng\nCategories: cs.DC, cs.LG, stat.ML\nAbstract: We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is po...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.574646",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.09562v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1707.09562v3"
        ]
      }
    },
    {
      "id": "28baa057",
      "concept": "rarely comes",
      "definition": "Raising the level of abstraction, however, rarely comes free - a performance penalty is possible",
      "context": "Paper: MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?\nAuthors: Yu Liu, Hantian Zhang, Luyuan Zeng\nCategories: cs.DC, cs.LG, stat.ML\nAbstract: We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is po...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.574698",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.09562v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1707.09562v3"
        ]
      }
    },
    {
      "id": "41149f6d",
      "concept": "amazon on",
      "definition": "We then compare the performance of the top winning code available from Kaggle with that of running machine learning clouds from both Azure and Amazon on mlbench",
      "context": "Paper: MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?\nAuthors: Yu Liu, Hantian Zhang, Luyuan Zeng\nCategories: cs.DC, cs.LG, stat.ML\nAbstract: We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is po...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.574745",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.09562v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1707.09562v3"
        ]
      }
    },
    {
      "id": "5ab528f7",
      "concept": "machine",
      "definition": "We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds",
      "context": "Paper: MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?\nAuthors: Yu Liu, Hantian Zhang, Luyuan Zeng\nCategories: cs.DC, cs.LG, stat.ML\nAbstract: We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is po...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.574793",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.09562v3",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1707.09562v3"
        ]
      }
    },
    {
      "id": "0e0f3465",
      "concept": "provided",
      "definition": "We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds",
      "context": "Paper: MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?\nAuthors: Yu Liu, Hantian Zhang, Luyuan Zeng\nCategories: cs.DC, cs.LG, stat.ML\nAbstract: We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is po...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.574839",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.09562v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1707.09562v3"
        ]
      }
    },
    {
      "id": "e3f92e2c",
      "concept": "study of",
      "definition": "We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds",
      "context": "Paper: MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?\nAuthors: Yu Liu, Hantian Zhang, Luyuan Zeng\nCategories: cs.DC, cs.LG, stat.ML\nAbstract: We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is po...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.574880",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.09562v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1707.09562v3"
        ]
      }
    },
    {
      "id": "57777f5d",
      "concept": "tasks",
      "definition": "We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds",
      "context": "Paper: MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?\nAuthors: Yu Liu, Hantian Zhang, Luyuan Zeng\nCategories: cs.DC, cs.LG, stat.ML\nAbstract: We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is po...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.574965",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.09562v3",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1707.09562v3"
        ]
      }
    },
    {
      "id": "2aec61dc",
      "concept": "from kaggle",
      "definition": "We present mlbench, a novel benchmark constructed by harvesting datasets from Kaggle competitions",
      "context": "Paper: MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?\nAuthors: Yu Liu, Hantian Zhang, Luyuan Zeng\nCategories: cs.DC, cs.LG, stat.ML\nAbstract: We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is po...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.575024",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.09562v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1707.09562v3"
        ]
      }
    },
    {
      "id": "d6ab5c52",
      "concept": "start",
      "definition": "We start with a brief review of data marketplaces and pricing desiderata",
      "context": "Paper: Data Pricing in Machine Learning Pipelines\nAuthors: Zicun Cong, Xuan Luo, Pei Jian\nCategories: cs.LG\nAbstract: Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be s...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.676775",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.07915v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.07915v1"
        ]
      }
    },
    {
      "id": "67cbafb7",
      "concept": "research",
      "definition": "In this article, we survey the principles and the latest research development of data pricing in machine learning pipelines",
      "context": "Paper: Data Pricing in Machine Learning Pipelines\nAuthors: Zicun Cong, Xuan Luo, Pei Jian\nCategories: cs.LG\nAbstract: Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be s...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.676854",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.07915v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.07915v1"
        ]
      }
    },
    {
      "id": "d6fc2ecf",
      "concept": "deployment",
      "definition": "We also investigate pricing in the step of collaborative training of machine learning models, and overview pricing machine learning models for end users in the step of machine learning deployment",
      "context": "Paper: Data Pricing in Machine Learning Pipelines\nAuthors: Zicun Cong, Xuan Luo, Pei Jian\nCategories: cs.LG\nAbstract: Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be s...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.676908",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.07915v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.07915v1"
        ]
      }
    },
    {
      "id": "553f70f8",
      "concept": "penetrating",
      "definition": "Data is critical and penetrating in the whole machine learning pipelines",
      "context": "Paper: Data Pricing in Machine Learning Pipelines\nAuthors: Zicun Cong, Xuan Luo, Pei Jian\nCategories: cs.LG\nAbstract: Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be s...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.676953",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.07915v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.07915v1"
        ]
      }
    },
    {
      "id": "632c222d",
      "concept": "whole",
      "definition": "Data is critical and penetrating in the whole machine learning pipelines",
      "context": "Paper: Data Pricing in Machine Learning Pipelines\nAuthors: Zicun Cong, Xuan Luo, Pei Jian\nCategories: cs.LG\nAbstract: Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be s...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.676996",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.07915v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.07915v1"
        ]
      }
    },
    {
      "id": "38979a72",
      "concept": "machine",
      "definition": "Machine learning is disruptive",
      "context": "Paper: Data Pricing in Machine Learning Pipelines\nAuthors: Zicun Cong, Xuan Luo, Pei Jian\nCategories: cs.LG\nAbstract: Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be s...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.677036",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.07915v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2108.07915v1"
        ]
      }
    },
    {
      "id": "44b2da3b",
      "concept": "overview",
      "definition": "We also investigate pricing in the step of collaborative training of machine learning models, and overview pricing machine learning models for end users in the step of machine learning deployment",
      "context": "Paper: Data Pricing in Machine Learning Pipelines\nAuthors: Zicun Cong, Xuan Luo, Pei Jian\nCategories: cs.LG\nAbstract: Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be s...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.677083",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.07915v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.07915v1"
        ]
      }
    },
    {
      "id": "4b775ee3",
      "concept": "survey",
      "definition": "In this article, we survey the principles and the latest research development of data pricing in machine learning pipelines",
      "context": "Paper: Data Pricing in Machine Learning Pipelines\nAuthors: Zicun Cong, Xuan Luo, Pei Jian\nCategories: cs.LG\nAbstract: Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be s...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.677126",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.07915v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.07915v1"
        ]
      }
    },
    {
      "id": "70f4914e",
      "concept": "as machine",
      "definition": "As machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many parties",
      "context": "Paper: Data Pricing in Machine Learning Pipelines\nAuthors: Zicun Cong, Xuan Luo, Pei Jian\nCategories: cs.LG\nAbstract: Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be s...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.677170",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.07915v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.07915v1"
        ]
      }
    },
    {
      "id": "cc1b15ab",
      "concept": "dynamic eco",
      "definition": "As machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many parties",
      "context": "Paper: Data Pricing in Machine Learning Pipelines\nAuthors: Zicun Cong, Xuan Luo, Pei Jian\nCategories: cs.LG\nAbstract: Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be s...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.677216",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.07915v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.07915v1"
        ]
      }
    },
    {
      "id": "e2681871",
      "concept": "without",
      "definition": "It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience",
      "context": "Paper: Techniques for Automated Machine Learning\nAuthors: Yi-Wei Chen, Qingquan Song, Xia Hu\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperpara...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.782450",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.08908v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1907.08908v1"
        ]
      }
    },
    {
      "id": "c77d31be",
      "concept": "off-the",
      "definition": "It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience",
      "context": "Paper: Techniques for Automated Machine Learning\nAuthors: Yi-Wei Chen, Qingquan Song, Xia Hu\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperpara...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.782619",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.08908v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1907.08908v1"
        ]
      }
    },
    {
      "id": "1ad60057",
      "concept": "given",
      "definition": "Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem",
      "context": "Paper: Techniques for Automated Machine Learning\nAuthors: Yi-Wei Chen, Qingquan Song, Xia Hu\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperpara...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.782733",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.08908v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1907.08908v1"
        ]
      }
    },
    {
      "id": "ddeda6ab",
      "concept": "and enable",
      "definition": "It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience",
      "context": "Paper: Techniques for Automated Machine Learning\nAuthors: Yi-Wei Chen, Qingquan Song, Xia Hu\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperpara...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.782845",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.08908v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1907.08908v1"
        ]
      }
    },
    {
      "id": "f2a565eb",
      "concept": "the current",
      "definition": "In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL)",
      "context": "Paper: Techniques for Automated Machine Learning\nAuthors: Yi-Wei Chen, Qingquan Song, Xia Hu\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperpara...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.782959",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.08908v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1907.08908v1"
        ]
      }
    },
    {
      "id": "9bdb8e9b",
      "concept": "state-of",
      "definition": "State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches",
      "context": "Paper: Techniques for Automated Machine Learning\nAuthors: Yi-Wei Chen, Qingquan Song, Xia Hu\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperpara...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.783077",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.08908v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1907.08908v1"
        ]
      }
    },
    {
      "id": "51a5f919",
      "concept": "machine",
      "definition": "Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem",
      "context": "Paper: Techniques for Automated Machine Learning\nAuthors: Yi-Wei Chen, Qingquan Song, Xia Hu\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperpara...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.783186",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.08908v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1907.08908v1"
        ]
      }
    },
    {
      "id": "fab40a8a",
      "concept": "presented",
      "definition": "State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches",
      "context": "Paper: Techniques for Automated Machine Learning\nAuthors: Yi-Wei Chen, Qingquan Song, Xia Hu\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperpara...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.783303",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.08908v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1907.08908v1"
        ]
      }
    },
    {
      "id": "bc046db5",
      "concept": "data scientists",
      "definition": "It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience",
      "context": "Paper: Techniques for Automated Machine Learning\nAuthors: Yi-Wei Chen, Qingquan Song, Xia Hu\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperpara...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.789819",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.08908v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1907.08908v1"
        ]
      }
    },
    {
      "id": "4e023d28",
      "concept": "adopted in",
      "definition": "State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches",
      "context": "Paper: Techniques for Automated Machine Learning\nAuthors: Yi-Wei Chen, Qingquan Song, Xia Hu\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperpara...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.790004",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.08908v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1907.08908v1"
        ]
      }
    },
    {
      "id": "45dc8e99",
      "concept": "has become",
      "definition": "With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products",
      "context": "Paper: The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning\nAuthors: Omer Subasi, Oceane Bel, Joseph Manzano\nCategories: cs.LG, cs.AI, cs.DC\nAbstract: With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.898282",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.03120v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.03120v1"
        ]
      }
    },
    {
      "id": "4d9ab500",
      "concept": "level overview",
      "definition": "We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks",
      "context": "Paper: The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning\nAuthors: Omer Subasi, Oceane Bel, Joseph Manzano\nCategories: cs.LG, cs.AI, cs.DC\nAbstract: With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.898447",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.03120v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.03120v1"
        ]
      }
    },
    {
      "id": "bd89ae96",
      "concept": "systems",
      "definition": "With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products",
      "context": "Paper: The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning\nAuthors: Omer Subasi, Oceane Bel, Joseph Manzano\nCategories: cs.LG, cs.AI, cs.DC\nAbstract: With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.898499",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.03120v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.03120v1"
        ]
      }
    },
    {
      "id": "144f7ac8",
      "concept": "research",
      "definition": "With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products",
      "context": "Paper: The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning\nAuthors: Omer Subasi, Oceane Bel, Joseph Manzano\nCategories: cs.LG, cs.AI, cs.DC\nAbstract: With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.898568",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.03120v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.03120v1"
        ]
      }
    },
    {
      "id": "0464ce6d",
      "concept": "discussion",
      "definition": "Our discussion encompasses parallel distributed learning, deep learning as well as federated learning",
      "context": "Paper: The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning\nAuthors: Omer Subasi, Oceane Bel, Joseph Manzano\nCategories: cs.LG, cs.AI, cs.DC\nAbstract: With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.898615",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.03120v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.03120v1"
        ]
      }
    },
    {
      "id": "b7f2c348",
      "concept": "machine",
      "definition": "With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products",
      "context": "Paper: The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning\nAuthors: Omer Subasi, Oceane Bel, Joseph Manzano\nCategories: cs.LG, cs.AI, cs.DC\nAbstract: With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.898651",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.03120v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2312.03120v1"
        ]
      }
    },
    {
      "id": "60f36d6d",
      "concept": "overview",
      "definition": "We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks",
      "context": "Paper: The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning\nAuthors: Omer Subasi, Oceane Bel, Joseph Manzano\nCategories: cs.LG, cs.AI, cs.DC\nAbstract: With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.898691",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.03120v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.03120v1"
        ]
      }
    },
    {
      "id": "73cf116c",
      "concept": "and deep",
      "definition": "In this study, we present a review of modern machine and deep learning",
      "context": "Paper: The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning\nAuthors: Omer Subasi, Oceane Bel, Joseph Manzano\nCategories: cs.LG, cs.AI, cs.DC\nAbstract: With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.898726",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.03120v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.03120v1"
        ]
      }
    },
    {
      "id": "b42571ff",
      "concept": "provide",
      "definition": "We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks",
      "context": "Paper: The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning\nAuthors: Omer Subasi, Oceane Bel, Joseph Manzano\nCategories: cs.LG, cs.AI, cs.DC\nAbstract: With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.898765",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.03120v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.03120v1"
        ]
      }
    },
    {
      "id": "94ca8f53",
      "concept": "deep",
      "definition": "In this study, we present a review of modern machine and deep learning",
      "context": "Paper: The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning\nAuthors: Omer Subasi, Oceane Bel, Joseph Manzano\nCategories: cs.LG, cs.AI, cs.DC\nAbstract: With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:33.898807",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.03120v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.03120v1"
        ]
      }
    },
    {
      "id": "9d389ec7",
      "concept": "research",
      "definition": "To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark",
      "context": "Paper: Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark\nAuthors: Jiajun Shen\nCategories: cs.DC\nAbstract: With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and eff...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.002011",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2206.07090v2",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2206.07090v2"
        ]
      }
    },
    {
      "id": "cbf21f4c",
      "concept": "analyze",
      "definition": "However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine",
      "context": "Paper: Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark\nAuthors: Jiajun Shen\nCategories: cs.DC\nAbstract: With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and eff...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.002120",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2206.07090v2",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2206.07090v2"
        ]
      }
    },
    {
      "id": "b2e4e5f9",
      "concept": "machine",
      "definition": "However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine",
      "context": "Paper: Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark\nAuthors: Jiajun Shen\nCategories: cs.DC\nAbstract: With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and eff...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.002163",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2206.07090v2",
        "extraction_confidence": 0.8999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2206.07090v2"
        ]
      }
    },
    {
      "id": "c42be446",
      "concept": "the research",
      "definition": "The research results have shown significant improvement in runtime and efficiency of parallelized machine learning algorithms",
      "context": "Paper: Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark\nAuthors: Jiajun Shen\nCategories: cs.DC\nAbstract: With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and eff...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.002207",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2206.07090v2",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2206.07090v2"
        ]
      }
    },
    {
      "id": "a1c5ca6c",
      "concept": "respectively",
      "definition": "To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark",
      "context": "Paper: Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark\nAuthors: Jiajun Shen\nCategories: cs.DC\nAbstract: With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and eff...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.002249",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2206.07090v2",
        "extraction_confidence": 0.8999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2206.07090v2"
        ]
      }
    },
    {
      "id": "1ec78c6a",
      "concept": "parallelized",
      "definition": "We compare the runtime and efficiency of traditional machine learning algorithms with parallelized machine learning algorithms respectively on the single machine and Spark platform",
      "context": "Paper: Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark\nAuthors: Jiajun Shen\nCategories: cs.DC\nAbstract: With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and eff...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.002292",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2206.07090v2",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2206.07090v2"
        ]
      }
    },
    {
      "id": "40815685",
      "concept": "this paper",
      "definition": "To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark",
      "context": "Paper: Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark\nAuthors: Jiajun Shen\nCategories: cs.DC\nAbstract: With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and eff...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.002327",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2206.07090v2",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2206.07090v2"
        ]
      }
    },
    {
      "id": "71f2e6de",
      "concept": "spark",
      "definition": "To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark",
      "context": "Paper: Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark\nAuthors: Jiajun Shen\nCategories: cs.DC\nAbstract: With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and eff...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.002390",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2206.07090v2",
        "extraction_confidence": 0.8999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2206.07090v2"
        ]
      }
    },
    {
      "id": "622487a0",
      "concept": "results have",
      "definition": "The research results have shown significant improvement in runtime and efficiency of parallelized machine learning algorithms",
      "context": "Paper: Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark\nAuthors: Jiajun Shen\nCategories: cs.DC\nAbstract: With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and eff...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.002433",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2206.07090v2",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2206.07090v2"
        ]
      }
    },
    {
      "id": "fd474bd2",
      "concept": "some",
      "definition": "To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark",
      "context": "Paper: Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark\nAuthors: Jiajun Shen\nCategories: cs.DC\nAbstract: With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and eff...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.002468",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2206.07090v2",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2206.07090v2"
        ]
      }
    },
    {
      "id": "d8f968a2",
      "concept": "required",
      "definition": "It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge",
      "context": "Paper: AutoCompete: A Framework for Machine Learning Competition\nAuthors: Abhishek Thakur, Artus Krohn-Grimberghe\nCategories: stat.ML, cs.LG\nAbstract: In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.115166",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1507.02188v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1507.02188v1"
        ]
      }
    },
    {
      "id": "77786f3b",
      "concept": "given machine",
      "definition": "It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge",
      "context": "Paper: AutoCompete: A Framework for Machine Learning Competition\nAuthors: Abhishek Thakur, Artus Krohn-Grimberghe\nCategories: stat.ML, cs.LG\nAbstract: In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.115259",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1507.02188v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1507.02188v1"
        ]
      }
    },
    {
      "id": "bcb4303e",
      "concept": "over",
      "definition": "This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions",
      "context": "Paper: AutoCompete: A Framework for Machine Learning Competition\nAuthors: Abhishek Thakur, Artus Krohn-Grimberghe\nCategories: stat.ML, cs.LG\nAbstract: In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.115293",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1507.02188v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1507.02188v1"
        ]
      }
    },
    {
      "id": "dc039dbe",
      "concept": "given",
      "definition": "It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge",
      "context": "Paper: AutoCompete: A Framework for Machine Learning Competition\nAuthors: Abhishek Thakur, Artus Krohn-Grimberghe\nCategories: stat.ML, cs.LG\nAbstract: In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.115326",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1507.02188v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1507.02188v1"
        ]
      }
    },
    {
      "id": "89c8eaea",
      "concept": "than",
      "definition": "This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions",
      "context": "Paper: AutoCompete: A Framework for Machine Learning Competition\nAuthors: Abhishek Thakur, Artus Krohn-Grimberghe\nCategories: stat.ML, cs.LG\nAbstract: In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.115371",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1507.02188v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1507.02188v1"
        ]
      }
    },
    {
      "id": "82cb45fc",
      "concept": "machine",
      "definition": "In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions",
      "context": "Paper: AutoCompete: A Framework for Machine Learning Competition\nAuthors: Abhishek Thakur, Artus Krohn-Grimberghe\nCategories: stat.ML, cs.LG\nAbstract: In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.115406",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1507.02188v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1507.02188v1"
        ]
      }
    },
    {
      "id": "7a5491d8",
      "concept": "highly automated",
      "definition": "In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions",
      "context": "Paper: AutoCompete: A Framework for Machine Learning Competition\nAuthors: Abhishek Thakur, Artus Krohn-Grimberghe\nCategories: stat.ML, cs.LG\nAbstract: In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.115440",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1507.02188v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1507.02188v1"
        ]
      }
    },
    {
      "id": "c925abd5",
      "concept": "provided",
      "definition": "The proposed system helps in identifying data types, choosing a machine learn- ing model, tuning hyper-parameters, avoiding over-fitting and optimization for a provided evaluation metric",
      "context": "Paper: AutoCompete: A Framework for Machine Learning Competition\nAuthors: Abhishek Thakur, Artus Krohn-Grimberghe\nCategories: stat.ML, cs.LG\nAbstract: In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.115487",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1507.02188v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1507.02188v1"
        ]
      }
    },
    {
      "id": "caf13d96",
      "concept": "more",
      "definition": "This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions",
      "context": "Paper: AutoCompete: A Framework for Machine Learning Competition\nAuthors: Abhishek Thakur, Artus Krohn-Grimberghe\nCategories: stat.ML, cs.LG\nAbstract: In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.115537",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1507.02188v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1507.02188v1"
        ]
      }
    },
    {
      "id": "9707f8f4",
      "concept": "two years",
      "definition": "This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions",
      "context": "Paper: AutoCompete: A Framework for Machine Learning Competition\nAuthors: Abhishek Thakur, Artus Krohn-Grimberghe\nCategories: stat.ML, cs.LG\nAbstract: In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.115571",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1507.02188v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1507.02188v1"
        ]
      }
    },
    {
      "id": "c5e21db4",
      "concept": "greedily",
      "definition": "Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks",
      "context": "Paper: Joint Training of Deep Boltzmann Machines\nAuthors: Ian Goodfellow, Aaron Courville, Yoshua Bengio\nCategories: stat.ML, cs.LG\nAbstract: We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.218713",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1212.2686v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1212.2686v1"
        ]
      }
    },
    {
      "id": "54502b71",
      "concept": "trains the",
      "definition": "Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks",
      "context": "Paper: Joint Training of Deep Boltzmann Machines\nAuthors: Ian Goodfellow, Aaron Courville, Yoshua Bengio\nCategories: stat.ML, cs.LG\nAbstract: We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.218865",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1212.2686v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1212.2686v1"
        ]
      }
    },
    {
      "id": "2f5c3ee9",
      "concept": "time",
      "definition": "Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks",
      "context": "Paper: Joint Training of Deep Boltzmann Machines\nAuthors: Ian Goodfellow, Aaron Courville, Yoshua Bengio\nCategories: stat.ML, cs.LG\nAbstract: We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.218924",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1212.2686v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1212.2686v1"
        ]
      }
    },
    {
      "id": "69fc5dfb",
      "concept": "new method",
      "definition": "We introduce a new method for training deep Boltzmann machines jointly",
      "context": "Paper: Joint Training of Deep Boltzmann Machines\nAuthors: Ian Goodfellow, Aaron Courville, Yoshua Bengio\nCategories: stat.ML, cs.LG\nAbstract: We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.218986",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1212.2686v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1212.2686v1"
        ]
      }
    },
    {
      "id": "19497b53",
      "concept": "well",
      "definition": "Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks",
      "context": "Paper: Joint Training of Deep Boltzmann Machines\nAuthors: Ian Goodfellow, Aaron Courville, Yoshua Bengio\nCategories: stat.ML, cs.LG\nAbstract: We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.219038",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1212.2686v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1212.2686v1"
        ]
      }
    },
    {
      "id": "6a92e88b",
      "concept": "introduce",
      "definition": "We introduce a new method for training deep Boltzmann machines jointly",
      "context": "Paper: Joint Training of Deep Boltzmann Machines\nAuthors: Ian Goodfellow, Aaron Courville, Yoshua Bengio\nCategories: stat.ML, cs.LG\nAbstract: We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.219084",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1212.2686v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1212.2686v1"
        ]
      }
    },
    {
      "id": "4a3c6d24",
      "concept": "require",
      "definition": "Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks",
      "context": "Paper: Joint Training of Deep Boltzmann Machines\nAuthors: Ian Goodfellow, Aaron Courville, Yoshua Bengio\nCategories: stat.ML, cs.LG\nAbstract: We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.219135",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1212.2686v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1212.2686v1"
        ]
      }
    },
    {
      "id": "cb10ed70",
      "concept": "machine",
      "definition": "We introduce a new method for training deep Boltzmann machines jointly",
      "context": "Paper: Joint Training of Deep Boltzmann Machines\nAuthors: Ian Goodfellow, Aaron Courville, Yoshua Bengio\nCategories: stat.ML, cs.LG\nAbstract: We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.219222",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1212.2686v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1212.2686v1"
        ]
      }
    },
    {
      "id": "b96b555b",
      "concept": "one layer",
      "definition": "Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks",
      "context": "Paper: Joint Training of Deep Boltzmann Machines\nAuthors: Ian Goodfellow, Aaron Courville, Yoshua Bengio\nCategories: stat.ML, cs.LG\nAbstract: We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.219288",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1212.2686v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1212.2686v1"
        ]
      }
    },
    {
      "id": "6e881959",
      "concept": "layer",
      "definition": "Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks",
      "context": "Paper: Joint Training of Deep Boltzmann Machines\nAuthors: Ian Goodfellow, Aaron Courville, Yoshua Bengio\nCategories: stat.ML, cs.LG\nAbstract: We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.219335",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1212.2686v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1212.2686v1"
        ]
      }
    },
    {
      "id": "5ab3e26e",
      "concept": "in new",
      "definition": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York",
      "context": "Paper: Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications\nAuthors: Kush R. Varshney\nCategories: stat.ML, cs.CY, cs.LG\nAbstract: This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.325767",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1607.02450v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1607.02450v2"
        ]
      }
    },
    {
      "id": "0217fa7b",
      "concept": "which was",
      "definition": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York",
      "context": "Paper: Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications\nAuthors: Kush R. Varshney\nCategories: stat.ML, cs.CY, cs.LG\nAbstract: This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.325923",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1607.02450v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1607.02450v2"
        ]
      }
    },
    {
      "id": "c408864b",
      "concept": "applications",
      "definition": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York",
      "context": "Paper: Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications\nAuthors: Kush R. Varshney\nCategories: stat.ML, cs.CY, cs.LG\nAbstract: This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.326050",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1607.02450v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1607.02450v2"
        ]
      }
    },
    {
      "id": "fbf19d8f",
      "concept": "workshop",
      "definition": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York",
      "context": "Paper: Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications\nAuthors: Kush R. Varshney\nCategories: stat.ML, cs.CY, cs.LG\nAbstract: This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.326160",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1607.02450v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1607.02450v2"
        ]
      }
    },
    {
      "id": "460616fc",
      "concept": "machine",
      "definition": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York",
      "context": "Paper: Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications\nAuthors: Kush R. Varshney\nCategories: stat.ML, cs.CY, cs.LG\nAbstract: This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.326261",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1607.02450v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1607.02450v2"
        ]
      }
    },
    {
      "id": "a5668207",
      "concept": "june",
      "definition": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York",
      "context": "Paper: Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications\nAuthors: Kush R. Varshney\nCategories: stat.ML, cs.CY, cs.LG\nAbstract: This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.326380",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1607.02450v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1607.02450v2"
        ]
      }
    },
    {
      "id": "445770f1",
      "concept": "york",
      "definition": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York",
      "context": "Paper: Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications\nAuthors: Kush R. Varshney\nCategories: stat.ML, cs.CY, cs.LG\nAbstract: This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.326486",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1607.02450v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1607.02450v2"
        ]
      }
    },
    {
      "id": "005e4514",
      "concept": "held",
      "definition": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York",
      "context": "Paper: Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications\nAuthors: Kush R. Varshney\nCategories: stat.ML, cs.CY, cs.LG\nAbstract: This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.326588",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1607.02450v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1607.02450v2"
        ]
      }
    },
    {
      "id": "45caf1c0",
      "concept": "of the",
      "definition": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York",
      "context": "Paper: Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications\nAuthors: Kush R. Varshney\nCategories: stat.ML, cs.CY, cs.LG\nAbstract: This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.326701",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1607.02450v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1607.02450v2"
        ]
      }
    },
    {
      "id": "52f57127",
      "concept": "social good",
      "definition": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York",
      "context": "Paper: Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications\nAuthors: Kush R. Varshney\nCategories: stat.ML, cs.CY, cs.LG\nAbstract: This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.326811",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1607.02450v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1607.02450v2"
        ]
      }
    },
    {
      "id": "4564ec63",
      "concept": "to rnns",
      "definition": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective",
      "context": "Paper: Mathematical Perspective of Machine Learning\nAuthors: Yarema Boryshchak\nCategories: cs.LG, stat.ML, 68T07\nAbstract: We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.429727",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.01503v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2007.01503v1"
        ]
      }
    },
    {
      "id": "fcac6b1f",
      "concept": "function",
      "definition": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective",
      "context": "Paper: Mathematical Perspective of Machine Learning\nAuthors: Yarema Boryshchak\nCategories: cs.LG, stat.ML, 68T07\nAbstract: We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.429874",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.01503v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2007.01503v1"
        ]
      }
    },
    {
      "id": "a204ca92",
      "concept": "approximation",
      "definition": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective",
      "context": "Paper: Mathematical Perspective of Machine Learning\nAuthors: Yarema Boryshchak\nCategories: cs.LG, stat.ML, 68T07\nAbstract: We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.430001",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.01503v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2007.01503v1"
        ]
      }
    },
    {
      "id": "c3493209",
      "concept": "machine",
      "definition": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective",
      "context": "Paper: Mathematical Perspective of Machine Learning\nAuthors: Yarema Boryshchak\nCategories: cs.LG, stat.ML, 68T07\nAbstract: We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.430119",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.01503v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2007.01503v1"
        ]
      }
    },
    {
      "id": "058f4c06",
      "concept": "at some",
      "definition": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective",
      "context": "Paper: Mathematical Perspective of Machine Learning\nAuthors: Yarema Boryshchak\nCategories: cs.LG, stat.ML, 68T07\nAbstract: We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.430257",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.01503v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2007.01503v1"
        ]
      }
    },
    {
      "id": "87509a8c",
      "concept": "networks",
      "definition": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective",
      "context": "Paper: Mathematical Perspective of Machine Learning\nAuthors: Yarema Boryshchak\nCategories: cs.LG, stat.ML, 68T07\nAbstract: We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.430395",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.01503v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2007.01503v1"
        ]
      }
    },
    {
      "id": "5632b601",
      "concept": "closer",
      "definition": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective",
      "context": "Paper: Mathematical Perspective of Machine Learning\nAuthors: Yarema Boryshchak\nCategories: cs.LG, stat.ML, 68T07\nAbstract: We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.430509",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.01503v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2007.01503v1"
        ]
      }
    },
    {
      "id": "6f68f931",
      "concept": "limitations of",
      "definition": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective",
      "context": "Paper: Mathematical Perspective of Machine Learning\nAuthors: Yarema Boryshchak\nCategories: cs.LG, stat.ML, 68T07\nAbstract: We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.430621",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.01503v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2007.01503v1"
        ]
      }
    },
    {
      "id": "be614b59",
      "concept": "gradient descent",
      "definition": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective",
      "context": "Paper: Mathematical Perspective of Machine Learning\nAuthors: Yarema Boryshchak\nCategories: cs.LG, stat.ML, 68T07\nAbstract: We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.430745",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.01503v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2007.01503v1"
        ]
      }
    },
    {
      "id": "fda07d94",
      "concept": "theoretical challenges",
      "definition": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective",
      "context": "Paper: Mathematical Perspective of Machine Learning\nAuthors: Yarema Boryshchak\nCategories: cs.LG, stat.ML, 68T07\nAbstract: We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:34.430863",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.01503v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2007.01503v1"
        ]
      }
    },
    {
      "id": "5a250bed",
      "concept": "answers multiple",
      "definition": "This dissertation proposes that the neural network of deep learning is a physical system, examines deep learning from three different perspectives: microscopic, macroscopic, and physical world views, answers multiple theoretical puzzles in deep learning by using physics principles",
      "context": "Paper: Opening the black box of deep learning\nAuthors: Dian Lei, Xiaoxiao Chen, Jianfei Zhao\nCategories: cs.LG, stat.ML\nAbstract: The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physica...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.050710",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.08355v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1805.08355v1"
        ]
      }
    },
    {
      "id": "104058b5",
      "concept": "research",
      "definition": "At present, most of the theoretical research on deep learning is based on mathematics",
      "context": "Paper: Opening the black box of deep learning\nAuthors: Dian Lei, Xiaoxiao Chen, Jianfei Zhao\nCategories: cs.LG, stat.ML\nAbstract: The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physica...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.050784",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.08355v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1805.08355v1"
        ]
      }
    },
    {
      "id": "688bc2a5",
      "concept": "explains",
      "definition": "For example, from the perspective of quantum mechanics and statistical physics, this dissertation presents the calculation methods for convolution calculation, pooling, normalization, and Restricted Boltzmann Machine, as well as the selection of cost functions, explains why deep learning must be deep, what characteristics are learned in deep learning, why Convolutional Neural Networks do not have to be trained layer by layer, and the limitations of deep learning, etc",
      "context": "Paper: Opening the black box of deep learning\nAuthors: Dian Lei, Xiaoxiao Chen, Jianfei Zhao\nCategories: cs.LG, stat.ML\nAbstract: The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physica...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.050824",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.08355v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1805.08355v1"
        ]
      }
    },
    {
      "id": "03d837a4",
      "concept": "insights into",
      "definition": "The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism",
      "context": "Paper: Opening the black box of deep learning\nAuthors: Dian Lei, Xiaoxiao Chen, Jianfei Zhao\nCategories: cs.LG, stat.ML\nAbstract: The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physica...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.050863",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.08355v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1805.08355v1"
        ]
      }
    },
    {
      "id": "23d857a2",
      "concept": "application",
      "definition": "The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism",
      "context": "Paper: Opening the black box of deep learning\nAuthors: Dian Lei, Xiaoxiao Chen, Jianfei Zhao\nCategories: cs.LG, stat.ML\nAbstract: The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physica...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.050901",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.08355v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1805.08355v1"
        ]
      }
    },
    {
      "id": "e39d4a52",
      "concept": "machine",
      "definition": "For example, from the perspective of quantum mechanics and statistical physics, this dissertation presents the calculation methods for convolution calculation, pooling, normalization, and Restricted Boltzmann Machine, as well as the selection of cost functions, explains why deep learning must be deep, what characteristics are learned in deep learning, why Convolutional Neural Networks do not have to be trained layer by layer, and the limitations of deep learning, etc",
      "context": "Paper: Opening the black box of deep learning\nAuthors: Dian Lei, Xiaoxiao Chen, Jianfei Zhao\nCategories: cs.LG, stat.ML\nAbstract: The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physica...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.050944",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.08355v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1805.08355v1"
        ]
      }
    },
    {
      "id": "c545c5b0",
      "concept": "black box",
      "definition": "The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism",
      "context": "Paper: Opening the black box of deep learning\nAuthors: Dian Lei, Xiaoxiao Chen, Jianfei Zhao\nCategories: cs.LG, stat.ML\nAbstract: The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physica...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.050989",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.08355v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1805.08355v1"
        ]
      }
    },
    {
      "id": "78c5bdd1",
      "concept": "meaningful",
      "definition": "The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism",
      "context": "Paper: Opening the black box of deep learning\nAuthors: Dian Lei, Xiaoxiao Chen, Jianfei Zhao\nCategories: cs.LG, stat.ML\nAbstract: The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physica...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.051030",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.08355v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1805.08355v1"
        ]
      }
    },
    {
      "id": "776ed066",
      "concept": "deep",
      "definition": "The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism",
      "context": "Paper: Opening the black box of deep learning\nAuthors: Dian Lei, Xiaoxiao Chen, Jianfei Zhao\nCategories: cs.LG, stat.ML\nAbstract: The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physica...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.051071",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.08355v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1805.08355v1"
        ]
      }
    },
    {
      "id": "7522f8fe",
      "concept": "mechanism",
      "definition": "The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism",
      "context": "Paper: Opening the black box of deep learning\nAuthors: Dian Lei, Xiaoxiao Chen, Jianfei Zhao\nCategories: cs.LG, stat.ML\nAbstract: The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physica...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.051114",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.08355v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1805.08355v1"
        ]
      }
    },
    {
      "id": "ad5fc5db",
      "concept": "convergence",
      "definition": "Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately",
      "context": "Paper: Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning\nAuthors: Aras R. Dargazany\nCategories: cs.NE, cs.LG\nAbstract: The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.153107",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.02130v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1908.02130v1"
        ]
      }
    },
    {
      "id": "5e99e55e",
      "concept": "research",
      "definition": "The past, present and future of deep learning is presented in this work",
      "context": "Paper: Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning\nAuthors: Aras R. Dargazany\nCategories: cs.NE, cs.LG\nAbstract: The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.153222",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.02130v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1908.02130v1"
        ]
      }
    },
    {
      "id": "1229c143",
      "concept": "given",
      "definition": "Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately",
      "context": "Paper: Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning\nAuthors: Aras R. Dargazany\nCategories: cs.NE, cs.LG\nAbstract: The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.153301",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.02130v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1908.02130v1"
        ]
      }
    },
    {
      "id": "2b4a2e5b",
      "concept": "and future",
      "definition": "The past, present and future of deep learning is presented in this work",
      "context": "Paper: Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning\nAuthors: Aras R. Dargazany\nCategories: cs.NE, cs.LG\nAbstract: The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.153373",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.02130v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1908.02130v1"
        ]
      }
    },
    {
      "id": "76f8d631",
      "concept": "towards",
      "definition": "The past, present and future of deep learning is presented in this work",
      "context": "Paper: Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning\nAuthors: Aras R. Dargazany\nCategories: cs.NE, cs.LG\nAbstract: The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.153429",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.02130v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1908.02130v1"
        ]
      }
    },
    {
      "id": "9473ed57",
      "concept": "presented",
      "definition": "The past, present and future of deep learning is presented in this work",
      "context": "Paper: Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning\nAuthors: Aras R. Dargazany\nCategories: cs.NE, cs.LG\nAbstract: The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.153476",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.02130v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1908.02130v1"
        ]
      }
    },
    {
      "id": "2c3347ae",
      "concept": "cortical learning",
      "definition": "Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately",
      "context": "Paper: Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning\nAuthors: Aras R. Dargazany\nCategories: cs.NE, cs.LG\nAbstract: The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.153522",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.02130v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1908.02130v1"
        ]
      }
    },
    {
      "id": "87c92936",
      "concept": "deep",
      "definition": "The past, present and future of deep learning is presented in this work",
      "context": "Paper: Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning\nAuthors: Aras R. Dargazany\nCategories: cs.NE, cs.LG\nAbstract: The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.153564",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.02130v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1908.02130v1"
        ]
      }
    },
    {
      "id": "08d26242",
      "concept": "future",
      "definition": "The past, present and future of deep learning is presented in this work",
      "context": "Paper: Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning\nAuthors: Aras R. Dargazany\nCategories: cs.NE, cs.LG\nAbstract: The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.153606",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.02130v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1908.02130v1"
        ]
      }
    },
    {
      "id": "5830cfbf",
      "concept": "the past",
      "definition": "The past, present and future of deep learning is presented in this work",
      "context": "Paper: Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning\nAuthors: Aras R. Dargazany\nCategories: cs.NE, cs.LG\nAbstract: The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.153650",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.02130v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1908.02130v1"
        ]
      }
    },
    {
      "id": "f7cf0021",
      "concept": "interpretability",
      "definition": "CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data",
      "context": "Paper: Concept-Oriented Deep Learning\nAuthors: Daniel T Chang\nCategories: cs.AI\nAbstract: Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, co...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.258259",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.01756v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1806.01756v1"
        ]
      }
    },
    {
      "id": "0387de65",
      "concept": "systems",
      "definition": "We discuss the major aspects of CODL including concept graph, concept representations, concept exemplars, and concept representation learning systems supporting incremental and continual learning",
      "context": "Paper: Concept-Oriented Deep Learning\nAuthors: Daniel T Chang\nCategories: cs.AI\nAbstract: Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, co...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.258405",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.01756v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1806.01756v1"
        ]
      }
    },
    {
      "id": "f5f0da0a",
      "concept": "continual",
      "definition": "We discuss the major aspects of CODL including concept graph, concept representations, concept exemplars, and concept representation learning systems supporting incremental and continual learning",
      "context": "Paper: Concept-Oriented Deep Learning\nAuthors: Daniel T Chang\nCategories: cs.AI\nAbstract: Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, co...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.258458",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.01756v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1806.01756v1"
        ]
      }
    },
    {
      "id": "63b602c8",
      "concept": "and concept",
      "definition": "We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability",
      "context": "Paper: Concept-Oriented Deep Learning\nAuthors: Daniel T Chang\nCategories: cs.AI\nAbstract: Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, co...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.258493",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.01756v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1806.01756v1"
        ]
      }
    },
    {
      "id": "f616d23a",
      "concept": "incremental and",
      "definition": "We discuss the major aspects of CODL including concept graph, concept representations, concept exemplars, and concept representation learning systems supporting incremental and continual learning",
      "context": "Paper: Concept-Oriented Deep Learning\nAuthors: Daniel T Chang\nCategories: cs.AI\nAbstract: Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, co...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.258528",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.01756v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1806.01756v1"
        ]
      }
    },
    {
      "id": "8ced6f8e",
      "concept": "labeled",
      "definition": "CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data",
      "context": "Paper: Concept-Oriented Deep Learning\nAuthors: Daniel T Chang\nCategories: cs.AI\nAbstract: Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, co...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.258565",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.01756v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1806.01756v1"
        ]
      }
    },
    {
      "id": "e3822d2d",
      "concept": "codl including",
      "definition": "We discuss the major aspects of CODL including concept graph, concept representations, concept exemplars, and concept representation learning systems supporting incremental and continual learning",
      "context": "Paper: Concept-Oriented Deep Learning\nAuthors: Daniel T Chang\nCategories: cs.AI\nAbstract: Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, co...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.258601",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.01756v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1806.01756v1"
        ]
      }
    },
    {
      "id": "edbbaf10",
      "concept": "we propose",
      "definition": "We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability",
      "context": "Paper: Concept-Oriented Deep Learning\nAuthors: Daniel T Chang\nCategories: cs.AI\nAbstract: Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, co...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.258636",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.01756v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1806.01756v1"
        ]
      }
    },
    {
      "id": "47b9bf81",
      "concept": "oriented deep",
      "definition": "We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability",
      "context": "Paper: Concept-Oriented Deep Learning\nAuthors: Daniel T Chang\nCategories: cs.AI\nAbstract: Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, co...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.258670",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.01756v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1806.01756v1"
        ]
      }
    },
    {
      "id": "bc95c155",
      "concept": "the major",
      "definition": "CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data",
      "context": "Paper: Concept-Oriented Deep Learning\nAuthors: Daniel T Chang\nCategories: cs.AI\nAbstract: Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, co...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.258703",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.01756v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1806.01756v1"
        ]
      }
    },
    {
      "id": "c543ebb6",
      "concept": "they use",
      "definition": "Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do their deep learning models look like",
      "context": "Paper: A First Look at Deep Learning Apps on Smartphones\nAuthors: Mengwei Xu, Jiawei Liu, Yuanqiang Liu\nCategories: cs.LG, cs.CY\nAbstract: We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.363957",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1812.05448v4",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1812.05448v4"
        ]
      }
    },
    {
      "id": "7e01e3d6",
      "concept": "research",
      "definition": "To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild",
      "context": "Paper: A First Look at Deep Learning Apps on Smartphones\nAuthors: Mengwei Xu, Jiawei Liu, Yuanqiang Liu\nCategories: cs.LG, cs.CY\nAbstract: We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.364024",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1812.05448v4",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1812.05448v4"
        ]
      }
    },
    {
      "id": "6e9c1a11",
      "concept": "the protection",
      "definition": "On the other hand, our findings urge optimizations on deep learning models deployed on smartphones, the protection of these models, and validation of research ideas on these models",
      "context": "Paper: A First Look at Deep Learning Apps on Smartphones\nAuthors: Mengwei Xu, Jiawei Liu, Yuanqiang Liu\nCategories: cs.LG, cs.CY\nAbstract: We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.364062",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1812.05448v4",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1812.05448v4"
        ]
      }
    },
    {
      "id": "5381564a",
      "concept": "prosperity of",
      "definition": "On one hand, our findings paint a promising picture of deep learning for smartphones, showing the prosperity of mobile deep learning frameworks as well as the prosperity of apps building their cores atop deep learning",
      "context": "Paper: A First Look at Deep Learning Apps on Smartphones\nAuthors: Mengwei Xu, Jiawei Liu, Yuanqiang Liu\nCategories: cs.LG, cs.CY\nAbstract: We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.364099",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1812.05448v4",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1812.05448v4"
        ]
      }
    },
    {
      "id": "80cc2b3d",
      "concept": "at deep",
      "definition": "We are in the dawn of deep learning explosion for smartphones",
      "context": "Paper: A First Look at Deep Learning Apps on Smartphones\nAuthors: Mengwei Xu, Jiawei Liu, Yuanqiang Liu\nCategories: cs.LG, cs.CY\nAbstract: We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.364136",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1812.05448v4",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1812.05448v4"
        ]
      }
    },
    {
      "id": "b0c83df7",
      "concept": "hand",
      "definition": "On one hand, our findings paint a promising picture of deep learning for smartphones, showing the prosperity of mobile deep learning frameworks as well as the prosperity of apps building their cores atop deep learning",
      "context": "Paper: A First Look at Deep Learning Apps on Smartphones\nAuthors: Mengwei Xu, Jiawei Liu, Yuanqiang Liu\nCategories: cs.LG, cs.CY\nAbstract: We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.364172",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1812.05448v4",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1812.05448v4"
        ]
      }
    },
    {
      "id": "eb16dc84",
      "concept": "of research",
      "definition": "On the other hand, our findings urge optimizations on deep learning models deployed on smartphones, the protection of these models, and validation of research ideas on these models",
      "context": "Paper: A First Look at Deep Learning Apps on Smartphones\nAuthors: Mengwei Xu, Jiawei Liu, Yuanqiang Liu\nCategories: cs.LG, cs.CY\nAbstract: We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.364208",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1812.05448v4",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1812.05448v4"
        ]
      }
    },
    {
      "id": "65968ff7",
      "concept": "they",
      "definition": "Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do their deep learning models look like",
      "context": "Paper: A First Look at Deep Learning Apps on Smartphones\nAuthors: Mengwei Xu, Jiawei Liu, Yuanqiang Liu\nCategories: cs.LG, cs.CY\nAbstract: We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.364240",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1812.05448v4",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1812.05448v4"
        ]
      }
    },
    {
      "id": "e345beb1",
      "concept": "and deep",
      "definition": "Our study has strong implications for app developers, smartphone vendors, and deep learning R\\&D",
      "context": "Paper: A First Look at Deep Learning Apps on Smartphones\nAuthors: Mengwei Xu, Jiawei Liu, Yuanqiang Liu\nCategories: cs.LG, cs.CY\nAbstract: We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.364274",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1812.05448v4",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1812.05448v4"
        ]
      }
    },
    {
      "id": "edd594fc",
      "concept": "their deep",
      "definition": "To this end, we build a new static tool that dissects apps and analyzes their deep learning functions",
      "context": "Paper: A First Look at Deep Learning Apps on Smartphones\nAuthors: Mengwei Xu, Jiawei Liu, Yuanqiang Liu\nCategories: cs.LG, cs.CY\nAbstract: We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do the...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.364306",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1812.05448v4",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1812.05448v4"
        ]
      }
    },
    {
      "id": "5bec037d",
      "concept": "systems",
      "definition": "How to understand deep learning systems remains an open problem",
      "context": "Paper: Geometrization of deep networks for the interpretability of deep   learning systems\nAuthors: Xiao Dong, Ling Zhou\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.465131",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.02354v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1901.02354v2"
        ]
      }
    },
    {
      "id": "b9ed2af6",
      "concept": "rule",
      "definition": "Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world",
      "context": "Paper: Geometrization of deep networks for the interpretability of deep   learning systems\nAuthors: Xiao Dong, Ling Zhou\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.465199",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.02354v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1901.02354v2"
        ]
      }
    },
    {
      "id": "d58e7953",
      "concept": "of the",
      "definition": "Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world",
      "context": "Paper: Geometrization of deep networks for the interpretability of deep   learning systems\nAuthors: Xiao Dong, Ling Zhou\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.465244",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.02354v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1901.02354v2"
        ]
      }
    },
    {
      "id": "77018a2c",
      "concept": "deep",
      "definition": "How to understand deep learning systems remains an open problem",
      "context": "Paper: Geometrization of deep networks for the interpretability of deep   learning systems\nAuthors: Xiao Dong, Ling Zhou\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.465284",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.02354v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1901.02354v2"
        ]
      }
    },
    {
      "id": "9ea9d418",
      "concept": "new scheme",
      "definition": "Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world",
      "context": "Paper: Geometrization of deep networks for the interpretability of deep   learning systems\nAuthors: Xiao Dong, Ling Zhou\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.465324",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.02354v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1901.02354v2"
        ]
      }
    },
    {
      "id": "b989dc99",
      "concept": "network",
      "definition": "In this paper we propose that the answer may lie in the geometrization of deep networks",
      "context": "Paper: Geometrization of deep networks for the interpretability of deep   learning systems\nAuthors: Xiao Dong, Ling Zhou\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.465378",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.02354v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1901.02354v2"
        ]
      }
    },
    {
      "id": "e6d0fcdf",
      "concept": "how to",
      "definition": "How to understand deep learning systems remains an open problem",
      "context": "Paper: Geometrization of deep networks for the interpretability of deep   learning systems\nAuthors: Xiao Dong, Ling Zhou\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.465420",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.02354v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1901.02354v2"
        ]
      }
    },
    {
      "id": "155f91c9",
      "concept": "and quantum",
      "definition": "Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world",
      "context": "Paper: Geometrization of deep networks for the interpretability of deep   learning systems\nAuthors: Xiao Dong, Ling Zhou\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.465460",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.02354v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1901.02354v2"
        ]
      }
    },
    {
      "id": "180a8164",
      "concept": "be used",
      "definition": "By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it may also help to solve the interpretability problem of deep learning systems",
      "context": "Paper: Geometrization of deep networks for the interpretability of deep   learning systems\nAuthors: Xiao Dong, Ling Zhou\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.465501",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.02354v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1901.02354v2"
        ]
      }
    },
    {
      "id": "ac091cf4",
      "concept": "open",
      "definition": "How to understand deep learning systems remains an open problem",
      "context": "Paper: Geometrization of deep networks for the interpretability of deep   learning systems\nAuthors: Xiao Dong, Ling Zhou\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it ma...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.465538",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.02354v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1901.02354v2"
        ]
      }
    },
    {
      "id": "7138d467",
      "concept": "research",
      "definition": "The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012",
      "context": "Paper: Why & When Deep Learning Works: Looking Inside Deep Learnings\nAuthors: Ronny Ronen\nCategories: cs.LG\nAbstract: The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.573782",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1705.03921v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1705.03921v1"
        ]
      }
    },
    {
      "id": "f88f906d",
      "concept": "asked six",
      "definition": "We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential",
      "context": "Paper: Why & When Deep Learning Works: Looking Inside Deep Learnings\nAuthors: Ronny Ronen\nCategories: cs.LG\nAbstract: The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.573923",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1705.03921v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1705.03921v1"
        ]
      }
    },
    {
      "id": "fbcf10e8",
      "concept": "machine",
      "definition": "The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012",
      "context": "Paper: Why & When Deep Learning Works: Looking Inside Deep Learnings\nAuthors: Ronny Ronen\nCategories: cs.LG\nAbstract: The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.573975",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1705.03921v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1705.03921v1"
        ]
      }
    },
    {
      "id": "f93b7a12",
      "concept": "institute for",
      "definition": "The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012",
      "context": "Paper: Why & When Deep Learning Works: Looking Inside Deep Learnings\nAuthors: Ronny Ronen\nCategories: cs.LG\nAbstract: The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.574025",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1705.03921v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1705.03921v1"
        ]
      }
    },
    {
      "id": "a6e14bca",
      "concept": "and deep",
      "definition": "The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012",
      "context": "Paper: Why & When Deep Learning Works: Looking Inside Deep Learnings\nAuthors: Ronny Ronen\nCategories: cs.LG\nAbstract: The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.574068",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1705.03921v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1705.03921v1"
        ]
      }
    },
    {
      "id": "7f6d9e3d",
      "concept": "deep",
      "definition": "The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012",
      "context": "Paper: Why & When Deep Learning Works: Looking Inside Deep Learnings\nAuthors: Ronny Ronen\nCategories: cs.LG\nAbstract: The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.574123",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1705.03921v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1705.03921v1"
        ]
      }
    },
    {
      "id": "ca313264",
      "concept": "resulted",
      "definition": "The output of this challenge resulted in five papers that address different facets of deep learning",
      "context": "Paper: Why & When Deep Learning Works: Looking Inside Deep Learnings\nAuthors: Ronny Ronen\nCategories: cs.LG\nAbstract: The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.574171",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1705.03921v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1705.03921v1"
        ]
      }
    },
    {
      "id": "f23cd5ba",
      "concept": "intelligence",
      "definition": "The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012",
      "context": "Paper: Why & When Deep Learning Works: Looking Inside Deep Learnings\nAuthors: Ronny Ronen\nCategories: cs.LG\nAbstract: The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.574211",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1705.03921v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1705.03921v1"
        ]
      }
    },
    {
      "id": "3f8b898e",
      "concept": "deep learning",
      "definition": "The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012",
      "context": "Paper: Why & When Deep Learning Works: Looking Inside Deep Learnings\nAuthors: Ronny Ronen\nCategories: cs.LG\nAbstract: The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.574252",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1705.03921v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1705.03921v1"
        ]
      }
    },
    {
      "id": "6fa91bd4",
      "concept": "their",
      "definition": "We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential",
      "context": "Paper: Why & When Deep Learning Works: Looking Inside Deep Learnings\nAuthors: Ronny Ronen\nCategories: cs.LG\nAbstract: The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.574296",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1705.03921v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1705.03921v1"
        ]
      }
    },
    {
      "id": "4fec510e",
      "concept": "can be",
      "definition": "Our method can be viewed as improving the robustness of deep learning systems from both the learning task and deep model",
      "context": "Paper: Learning Task-aware Robust Deep Learning Systems\nAuthors: Keji Han, Yun Li, Xianzhong Long\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.697649",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2010.05125v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2010.05125v2"
        ]
      }
    },
    {
      "id": "4847e629",
      "concept": "aware method",
      "definition": "Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracy",
      "context": "Paper: Learning Task-aware Robust Deep Learning Systems\nAuthors: Keji Han, Yun Li, Xianzhong Long\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.697842",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2010.05125v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2010.05125v2"
        ]
      }
    },
    {
      "id": "5631daeb",
      "concept": "systems",
      "definition": "Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task",
      "context": "Paper: Learning Task-aware Robust Deep Learning Systems\nAuthors: Keji Han, Yun Li, Xianzhong Long\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.697974",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2010.05125v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2010.05125v2"
        ]
      }
    },
    {
      "id": "41243da0",
      "concept": "on robustness",
      "definition": "Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task",
      "context": "Paper: Learning Task-aware Robust Deep Learning Systems\nAuthors: Keji Han, Yun Li, Xianzhong Long\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.698097",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2010.05125v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2010.05125v2"
        ]
      }
    },
    {
      "id": "1acac75d",
      "concept": "than",
      "definition": "Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracy",
      "context": "Paper: Learning Task-aware Robust Deep Learning Systems\nAuthors: Keji Han, Yun Li, Xianzhong Long\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.698222",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2010.05125v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2010.05125v2"
        ]
      }
    },
    {
      "id": "15b77e70",
      "concept": "many works",
      "definition": "Many works demonstrate that deep learning system is vulnerable to adversarial attack",
      "context": "Paper: Learning Task-aware Robust Deep Learning Systems\nAuthors: Keji Han, Yun Li, Xianzhong Long\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.698335",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2010.05125v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2010.05125v2"
        ]
      }
    },
    {
      "id": "67788c15",
      "concept": "task-aware",
      "definition": "Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracy",
      "context": "Paper: Learning Task-aware Robust Deep Learning Systems\nAuthors: Keji Han, Yun Li, Xianzhong Long\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.701604",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2010.05125v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2010.05125v2"
        ]
      }
    },
    {
      "id": "310ad79a",
      "concept": "and deep",
      "definition": "Our method can be viewed as improving the robustness of deep learning systems from both the learning task and deep model",
      "context": "Paper: Learning Task-aware Robust Deep Learning Systems\nAuthors: Keji Han, Yun Li, Xianzhong Long\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.701821",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2010.05125v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2010.05125v2"
        ]
      }
    },
    {
      "id": "5e2697cd",
      "concept": "of the",
      "definition": "Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task",
      "context": "Paper: Learning Task-aware Robust Deep Learning Systems\nAuthors: Keji Han, Yun Li, Xianzhong Long\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.701953",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2010.05125v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2010.05125v2"
        ]
      }
    },
    {
      "id": "4ef08a9a",
      "concept": "more",
      "definition": "Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracy",
      "context": "Paper: Learning Task-aware Robust Deep Learning Systems\nAuthors: Keji Han, Yun Li, Xianzhong Long\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.702082",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2010.05125v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2010.05125v2"
        ]
      }
    },
    {
      "id": "8eb081c9",
      "concept": "se problems",
      "definition": "How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks",
      "context": "Paper: Deep Learning in Software Engineering\nAuthors: Xiaochen Li, He Jiang, Zhilei Ren\nCategories: cs.SE\nAbstract: Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in S...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.817517",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.04825v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1805.04825v1"
        ]
      }
    },
    {
      "id": "785033ab",
      "concept": "research",
      "definition": "How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks",
      "context": "Paper: Deep Learning in Software Engineering\nAuthors: Xiaochen Li, He Jiang, Zhilei Ren\nCategories: cs.SE\nAbstract: Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in S...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.817646",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.04825v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1805.04825v1"
        ]
      }
    },
    {
      "id": "e3b62cb9",
      "concept": "improve the",
      "definition": "How to improve the effectiveness, efficiency, understandability, and testability of deep learning based solutions may attract more SE researchers in the future",
      "context": "Paper: Deep Learning in Software Engineering\nAuthors: Xiaochen Li, He Jiang, Zhilei Ren\nCategories: cs.SE\nAbstract: Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in S...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.817702",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.04825v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1805.04825v1"
        ]
      }
    },
    {
      "id": "f313f65c",
      "concept": "variants",
      "definition": "7% papers only use standard deep learning models and their variants to solve SE problems",
      "context": "Paper: Deep Learning in Software Engineering\nAuthors: Xiaochen Li, He Jiang, Zhilei Ren\nCategories: cs.SE\nAbstract: Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in S...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.817747",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.04825v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1805.04825v1"
        ]
      }
    },
    {
      "id": "e032ce52",
      "concept": "still",
      "definition": "However, many open issues still remain to be investigated",
      "context": "Paper: Deep Learning in Software Engineering\nAuthors: Xiaochen Li, He Jiang, Zhilei Ren\nCategories: cs.SE\nAbstract: Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in S...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.817793",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.04825v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1805.04825v1"
        ]
      }
    },
    {
      "id": "8cd56a71",
      "concept": "recent",
      "definition": "Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE)",
      "context": "Paper: Deep Learning in Software Engineering\nAuthors: Xiaochen Li, He Jiang, Zhilei Ren\nCategories: cs.SE\nAbstract: Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in S...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.817836",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.04825v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1805.04825v1"
        ]
      }
    },
    {
      "id": "75339728",
      "concept": "more",
      "definition": "How to improve the effectiveness, efficiency, understandability, and testability of deep learning based solutions may attract more SE researchers in the future",
      "context": "Paper: Deep Learning in Software Engineering\nAuthors: Xiaochen Li, He Jiang, Zhilei Ren\nCategories: cs.SE\nAbstract: Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in S...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.817895",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.04825v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1805.04825v1"
        ]
      }
    },
    {
      "id": "95eca6fe",
      "concept": "improve",
      "definition": "How to improve the effectiveness, efficiency, understandability, and testability of deep learning based solutions may attract more SE researchers in the future",
      "context": "Paper: Deep Learning in Software Engineering\nAuthors: Xiaochen Li, He Jiang, Zhilei Ren\nCategories: cs.SE\nAbstract: Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in S...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.817947",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.04825v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1805.04825v1"
        ]
      }
    },
    {
      "id": "4f89fbac",
      "concept": "deep",
      "definition": "Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE)",
      "context": "Paper: Deep Learning in Software Engineering\nAuthors: Xiaochen Li, He Jiang, Zhilei Ren\nCategories: cs.SE\nAbstract: Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in S...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.817989",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.04825v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1805.04825v1"
        ]
      }
    },
    {
      "id": "71c016a8",
      "concept": "tasks",
      "definition": "How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks",
      "context": "Paper: Deep Learning in Software Engineering\nAuthors: Xiaochen Li, He Jiang, Zhilei Ren\nCategories: cs.SE\nAbstract: Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in S...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.818036",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.04825v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1805.04825v1"
        ]
      }
    },
    {
      "id": "9aa87b8a",
      "concept": "based deep",
      "definition": "Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers",
      "context": "Paper: Moving Deep Learning into Web Browser: How Far Can We Go?\nAuthors: Yun Ma, Dongwei Xiang, Shuyu Zheng\nCategories: cs.SE\nAbstract: Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been support...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.935440",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.09388v2",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1901.09388v2"
        ]
      }
    },
    {
      "id": "876c5d32",
      "concept": "application",
      "definition": "Our findings could help application developers, deep-learning framework vendors and browser vendors to improve the efficiency of deep learning in browsers",
      "context": "Paper: Moving Deep Learning into Web Browser: How Far Can We Go?\nAuthors: Yun Ma, Dongwei Xiang, Shuyu Zheng\nCategories: cs.SE\nAbstract: Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been support...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.944754",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.09388v2",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1901.09388v2"
        ]
      }
    },
    {
      "id": "7d2b5a5a",
      "concept": "improve",
      "definition": "Our findings could help application developers, deep-learning framework vendors and browser vendors to improve the efficiency of deep learning in browsers",
      "context": "Paper: Moving Deep Learning into Web Browser: How Far Can We Go?\nAuthors: Yun Ma, Dongwei Xiang, Shuyu Zheng\nCategories: cs.SE\nAbstract: Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been support...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.944907",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.09388v2",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1901.09388v2"
        ]
      }
    },
    {
      "id": "f29693ed",
      "concept": "could",
      "definition": "Our findings could help application developers, deep-learning framework vendors and browser vendors to improve the efficiency of deep learning in browsers",
      "context": "Paper: Moving Deep Learning into Web Browser: How Far Can We Go?\nAuthors: Yun Ma, Dongwei Xiang, Shuyu Zheng\nCategories: cs.SE\nAbstract: Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been support...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.944972",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.09388v2",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1901.09388v2"
        ]
      }
    },
    {
      "id": "3e8cc974",
      "concept": "deep",
      "definition": "Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers",
      "context": "Paper: Moving Deep Learning into Web Browser: How Far Can We Go?\nAuthors: Yun Ma, Dongwei Xiang, Shuyu Zheng\nCategories: cs.SE\nAbstract: Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been support...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.945025",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.09388v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1901.09388v2"
        ]
      }
    },
    {
      "id": "eb516220",
      "concept": "can we",
      "definition": "Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers",
      "context": "Paper: Moving Deep Learning into Web Browser: How Far Can We Go?\nAuthors: Yun Ma, Dongwei Xiang, Shuyu Zheng\nCategories: cs.SE\nAbstract: Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been support...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.945078",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.09388v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1901.09388v2"
        ]
      }
    },
    {
      "id": "aa6a2b2c",
      "concept": "tasks",
      "definition": "Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers",
      "context": "Paper: Moving Deep Learning into Web Browser: How Far Can We Go?\nAuthors: Yun Ma, Dongwei Xiang, Shuyu Zheng\nCategories: cs.SE\nAbstract: Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been support...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.945129",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.09388v2",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1901.09388v2"
        ]
      }
    },
    {
      "id": "06326daf",
      "concept": "knowledge",
      "definition": "To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers",
      "context": "Paper: Moving Deep Learning into Web Browser: How Far Can We Go?\nAuthors: Yun Ma, Dongwei Xiang, Shuyu Zheng\nCategories: cs.SE\nAbstract: Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been support...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.945185",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.09388v2",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1901.09388v2"
        ]
      }
    },
    {
      "id": "488cd5be",
      "concept": "first",
      "definition": "To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers",
      "context": "Paper: Moving Deep Learning into Web Browser: How Far Can We Go?\nAuthors: Yun Ma, Dongwei Xiang, Shuyu Zheng\nCategories: cs.SE\nAbstract: Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been support...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.945236",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.09388v2",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1901.09388v2"
        ]
      }
    },
    {
      "id": "574ed44a",
      "concept": "performance of",
      "definition": "Then we measure the performance of different frameworks when running different deep learning tasks",
      "context": "Paper: Moving Deep Learning into Web Browser: How Far Can We Go?\nAuthors: Yun Ma, Dongwei Xiang, Shuyu Zheng\nCategories: cs.SE\nAbstract: Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been support...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:36.945285",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.09388v2",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1901.09388v2"
        ]
      }
    },
    {
      "id": "dbabf7c6",
      "concept": "and challenges",
      "definition": "Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks",
      "context": "Paper: Quantum Neural Networks: Concepts, Applications, and Challenges\nAuthors: Yunseok Kwak, Won Joon Yun, Soyi Jung\nCategories: quant-ph, cs.LG\nAbstract: Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.046567",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.01468v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2108.01468v1"
        ]
      }
    },
    {
      "id": "26fe2f1a",
      "concept": "quantum circuits",
      "definition": "The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted",
      "context": "Paper: Quantum Neural Networks: Concepts, Applications, and Challenges\nAuthors: Yunseok Kwak, Won Joon Yun, Soyi Jung\nCategories: quant-ph, cs.LG\nAbstract: Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.046695",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.01468v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.01468v1"
        ]
      }
    },
    {
      "id": "45c966d8",
      "concept": "research",
      "definition": "Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks",
      "context": "Paper: Quantum Neural Networks: Concepts, Applications, and Challenges\nAuthors: Yunseok Kwak, Won Joon Yun, Soyi Jung\nCategories: quant-ph, cs.LG\nAbstract: Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.046749",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.01468v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.01468v1"
        ]
      }
    },
    {
      "id": "b7364051",
      "concept": "explains",
      "definition": "This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements",
      "context": "Paper: Quantum Neural Networks: Concepts, Applications, and Challenges\nAuthors: Yunseok Kwak, Won Joon Yun, Soyi Jung\nCategories: quant-ph, cs.LG\nAbstract: Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.046799",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.01468v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.01468v1"
        ]
      }
    },
    {
      "id": "cce1e6a2",
      "concept": "quantum computing",
      "definition": "Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks",
      "context": "Paper: Quantum Neural Networks: Concepts, Applications, and Challenges\nAuthors: Yunseok Kwak, Won Joon Yun, Soyi Jung\nCategories: quant-ph, cs.LG\nAbstract: Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.046842",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.01468v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.01468v1"
        ]
      }
    },
    {
      "id": "15f0a598",
      "concept": "introduces major",
      "definition": "This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements",
      "context": "Paper: Quantum Neural Networks: Concepts, Applications, and Challenges\nAuthors: Yunseok Kwak, Won Joon Yun, Soyi Jung\nCategories: quant-ph, cs.LG\nAbstract: Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.046884",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.01468v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.01468v1"
        ]
      }
    },
    {
      "id": "9028d18b",
      "concept": "for long",
      "definition": "The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted",
      "context": "Paper: Quantum Neural Networks: Concepts, Applications, and Challenges\nAuthors: Yunseok Kwak, Won Joon Yun, Soyi Jung\nCategories: quant-ph, cs.LG\nAbstract: Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.046925",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.01468v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.01468v1"
        ]
      }
    },
    {
      "id": "314b6ac4",
      "concept": "long",
      "definition": "The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted",
      "context": "Paper: Quantum Neural Networks: Concepts, Applications, and Challenges\nAuthors: Yunseok Kwak, Won Joon Yun, Soyi Jung\nCategories: quant-ph, cs.LG\nAbstract: Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.046964",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.01468v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.01468v1"
        ]
      }
    },
    {
      "id": "10fe73b0",
      "concept": "application",
      "definition": "Lastly, this paper presents various future research directions and application fields of quantum deep learning",
      "context": "Paper: Quantum Neural Networks: Concepts, Applications, and Challenges\nAuthors: Yunseok Kwak, Won Joon Yun, Soyi Jung\nCategories: quant-ph, cs.LG\nAbstract: Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.047004",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.01468v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2108.01468v1"
        ]
      }
    },
    {
      "id": "7dc22252",
      "concept": "the research",
      "definition": "The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted",
      "context": "Paper: Quantum Neural Networks: Concepts, Applications, and Challenges\nAuthors: Yunseok Kwak, Won Joon Yun, Soyi Jung\nCategories: quant-ph, cs.LG\nAbstract: Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.047041",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.01468v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.01468v1"
        ]
      }
    },
    {
      "id": "7d9c3e79",
      "concept": "state-of",
      "definition": "Extensive experiments show that NetBooster consistently outperforms state-of-the-art tiny deep learning solutions",
      "context": "Paper: NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders   of Deep Giants\nAuthors: Zhongzhi Yu, Yonggan Fu, Jiayi Yuan\nCategories: cs.LG, cs.DC\nAbstract: Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.158089",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2306.13586v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2306.13586v1"
        ]
      }
    },
    {
      "id": "20537f67",
      "concept": "deploying",
      "definition": "Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices",
      "context": "Paper: NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders   of Deep Giants\nAuthors: Zhongzhi Yu, Yonggan Fu, Jiayi Yuan\nCategories: cs.LG, cs.DC\nAbstract: Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.158277",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2306.13586v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2306.13586v1"
        ]
      }
    },
    {
      "id": "966a8901",
      "concept": "still",
      "definition": "However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs)",
      "context": "Paper: NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders   of Deep Giants\nAuthors: Zhongzhi Yu, Yonggan Fu, Jiayi Yuan\nCategories: cs.LG, cs.DC\nAbstract: Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.164555",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2306.13586v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2306.13586v1"
        ]
      }
    },
    {
      "id": "1b742532",
      "concept": "empower",
      "definition": "To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the architectures of TNNs via an expansion-then-contraction strategy",
      "context": "Paper: NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders   of Deep Giants\nAuthors: Zhongzhi Yu, Yonggan Fu, Jiayi Yuan\nCategories: cs.LG, cs.DC\nAbstract: Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.164763",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2306.13586v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2306.13586v1"
        ]
      }
    },
    {
      "id": "84d3f354",
      "concept": "numerous",
      "definition": "Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices",
      "context": "Paper: NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders   of Deep Giants\nAuthors: Zhongzhi Yu, Yonggan Fu, Jiayi Yuan\nCategories: cs.LG, cs.DC\nAbstract: Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.164919",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2306.13586v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2306.13586v1"
        ]
      }
    },
    {
      "id": "6ceb1c5f",
      "concept": "attracted increasing",
      "definition": "Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices",
      "context": "Paper: NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders   of Deep Giants\nAuthors: Zhongzhi Yu, Yonggan Fu, Jiayi Yuan\nCategories: cs.LG, cs.DC\nAbstract: Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.165052",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2306.13586v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2306.13586v1"
        ]
      }
    },
    {
      "id": "3994476e",
      "concept": "via an",
      "definition": "To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the architectures of TNNs via an expansion-then-contraction strategy",
      "context": "Paper: NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders   of Deep Giants\nAuthors: Zhongzhi Yu, Yonggan Fu, Jiayi Yuan\nCategories: cs.LG, cs.DC\nAbstract: Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.165184",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2306.13586v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2306.13586v1"
        ]
      }
    },
    {
      "id": "de62a5d4",
      "concept": "demand",
      "definition": "Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices",
      "context": "Paper: NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders   of Deep Giants\nAuthors: Zhongzhi Yu, Yonggan Fu, Jiayi Yuan\nCategories: cs.LG, cs.DC\nAbstract: Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.165314",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2306.13586v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2306.13586v1"
        ]
      }
    },
    {
      "id": "fcc0ebdd",
      "concept": "shoulders",
      "definition": "Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices",
      "context": "Paper: NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders   of Deep Giants\nAuthors: Zhongzhi Yu, Yonggan Fu, Jiayi Yuan\nCategories: cs.LG, cs.DC\nAbstract: Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.165478",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2306.13586v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2306.13586v1"
        ]
      }
    },
    {
      "id": "9e3033c5",
      "concept": "deep",
      "definition": "Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices",
      "context": "Paper: NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders   of Deep Giants\nAuthors: Zhongzhi Yu, Yonggan Fu, Jiayi Yuan\nCategories: cs.LG, cs.DC\nAbstract: Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.165614",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2306.13586v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2306.13586v1"
        ]
      }
    },
    {
      "id": "668ab307",
      "concept": "and state",
      "definition": "We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD",
      "context": "Paper: Greedy Deep Dictionary Learning\nAuthors: Snigdha Tariyal, Angshul Majumdar, Richa Singh\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tool...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.275285",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1602.00203v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1602.00203v1"
        ]
      }
    },
    {
      "id": "7f7d9b3d",
      "concept": "than",
      "definition": "Our method yields better results than all",
      "context": "Paper: Greedy Deep Dictionary Learning\nAuthors: Snigdha Tariyal, Angshul Majumdar, Richa Singh\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tool...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.275510",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1602.00203v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1602.00203v1"
        ]
      }
    },
    {
      "id": "31a09252",
      "concept": "some benchmark",
      "definition": "We apply the proposed technique on some benchmark deep learning datasets",
      "context": "Paper: Greedy Deep Dictionary Learning\nAuthors: Snigdha Tariyal, Angshul Majumdar, Richa Singh\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tool...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.275663",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1602.00203v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1602.00203v1"
        ]
      }
    },
    {
      "id": "ed162c94",
      "concept": "shallow",
      "definition": "This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known",
      "context": "Paper: Greedy Deep Dictionary Learning\nAuthors: Snigdha Tariyal, Angshul Majumdar, Richa Singh\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tool...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.275806",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1602.00203v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1602.00203v1"
        ]
      }
    },
    {
      "id": "0cbb3e27",
      "concept": "and deep",
      "definition": "We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD",
      "context": "Paper: Greedy Deep Dictionary Learning\nAuthors: Snigdha Tariyal, Angshul Majumdar, Richa Singh\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tool...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.275951",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1602.00203v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1602.00203v1"
        ]
      }
    },
    {
      "id": "94b1153e",
      "concept": "of the",
      "definition": "We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD",
      "context": "Paper: Greedy Deep Dictionary Learning\nAuthors: Snigdha Tariyal, Angshul Majumdar, Richa Singh\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tool...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.276092",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1602.00203v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1602.00203v1"
        ]
      }
    },
    {
      "id": "edd58062",
      "concept": "technique on",
      "definition": "We apply the proposed technique on some benchmark deep learning datasets",
      "context": "Paper: Greedy Deep Dictionary Learning\nAuthors: Snigdha Tariyal, Angshul Majumdar, Richa Singh\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tool...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.276231",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1602.00203v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1602.00203v1"
        ]
      }
    },
    {
      "id": "6458097a",
      "concept": "some",
      "definition": "We apply the proposed technique on some benchmark deep learning datasets",
      "context": "Paper: Greedy Deep Dictionary Learning\nAuthors: Snigdha Tariyal, Angshul Majumdar, Richa Singh\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tool...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.282466",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1602.00203v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1602.00203v1"
        ]
      }
    },
    {
      "id": "a7985dfe",
      "concept": "art supervised",
      "definition": "We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD",
      "context": "Paper: Greedy Deep Dictionary Learning\nAuthors: Snigdha Tariyal, Angshul Majumdar, Richa Singh\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tool...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.282761",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1602.00203v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1602.00203v1"
        ]
      }
    },
    {
      "id": "cf772aed",
      "concept": "deep",
      "definition": "In this work we propose a new deep learning tool called deep dictionary learning",
      "context": "Paper: Greedy Deep Dictionary Learning\nAuthors: Snigdha Tariyal, Angshul Majumdar, Richa Singh\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tool...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.282862",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1602.00203v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1602.00203v1"
        ]
      }
    },
    {
      "id": "a504c19d",
      "concept": "start",
      "definition": "We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.400983",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "064c8628",
      "concept": "research",
      "definition": "In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.401191",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "496e0fa6",
      "concept": "and state",
      "definition": "In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.401343",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "d48632d5",
      "concept": "segmentation",
      "definition": "(i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.401530",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "cde139e2",
      "concept": "available datasets",
      "definition": "Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.401691",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "27aaa621",
      "concept": "research directions",
      "definition": "Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.401859",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "c06fc9ae",
      "concept": "state-of",
      "definition": "In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.402008",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "f3133057",
      "concept": "image",
      "definition": "(i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.402180",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "b9a334f9",
      "concept": "recent",
      "definition": "Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.402344",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "3b377d4b",
      "concept": "and deep",
      "definition": "We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.410675",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "e8ed9565",
      "concept": "uses",
      "definition": "The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neural network components which capture complex non-linear stochastic relationships between the random variables",
      "context": "Paper: Probabilistic Deep Learning with Probabilistic Neural Networks and Deep   Probabilistic Models\nAuthors: Daniel T. Chang\nCategories: cs.LG, stat.ML\nAbstract: Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.515620",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2106.00120v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2106.00120v3"
        ]
      }
    },
    {
      "id": "a5a799f1",
      "concept": "the former",
      "definition": "The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neural network components which capture complex non-linear stochastic relationships between the random variables",
      "context": "Paper: Probabilistic Deep Learning with Probabilistic Neural Networks and Deep   Probabilistic Models\nAuthors: Daniel T. Chang\nCategories: cs.LG, stat.ML\nAbstract: Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.515768",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2106.00120v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2106.00120v3"
        ]
      }
    },
    {
      "id": "ddc2c9d3",
      "concept": "two approaches",
      "definition": "We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models",
      "context": "Paper: Probabilistic Deep Learning with Probabilistic Neural Networks and Deep   Probabilistic Models\nAuthors: Daniel T. Chang\nCategories: cs.LG, stat.ML\nAbstract: Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.515829",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2106.00120v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2106.00120v3"
        ]
      }
    },
    {
      "id": "374b0a3c",
      "concept": "components",
      "definition": "The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neural network components which capture complex non-linear stochastic relationships between the random variables",
      "context": "Paper: Probabilistic Deep Learning with Probabilistic Neural Networks and Deep   Probabilistic Models\nAuthors: Daniel T. Chang\nCategories: cs.LG, stat.ML\nAbstract: Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.515886",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2106.00120v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2106.00120v3"
        ]
      }
    },
    {
      "id": "e5cfebbe",
      "concept": "for both",
      "definition": "TensorFlow Probability is a library for probabilistic modeling and inference which can be used for both approaches of probabilistic deep learning",
      "context": "Paper: Probabilistic Deep Learning with Probabilistic Neural Networks and Deep   Probabilistic Models\nAuthors: Daniel T. Chang\nCategories: cs.LG, stat.ML\nAbstract: Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.515942",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2106.00120v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2106.00120v3"
        ]
      }
    },
    {
      "id": "8eab0a49",
      "concept": "uses probabilistic",
      "definition": "The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neural network components which capture complex non-linear stochastic relationships between the random variables",
      "context": "Paper: Probabilistic Deep Learning with Probabilistic Neural Networks and Deep   Probabilistic Models\nAuthors: Daniel T. Chang\nCategories: cs.LG, stat.ML\nAbstract: Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.515994",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2106.00120v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2106.00120v3"
        ]
      }
    },
    {
      "id": "297fab3d",
      "concept": "some",
      "definition": "We discuss some major examples of each approach including Bayesian neural networks and mixture density networks (for probabilistic neural networks), and variational autoencoders, deep Gaussian processes and deep mixed effects models (for deep probabilistic models)",
      "context": "Paper: Probabilistic Deep Learning with Probabilistic Neural Networks and Deep   Probabilistic Models\nAuthors: Daniel T. Chang\nCategories: cs.LG, stat.ML\nAbstract: Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.516044",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2106.00120v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2106.00120v3"
        ]
      }
    },
    {
      "id": "97a242d1",
      "concept": "deep",
      "definition": "Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty",
      "context": "Paper: Probabilistic Deep Learning with Probabilistic Neural Networks and Deep   Probabilistic Models\nAuthors: Daniel T. Chang\nCategories: cs.LG, stat.ML\nAbstract: Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.516092",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2106.00120v3",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2106.00120v3"
        ]
      }
    },
    {
      "id": "bdb64a5d",
      "concept": "we include",
      "definition": "We include its code examples for illustration",
      "context": "Paper: Probabilistic Deep Learning with Probabilistic Neural Networks and Deep   Probabilistic Models\nAuthors: Daniel T. Chang\nCategories: cs.LG, stat.ML\nAbstract: Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.516147",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2106.00120v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2106.00120v3"
        ]
      }
    },
    {
      "id": "1a9f17e3",
      "concept": "network",
      "definition": "It is based on the use of probabilistic models and deep neural networks",
      "context": "Paper: Probabilistic Deep Learning with Probabilistic Neural Networks and Deep   Probabilistic Models\nAuthors: Daniel T. Chang\nCategories: cs.LG, stat.ML\nAbstract: Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.516231",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2106.00120v3",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2106.00120v3"
        ]
      }
    },
    {
      "id": "f40c4a59",
      "concept": "deployment",
      "definition": "This paper aims to gather information about these advances from the literature and show how and at which points along the lifecycle of Deep Learning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation) it is possible to reduce energy consumption",
      "context": "Paper: Towards energy-efficient Deep Learning: An overview of energy-efficient   approaches along the Deep Learning Lifecycle\nAuthors: Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon\nCategories: cs.LG\nAbstract: Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which poin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.624014",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.01980v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.01980v1"
        ]
      }
    },
    {
      "id": "fae60fd7",
      "concept": "associated environmental",
      "definition": "However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs",
      "context": "Paper: Towards energy-efficient Deep Learning: An overview of energy-efficient   approaches along the Deep Learning Lifecycle\nAuthors: Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon\nCategories: cs.LG\nAbstract: Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which poin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.624183",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.01980v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.01980v1"
        ]
      }
    },
    {
      "id": "481a1889",
      "concept": "learning applications",
      "definition": "Deep Learning has enabled many advances in machine learning applications in the last few years",
      "context": "Paper: Towards energy-efficient Deep Learning: An overview of energy-efficient   approaches along the Deep Learning Lifecycle\nAuthors: Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon\nCategories: cs.LG\nAbstract: Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which poin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.624309",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.01980v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.01980v1"
        ]
      }
    },
    {
      "id": "6df7397d",
      "concept": "researchers and",
      "definition": "Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years",
      "context": "Paper: Towards energy-efficient Deep Learning: An overview of energy-efficient   approaches along the Deep Learning Lifecycle\nAuthors: Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon\nCategories: cs.LG\nAbstract: Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which poin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.629727",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.01980v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.01980v1"
        ]
      }
    },
    {
      "id": "2f26811f",
      "concept": "machine",
      "definition": "Deep Learning has enabled many advances in machine learning applications in the last few years",
      "context": "Paper: Towards energy-efficient Deep Learning: An overview of energy-efficient   approaches along the Deep Learning Lifecycle\nAuthors: Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon\nCategories: cs.LG\nAbstract: Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which poin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.629936",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.01980v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.01980v1"
        ]
      }
    },
    {
      "id": "91b1bf4c",
      "concept": "about these",
      "definition": "This paper aims to gather information about these advances from the literature and show how and at which points along the lifecycle of Deep Learning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation) it is possible to reduce energy consumption",
      "context": "Paper: Towards energy-efficient Deep Learning: An overview of energy-efficient   approaches along the Deep Learning Lifecycle\nAuthors: Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon\nCategories: cs.LG\nAbstract: Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which poin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.630106",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.01980v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.01980v1"
        ]
      }
    },
    {
      "id": "c9df71c2",
      "concept": "energy-efficient",
      "definition": "Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years",
      "context": "Paper: Towards energy-efficient Deep Learning: An overview of energy-efficient   approaches along the Deep Learning Lifecycle\nAuthors: Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon\nCategories: cs.LG\nAbstract: Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which poin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.630249",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.01980v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2303.01980v1"
        ]
      }
    },
    {
      "id": "998aa459",
      "concept": "overview",
      "definition": "Deep Learning has enabled many advances in machine learning applications in the last few years",
      "context": "Paper: Towards energy-efficient Deep Learning: An overview of energy-efficient   approaches along the Deep Learning Lifecycle\nAuthors: Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon\nCategories: cs.LG\nAbstract: Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which poin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.630414",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.01980v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2303.01980v1"
        ]
      }
    },
    {
      "id": "17d89d0e",
      "concept": "this paper",
      "definition": "This paper aims to gather information about these advances from the literature and show how and at which points along the lifecycle of Deep Learning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation) it is possible to reduce energy consumption",
      "context": "Paper: Towards energy-efficient Deep Learning: An overview of energy-efficient   approaches along the Deep Learning Lifecycle\nAuthors: Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon\nCategories: cs.LG\nAbstract: Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which poin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.630554",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.01980v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.01980v1"
        ]
      }
    },
    {
      "id": "81acaa5e",
      "concept": "deep",
      "definition": "Deep Learning has enabled many advances in machine learning applications in the last few years",
      "context": "Paper: Towards energy-efficient Deep Learning: An overview of energy-efficient   approaches along the Deep Learning Lifecycle\nAuthors: Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon\nCategories: cs.LG\nAbstract: Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which poin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.630688",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.01980v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2303.01980v1"
        ]
      }
    },
    {
      "id": "5ad3b99d",
      "concept": "great significance",
      "definition": "This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learning",
      "context": "Paper: A Unified Framework of Deep Neural Networks by Capsules\nAuthors: Yujian Li, Chuanhui Shan\nCategories: cs.LG, stat.ML\nAbstract: With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis o...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.734988",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.03551v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1805.03551v2"
        ]
      }
    },
    {
      "id": "a2147f50",
      "concept": "unified framework",
      "definition": "Then, we set up a unified framework for deep learning with capsule networks",
      "context": "Paper: A Unified Framework of Deep Neural Networks by Capsules\nAuthors: Yujian Li, Chuanhui Shan\nCategories: cs.LG, stat.ML\nAbstract: With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis o...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.735194",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.03551v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1805.03551v2"
        ]
      }
    },
    {
      "id": "b4d6371c",
      "concept": "capsule",
      "definition": "Then, we set up a unified framework for deep learning with capsule networks",
      "context": "Paper: A Unified Framework of Deep Neural Networks by Capsules\nAuthors: Yujian Li, Chuanhui Shan\nCategories: cs.LG, stat.ML\nAbstract: With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis o...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.735339",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.03551v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1805.03551v2"
        ]
      }
    },
    {
      "id": "ba081fae",
      "concept": "provide",
      "definition": "This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learning",
      "context": "Paper: A Unified Framework of Deep Neural Networks by Capsules\nAuthors: Yujian Li, Chuanhui Shan\nCategories: cs.LG, stat.ML\nAbstract: With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis o...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.748599",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.03551v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1805.03551v2"
        ]
      }
    },
    {
      "id": "9ecfc69a",
      "concept": "could",
      "definition": "This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learning",
      "context": "Paper: A Unified Framework of Deep Neural Networks by Capsules\nAuthors: Yujian Li, Chuanhui Shan\nCategories: cs.LG, stat.ML\nAbstract: With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis o...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.748776",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.03551v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1805.03551v2"
        ]
      }
    },
    {
      "id": "1d26fa0f",
      "concept": "theoretical basis",
      "definition": "This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learning",
      "context": "Paper: A Unified Framework of Deep Neural Networks by Capsules\nAuthors: Yujian Li, Chuanhui Shan\nCategories: cs.LG, stat.ML\nAbstract: With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis o...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.748926",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.03551v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1805.03551v2"
        ]
      }
    },
    {
      "id": "4be7fe14",
      "concept": "deep",
      "definition": "With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue",
      "context": "Paper: A Unified Framework of Deep Neural Networks by Capsules\nAuthors: Yujian Li, Chuanhui Shan\nCategories: cs.LG, stat.ML\nAbstract: With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis o...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.749058",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.03551v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1805.03551v2"
        ]
      }
    },
    {
      "id": "f7db4692",
      "concept": "first",
      "definition": "We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs",
      "context": "Paper: A Unified Framework of Deep Neural Networks by Capsules\nAuthors: Yujian Li, Chuanhui Shan\nCategories: cs.LG, stat.ML\nAbstract: With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis o...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.749796",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.03551v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1805.03551v2"
        ]
      }
    },
    {
      "id": "de787fb5",
      "concept": "of connected",
      "definition": "We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs",
      "context": "Paper: A Unified Framework of Deep Neural Networks by Capsules\nAuthors: Yujian Li, Chuanhui Shan\nCategories: cs.LG, stat.ML\nAbstract: With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis o...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.749994",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.03551v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1805.03551v2"
        ]
      }
    },
    {
      "id": "d957db6c",
      "concept": "graphic",
      "definition": "This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learning",
      "context": "Paper: A Unified Framework of Deep Neural Networks by Capsules\nAuthors: Yujian Li, Chuanhui Shan\nCategories: cs.LG, stat.ML\nAbstract: With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis o...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.750132",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1805.03551v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1805.03551v2"
        ]
      }
    },
    {
      "id": "a6f03cf4",
      "concept": "research",
      "definition": "The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents",
      "context": "Paper: Integrating Learning and Reasoning with Deep Logic Models\nAuthors: Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti\nCategories: cs.LG, stat.ML\nAbstract: Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.867889",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.04195v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1901.04195v1"
        ]
      }
    },
    {
      "id": "56e7a304",
      "concept": "especially",
      "definition": "Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns",
      "context": "Paper: Integrating Learning and Reasoning with Deep Logic Models\nAuthors: Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti\nCategories: cs.LG, stat.ML\nAbstract: Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.868076",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.04195v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1901.04195v1"
        ]
      }
    },
    {
      "id": "405e1f43",
      "concept": "to take",
      "definition": "Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments",
      "context": "Paper: Integrating Learning and Reasoning with Deep Logic Models\nAuthors: Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti\nCategories: cs.LG, stat.ML\nAbstract: Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.868200",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.04195v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1901.04195v1"
        ]
      }
    },
    {
      "id": "ceb538a7",
      "concept": "patterns",
      "definition": "Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns",
      "context": "Paper: Integrating Learning and Reasoning with Deep Logic Models\nAuthors: Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti\nCategories: cs.LG, stat.ML\nAbstract: Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.868317",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.04195v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1901.04195v1"
        ]
      }
    },
    {
      "id": "d00c6c06",
      "concept": "hand",
      "definition": "Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments",
      "context": "Paper: Integrating Learning and Reasoning with Deep Logic Models\nAuthors: Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti\nCategories: cs.LG, stat.ML\nAbstract: Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.868469",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.04195v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1901.04195v1"
        ]
      }
    },
    {
      "id": "99278293",
      "concept": "logic",
      "definition": "Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments",
      "context": "Paper: Integrating Learning and Reasoning with Deep Logic Models\nAuthors: Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti\nCategories: cs.LG, stat.ML\nAbstract: Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.868600",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.04195v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1901.04195v1"
        ]
      }
    },
    {
      "id": "f454f646",
      "concept": "still",
      "definition": "The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents",
      "context": "Paper: Integrating Learning and Reasoning with Deep Logic Models\nAuthors: Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti\nCategories: cs.LG, stat.ML\nAbstract: Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.868784",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.04195v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1901.04195v1"
        ]
      }
    },
    {
      "id": "3cb610f1",
      "concept": "of the",
      "definition": "Deep Logic Models create an end-to-end differentiable architecture, where deep learners are embedded into a network implementing a continuous relaxation of the logic knowledge",
      "context": "Paper: Integrating Learning and Reasoning with Deep Logic Models\nAuthors: Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti\nCategories: cs.LG, stat.ML\nAbstract: Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.868946",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.04195v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1901.04195v1"
        ]
      }
    },
    {
      "id": "e58987a7",
      "concept": "differentiable",
      "definition": "Deep Logic Models create an end-to-end differentiable architecture, where deep learners are embedded into a network implementing a continuous relaxation of the logic knowledge",
      "context": "Paper: Integrating Learning and Reasoning with Deep Logic Models\nAuthors: Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti\nCategories: cs.LG, stat.ML\nAbstract: Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.869094",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.04195v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1901.04195v1"
        ]
      }
    },
    {
      "id": "d571d119",
      "concept": "and classification",
      "definition": "Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns",
      "context": "Paper: Integrating Learning and Reasoning with Deep Logic Models\nAuthors: Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti\nCategories: cs.LG, stat.ML\nAbstract: Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.869233",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1901.04195v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1901.04195v1"
        ]
      }
    },
    {
      "id": "9667f2ad",
      "concept": "recent past",
      "definition": "In the recent past, deep learning techniques have been frequently applied in biometric template protection systems for various purposes",
      "context": "Paper: Deep Learning in the Field of Biometric Template Protection: An Overview\nAuthors: Christian Rathgeb, Jascha Kolberg, Andreas Uhl\nCategories: cs.CV\nAbstract: Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fa...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.987992",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.02715v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.02715v1"
        ]
      }
    },
    {
      "id": "f1d2edba",
      "concept": "to achieve",
      "definition": "Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance",
      "context": "Paper: Deep Learning in the Field of Biometric Template Protection: An Overview\nAuthors: Christian Rathgeb, Jascha Kolberg, Andreas Uhl\nCategories: cs.CV\nAbstract: Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fa...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.988170",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.02715v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.02715v1"
        ]
      }
    },
    {
      "id": "11444271",
      "concept": "systems",
      "definition": "Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance",
      "context": "Paper: Deep Learning in the Field of Biometric Template Protection: An Overview\nAuthors: Christian Rathgeb, Jascha Kolberg, Andreas Uhl\nCategories: cs.CV\nAbstract: Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fa...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.988298",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.02715v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.02715v1"
        ]
      }
    },
    {
      "id": "fce0b53d",
      "concept": "deployment",
      "definition": "Technologies of biometric template protection are designed to enable a secure and privacy-preserving deployment of biometrics",
      "context": "Paper: Deep Learning in the Field of Biometric Template Protection: An Overview\nAuthors: Christian Rathgeb, Jascha Kolberg, Andreas Uhl\nCategories: cs.CV\nAbstract: Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fa...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.988470",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.02715v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.02715v1"
        ]
      }
    },
    {
      "id": "beb6c08f",
      "concept": "or template",
      "definition": "Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fairness, vulnerability to attacks, or template protection",
      "context": "Paper: Deep Learning in the Field of Biometric Template Protection: An Overview\nAuthors: Christian Rathgeb, Jascha Kolberg, Andreas Uhl\nCategories: cs.CV\nAbstract: Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fa...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.988615",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.02715v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.02715v1"
        ]
      }
    },
    {
      "id": "61e0f998",
      "concept": "recognition",
      "definition": "Deep learning has revolutionised the field of pattern recognition, including biometric recognition",
      "context": "Paper: Deep Learning in the Field of Biometric Template Protection: An Overview\nAuthors: Christian Rathgeb, Jascha Kolberg, Andreas Uhl\nCategories: cs.CV\nAbstract: Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fa...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.988743",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.02715v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.02715v1"
        ]
      }
    },
    {
      "id": "1d0af46d",
      "concept": "today",
      "definition": "Today, deep learning represents the most popular and successful form of machine learning",
      "context": "Paper: Deep Learning in the Field of Biometric Template Protection: An Overview\nAuthors: Christian Rathgeb, Jascha Kolberg, Andreas Uhl\nCategories: cs.CV\nAbstract: Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fa...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.988897",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.02715v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.02715v1"
        ]
      }
    },
    {
      "id": "4760a4ee",
      "concept": "machine",
      "definition": "Today, deep learning represents the most popular and successful form of machine learning",
      "context": "Paper: Deep Learning in the Field of Biometric Template Protection: An Overview\nAuthors: Christian Rathgeb, Jascha Kolberg, Andreas Uhl\nCategories: cs.CV\nAbstract: Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fa...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.989023",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.02715v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.02715v1"
        ]
      }
    },
    {
      "id": "d233a8f6",
      "concept": "overview",
      "definition": "This work provides an overview of how advances in deep learning take influence on the field of biometric template protection",
      "context": "Paper: Deep Learning in the Field of Biometric Template Protection: An Overview\nAuthors: Christian Rathgeb, Jascha Kolberg, Andreas Uhl\nCategories: cs.CV\nAbstract: Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fa...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.989180",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.02715v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2303.02715v1"
        ]
      }
    },
    {
      "id": "0896ac85",
      "concept": "recent",
      "definition": "In the recent past, deep learning techniques have been frequently applied in biometric template protection systems for various purposes",
      "context": "Paper: Deep Learning in the Field of Biometric Template Protection: An Overview\nAuthors: Christian Rathgeb, Jascha Kolberg, Andreas Uhl\nCategories: cs.CV\nAbstract: Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fa...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:37.989326",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.02715v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2303.02715v1"
        ]
      }
    },
    {
      "id": "39bb7efe",
      "concept": "solve problems",
      "definition": "Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.106254",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "904ae4dc",
      "concept": "furthermore",
      "definition": "Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policies",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.106482",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "92296ac4",
      "concept": "research",
      "definition": "Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.106966",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "9f3d2ac9",
      "concept": "within",
      "definition": "From exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth view",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.107497",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "ce707dee",
      "concept": "formalize and",
      "definition": "In this paper, we will formalize and analyze generalization in deep reinforcement learning",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.107990",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "ea0709a8",
      "concept": "analyze",
      "definition": "In this paper, we will formalize and analyze generalization in deep reinforcement learning",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.108159",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "516bd362",
      "concept": "questions the",
      "definition": "While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.110081",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "41fbbbc4",
      "concept": "still",
      "definition": "While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.112508",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "88e7ad4e",
      "concept": "analyzing",
      "definition": "Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.112849",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "20ab45d6",
      "concept": "ongoing",
      "definition": "While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.113039",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "37d66c68",
      "concept": "machine",
      "definition": "In order to address this very crucial question, here we see deep learning from perspective of mechanical learning and learning machine (see [1], [2])",
      "context": "Paper: What Really is Deep Learning Doing?\nAuthors: Chuyu Xiong\nCategories: cs.LG, cs.NE\nAbstract: Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.224240",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1711.03577v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1711.03577v1"
        ]
      }
    },
    {
      "id": "48fa3a06",
      "concept": "tried to",
      "definition": "For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation",
      "context": "Paper: What Really is Deep Learning Doing?\nAuthors: Chuyu Xiong\nCategories: cs.LG, cs.NE\nAbstract: Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.225309",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1711.03577v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1711.03577v1"
        ]
      }
    },
    {
      "id": "bf271d4a",
      "concept": "still",
      "definition": "Yet, what deep learning is really doing is still an open question",
      "context": "Paper: What Really is Deep Learning Doing?\nAuthors: Chuyu Xiong\nCategories: cs.LG, cs.NE\nAbstract: Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.227169",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1711.03577v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1711.03577v1"
        ]
      }
    },
    {
      "id": "857db328",
      "concept": "advantages and",
      "definition": "We also will discuss advantages and disadvantages of deep learning at the end of this work",
      "context": "Paper: What Really is Deep Learning Doing?\nAuthors: Chuyu Xiong\nCategories: cs.LG, cs.NE\nAbstract: Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.227455",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1711.03577v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1711.03577v1"
        ]
      }
    },
    {
      "id": "e5e81e7e",
      "concept": "more",
      "definition": "Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more",
      "context": "Paper: What Really is Deep Learning Doing?\nAuthors: Chuyu Xiong\nCategories: cs.LG, cs.NE\nAbstract: Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.227588",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1711.03577v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1711.03577v1"
        ]
      }
    },
    {
      "id": "fd284f3b",
      "concept": "the view",
      "definition": "For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation",
      "context": "Paper: What Really is Deep Learning Doing?\nAuthors: Chuyu Xiong\nCategories: cs.LG, cs.NE\nAbstract: Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.227725",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1711.03577v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1711.03577v1"
        ]
      }
    },
    {
      "id": "4dc0aa75",
      "concept": "language",
      "definition": "Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more",
      "context": "Paper: What Really is Deep Learning Doing?\nAuthors: Chuyu Xiong\nCategories: cs.LG, cs.NE\nAbstract: Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.227852",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1711.03577v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1711.03577v1"
        ]
      }
    },
    {
      "id": "2a56f557",
      "concept": "deep",
      "definition": "Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more",
      "context": "Paper: What Really is Deep Learning Doing?\nAuthors: Chuyu Xiong\nCategories: cs.LG, cs.NE\nAbstract: Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.227970",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1711.03577v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1711.03577v1"
        ]
      }
    },
    {
      "id": "94314bde",
      "concept": "works in",
      "definition": "There are a lot of works in this direction",
      "context": "Paper: What Really is Deep Learning Doing?\nAuthors: Chuyu Xiong\nCategories: cs.LG, cs.NE\nAbstract: Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.228094",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1711.03577v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1711.03577v1"
        ]
      }
    },
    {
      "id": "201f0f76",
      "concept": "at the",
      "definition": "We also will discuss advantages and disadvantages of deep learning at the end of this work",
      "context": "Paper: What Really is Deep Learning Doing?\nAuthors: Chuyu Xiong\nCategories: cs.LG, cs.NE\nAbstract: Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:38.228219",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1711.03577v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1711.03577v1"
        ]
      }
    },
    {
      "id": "ecdf6f24",
      "concept": "seen as",
      "definition": "Especially, Machine Learning with Neural Networks is seen as an optimization problem",
      "context": "Paper: Lecture Notes: Neural Network Architectures\nAuthors: Evelyn Herberg\nCategories: cs.LG, math.OC, 68T07\nAbstract: These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.882139",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.05133v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.05133v2"
        ]
      }
    },
    {
      "id": "3dda55e4",
      "concept": "architectures",
      "definition": "These lecture notes provide an overview of Neural Network architectures from a mathematical point of view",
      "context": "Paper: Lecture Notes: Neural Network Architectures\nAuthors: Evelyn Herberg\nCategories: cs.LG, math.OC, 68T07\nAbstract: These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.882262",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.05133v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2304.05133v2"
        ]
      }
    },
    {
      "id": "edf71064",
      "concept": "especially",
      "definition": "Especially, Machine Learning with Neural Networks is seen as an optimization problem",
      "context": "Paper: Lecture Notes: Neural Network Architectures\nAuthors: Evelyn Herberg\nCategories: cs.LG, math.OC, 68T07\nAbstract: These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.882314",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.05133v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.05133v2"
        ]
      }
    },
    {
      "id": "b9abd98e",
      "concept": "with neural",
      "definition": "Especially, Machine Learning with Neural Networks is seen as an optimization problem",
      "context": "Paper: Lecture Notes: Neural Network Architectures\nAuthors: Evelyn Herberg\nCategories: cs.LG, math.OC, 68T07\nAbstract: These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.884518",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.05133v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.05133v2"
        ]
      }
    },
    {
      "id": "ff9d9a24",
      "concept": "network architectures",
      "definition": "These lecture notes provide an overview of Neural Network architectures from a mathematical point of view",
      "context": "Paper: Lecture Notes: Neural Network Architectures\nAuthors: Evelyn Herberg\nCategories: cs.LG, math.OC, 68T07\nAbstract: These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.884763",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.05133v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2304.05133v2"
        ]
      }
    },
    {
      "id": "ffa35be9",
      "concept": "machine",
      "definition": "Especially, Machine Learning with Neural Networks is seen as an optimization problem",
      "context": "Paper: Lecture Notes: Neural Network Architectures\nAuthors: Evelyn Herberg\nCategories: cs.LG, math.OC, 68T07\nAbstract: These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.884840",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.05133v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.05133v2"
        ]
      }
    },
    {
      "id": "8142dae1",
      "concept": "covered are",
      "definition": "Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network",
      "context": "Paper: Lecture Notes: Neural Network Architectures\nAuthors: Evelyn Herberg\nCategories: cs.LG, math.OC, 68T07\nAbstract: These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.884924",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.05133v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.05133v2"
        ]
      }
    },
    {
      "id": "421d4fbc",
      "concept": "overview",
      "definition": "These lecture notes provide an overview of Neural Network architectures from a mathematical point of view",
      "context": "Paper: Lecture Notes: Neural Network Architectures\nAuthors: Evelyn Herberg\nCategories: cs.LG, math.OC, 68T07\nAbstract: These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.884977",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.05133v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.05133v2"
        ]
      }
    },
    {
      "id": "15e88642",
      "concept": "seen",
      "definition": "Especially, Machine Learning with Neural Networks is seen as an optimization problem",
      "context": "Paper: Lecture Notes: Neural Network Architectures\nAuthors: Evelyn Herberg\nCategories: cs.LG, math.OC, 68T07\nAbstract: These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.885025",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.05133v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.05133v2"
        ]
      }
    },
    {
      "id": "82fa52f0",
      "concept": "neural",
      "definition": "These lecture notes provide an overview of Neural Network architectures from a mathematical point of view",
      "context": "Paper: Lecture Notes: Neural Network Architectures\nAuthors: Evelyn Herberg\nCategories: cs.LG, math.OC, 68T07\nAbstract: These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.885073",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.05133v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2304.05133v2"
        ]
      }
    },
    {
      "id": "c9a7bd23",
      "concept": "to the",
      "definition": "The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics",
      "context": "Paper: Self-Organizing Multilayered Neural Networks of Optimal Complexity\nAuthors: V. Schetinin\nCategories: cs.NE, cs.AI\nAbstract: The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.989267",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0504056v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0504056v1"
        ]
      }
    },
    {
      "id": "ddd7bd54",
      "concept": "considered",
      "definition": "The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set",
      "context": "Paper: Self-Organizing Multilayered Neural Networks of Optimal Complexity\nAuthors: V. Schetinin\nCategories: cs.NE, cs.AI\nAbstract: The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.990293",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0504056v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0504056v1"
        ]
      }
    },
    {
      "id": "8cb2f5cd",
      "concept": "logical",
      "definition": "The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics",
      "context": "Paper: Self-Organizing Multilayered Neural Networks of Optimal Complexity\nAuthors: V. Schetinin\nCategories: cs.NE, cs.AI\nAbstract: The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.990420",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0504056v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0504056v1"
        ]
      }
    },
    {
      "id": "71515dff",
      "concept": "used to",
      "definition": "The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics",
      "context": "Paper: Self-Organizing Multilayered Neural Networks of Optimal Complexity\nAuthors: V. Schetinin\nCategories: cs.NE, cs.AI\nAbstract: The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.990473",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0504056v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0504056v1"
        ]
      }
    },
    {
      "id": "17c7da1e",
      "concept": "diagnostics",
      "definition": "The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics",
      "context": "Paper: Self-Organizing Multilayered Neural Networks of Optimal Complexity\nAuthors: V. Schetinin\nCategories: cs.NE, cs.AI\nAbstract: The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.990518",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0504056v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0504056v1"
        ]
      }
    },
    {
      "id": "41984468",
      "concept": "medical",
      "definition": "The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics",
      "context": "Paper: Self-Organizing Multilayered Neural Networks of Optimal Complexity\nAuthors: V. Schetinin\nCategories: cs.NE, cs.AI\nAbstract: The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.990561",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0504056v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0504056v1"
        ]
      }
    },
    {
      "id": "35d2849f",
      "concept": "neural",
      "definition": "The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set",
      "context": "Paper: Self-Organizing Multilayered Neural Networks of Optimal Complexity\nAuthors: V. Schetinin\nCategories: cs.NE, cs.AI\nAbstract: The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.990603",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0504056v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/0504056v1"
        ]
      }
    },
    {
      "id": "e318e3be",
      "concept": "networks",
      "definition": "The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set",
      "context": "Paper: Self-Organizing Multilayered Neural Networks of Optimal Complexity\nAuthors: V. Schetinin\nCategories: cs.NE, cs.AI\nAbstract: The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.990646",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0504056v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/0504056v1"
        ]
      }
    },
    {
      "id": "68f0e412",
      "concept": "medical diagnostics",
      "definition": "The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics",
      "context": "Paper: Self-Organizing Multilayered Neural Networks of Optimal Complexity\nAuthors: V. Schetinin\nCategories: cs.NE, cs.AI\nAbstract: The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.990683",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0504056v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0504056v1"
        ]
      }
    },
    {
      "id": "ced811f9",
      "concept": "multi",
      "definition": "The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics",
      "context": "Paper: Self-Organizing Multilayered Neural Networks of Optimal Complexity\nAuthors: V. Schetinin\nCategories: cs.NE, cs.AI\nAbstract: The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:39.990719",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0504056v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/0504056v1"
        ]
      }
    },
    {
      "id": "dd36bcd9",
      "concept": "can be",
      "definition": "Functions are rich in meaning and can be interpreted in a variety of ways",
      "context": "Paper: Neural Network Processing Neural Networks: An efficient way to learn   higher order functions\nAuthors: Firat Tuna\nCategories: cs.LG, cs.NE\nAbstract: Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.096312",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.05640v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.05640v2"
        ]
      }
    },
    {
      "id": "1784fe1c",
      "concept": "functions are",
      "definition": "Functions are rich in meaning and can be interpreted in a variety of ways",
      "context": "Paper: Neural Network Processing Neural Networks: An efficient way to learn   higher order functions\nAuthors: Firat Tuna\nCategories: cs.LG, cs.NE\nAbstract: Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.096495",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.05640v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.05640v2"
        ]
      }
    },
    {
      "id": "e7d70338",
      "concept": "process",
      "definition": "In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values",
      "context": "Paper: Neural Network Processing Neural Networks: An efficient way to learn   higher order functions\nAuthors: Firat Tuna\nCategories: cs.LG, cs.NE\nAbstract: Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.096547",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.05640v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1911.05640v2"
        ]
      }
    },
    {
      "id": "97568b1f",
      "concept": "nnpnns",
      "definition": "In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values",
      "context": "Paper: Neural Network Processing Neural Networks: An efficient way to learn   higher order functions\nAuthors: Firat Tuna\nCategories: cs.LG, cs.NE\nAbstract: Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.096595",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.05640v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.05640v2"
        ]
      }
    },
    {
      "id": "f1d363c7",
      "concept": "just numerical",
      "definition": "In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values",
      "context": "Paper: Neural Network Processing Neural Networks: An efficient way to learn   higher order functions\nAuthors: Firat Tuna\nCategories: cs.LG, cs.NE\nAbstract: Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.096641",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.05640v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.05640v2"
        ]
      }
    },
    {
      "id": "e99af4af",
      "concept": "we propose",
      "definition": "In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values",
      "context": "Paper: Neural Network Processing Neural Networks: An efficient way to learn   higher order functions\nAuthors: Firat Tuna\nCategories: cs.LG, cs.NE\nAbstract: Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.096687",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.05640v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.05640v2"
        ]
      }
    },
    {
      "id": "3b2412a1",
      "concept": "propose",
      "definition": "In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values",
      "context": "Paper: Neural Network Processing Neural Networks: An efficient way to learn   higher order functions\nAuthors: Firat Tuna\nCategories: cs.LG, cs.NE\nAbstract: Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.096733",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.05640v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.05640v2"
        ]
      }
    },
    {
      "id": "090f567b",
      "concept": "learn",
      "definition": "Functions are rich in meaning and can be interpreted in a variety of ways",
      "context": "Paper: Neural Network Processing Neural Networks: An efficient way to learn   higher order functions\nAuthors: Firat Tuna\nCategories: cs.LG, cs.NE\nAbstract: Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.096777",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.05640v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1911.05640v2"
        ]
      }
    },
    {
      "id": "6a1a18b1",
      "concept": "order",
      "definition": "Functions are rich in meaning and can be interpreted in a variety of ways",
      "context": "Paper: Neural Network Processing Neural Networks: An efficient way to learn   higher order functions\nAuthors: Firat Tuna\nCategories: cs.LG, cs.NE\nAbstract: Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.096823",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.05640v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1911.05640v2"
        ]
      }
    },
    {
      "id": "16f07c79",
      "concept": "and numerical",
      "definition": "In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values",
      "context": "Paper: Neural Network Processing Neural Networks: An efficient way to learn   higher order functions\nAuthors: Firat Tuna\nCategories: cs.LG, cs.NE\nAbstract: Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.096871",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.05640v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1911.05640v2"
        ]
      }
    },
    {
      "id": "8aca9772",
      "concept": "computation issue",
      "definition": "Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems",
      "context": "Paper: Guaranteed Quantization Error Computation for Neural Network Model   Compression\nAuthors: Wesley Cooke, Zihao Mo, Weiming Xiang\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods ar...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.198969",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.13812v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.13812v1"
        ]
      }
    },
    {
      "id": "aa412ff8",
      "concept": "merged",
      "definition": "A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks",
      "context": "Paper: Guaranteed Quantization Error Computation for Neural Network Model   Compression\nAuthors: Wesley Cooke, Zihao Mo, Weiming Xiang\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods ar...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.199074",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.13812v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.13812v1"
        ]
      }
    },
    {
      "id": "332fc516",
      "concept": "systems",
      "definition": "Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems",
      "context": "Paper: Guaranteed Quantization Error Computation for Neural Network Model   Compression\nAuthors: Wesley Cooke, Zihao Mo, Weiming Xiang\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods ar...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.199122",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.13812v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.13812v1"
        ]
      }
    },
    {
      "id": "aad9d6d6",
      "concept": "industrial",
      "definition": "Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems",
      "context": "Paper: Guaranteed Quantization Error Computation for Neural Network Model   Compression\nAuthors: Wesley Cooke, Zihao Mo, Weiming Xiang\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods ar...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.199165",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.13812v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.13812v1"
        ]
      }
    },
    {
      "id": "036e77b2",
      "concept": "addressed",
      "definition": "The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper",
      "context": "Paper: Guaranteed Quantization Error Computation for Neural Network Model   Compression\nAuthors: Wesley Cooke, Zihao Mo, Weiming Xiang\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods ar...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.199201",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.13812v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.13812v1"
        ]
      }
    },
    {
      "id": "a2a1ba29",
      "concept": "merged neural",
      "definition": "A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks",
      "context": "Paper: Guaranteed Quantization Error Computation for Neural Network Model   Compression\nAuthors: Wesley Cooke, Zihao Mo, Weiming Xiang\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods ar...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.199240",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.13812v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.13812v1"
        ]
      }
    },
    {
      "id": "c0c5c072",
      "concept": "of the",
      "definition": "Finally, a numerical example is proposed to validate the applicability and effectiveness of the proposed approach",
      "context": "Paper: Guaranteed Quantization Error Computation for Neural Network Model   Compression\nAuthors: Wesley Cooke, Zihao Mo, Weiming Xiang\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods ar...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.199282",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.13812v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.13812v1"
        ]
      }
    },
    {
      "id": "c3882ffb",
      "concept": "version to",
      "definition": "A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks",
      "context": "Paper: Guaranteed Quantization Error Computation for Neural Network Model   Compression\nAuthors: Wesley Cooke, Zihao Mo, Weiming Xiang\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods ar...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.199327",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.13812v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.13812v1"
        ]
      }
    },
    {
      "id": "8740558c",
      "concept": "address the",
      "definition": "Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems",
      "context": "Paper: Guaranteed Quantization Error Computation for Neural Network Model   Compression\nAuthors: Wesley Cooke, Zihao Mo, Weiming Xiang\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods ar...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.199391",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.13812v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.13812v1"
        ]
      }
    },
    {
      "id": "563924f7",
      "concept": "deep",
      "definition": "Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems",
      "context": "Paper: Guaranteed Quantization Error Computation for Neural Network Model   Compression\nAuthors: Wesley Cooke, Zihao Mo, Weiming Xiang\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods ar...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.199439",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.13812v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.13812v1"
        ]
      }
    },
    {
      "id": "57a9c3c5",
      "concept": "of its",
      "definition": "Using this representation we show that: (1) a \"sweet spot\" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks",
      "context": "Paper: Graph Structure of Neural Networks\nAuthors: Jiaxuan You, Jure Leskovec, Kaiming He\nCategories: cs.LG, cs.CV, cs.SI, stat.ML\nAbstract: Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.305574",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.06559v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2007.06559v2"
        ]
      }
    },
    {
      "id": "469db2d1",
      "concept": "be identified",
      "definition": "Using this representation we show that: (1) a \"sweet spot\" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks",
      "context": "Paper: Graph Structure of Neural Networks\nAuthors: Jiaxuan You, Jure Leskovec, Kaiming He\nCategories: cs.LG, cs.CV, cs.SI, stat.ML\nAbstract: Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.305703",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.06559v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2007.06559v2"
        ]
      }
    },
    {
      "id": "6a974e29",
      "concept": "biological neural",
      "definition": "Using this representation we show that: (1) a \"sweet spot\" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks",
      "context": "Paper: Graph Structure of Neural Networks\nAuthors: Jiaxuan You, Jure Leskovec, Kaiming He\nCategories: cs.LG, cs.CV, cs.SI, stat.ML\nAbstract: Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.305765",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.06559v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2007.06559v2"
        ]
      }
    },
    {
      "id": "1deb672c",
      "concept": "performance is",
      "definition": "Using this representation we show that: (1) a \"sweet spot\" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks",
      "context": "Paper: Graph Structure of Neural Networks\nAuthors: Jiaxuan You, Jure Leskovec, Kaiming He\nCategories: cs.LG, cs.CV, cs.SI, stat.ML\nAbstract: Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.305820",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.06559v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2007.06559v2"
        ]
      }
    },
    {
      "id": "08ec95e1",
      "concept": "of the",
      "definition": "However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance",
      "context": "Paper: Graph Structure of Neural Networks\nAuthors: Jiaxuan You, Jure Leskovec, Kaiming He\nCategories: cs.LG, cs.CV, cs.SI, stat.ML\nAbstract: Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.305870",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.06559v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2007.06559v2"
        ]
      }
    },
    {
      "id": "605c1c93",
      "concept": "performing neural",
      "definition": "Using this representation we show that: (1) a \"sweet spot\" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks",
      "context": "Paper: Graph Structure of Neural Networks\nAuthors: Jiaxuan You, Jure Leskovec, Kaiming He\nCategories: cs.LG, cs.CV, cs.SI, stat.ML\nAbstract: Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.305931",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.06559v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2007.06559v2"
        ]
      }
    },
    {
      "id": "b4fe84dc",
      "concept": "correspond to",
      "definition": "To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure",
      "context": "Paper: Graph Structure of Neural Networks\nAuthors: Jiaxuan You, Jure Leskovec, Kaiming He\nCategories: cs.LG, cs.CV, cs.SI, stat.ML\nAbstract: Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.305984",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.06559v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2007.06559v2"
        ]
      }
    },
    {
      "id": "299b6ece",
      "concept": "the graph",
      "definition": "However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance",
      "context": "Paper: Graph Structure of Neural Networks\nAuthors: Jiaxuan You, Jure Leskovec, Kaiming He\nCategories: cs.LG, cs.CV, cs.SI, stat.ML\nAbstract: Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.306033",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.06559v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2007.06559v2"
        ]
      }
    },
    {
      "id": "60596bff",
      "concept": "tasks",
      "definition": "Using this representation we show that: (1) a \"sweet spot\" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks",
      "context": "Paper: Graph Structure of Neural Networks\nAuthors: Jiaxuan You, Jure Leskovec, Kaiming He\nCategories: cs.LG, cs.CV, cs.SI, stat.ML\nAbstract: Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.306086",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.06559v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2007.06559v2"
        ]
      }
    },
    {
      "id": "a2b6286c",
      "concept": "despite",
      "definition": "However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance",
      "context": "Paper: Graph Structure of Neural Networks\nAuthors: Jiaxuan You, Jure Leskovec, Kaiming He\nCategories: cs.LG, cs.CV, cs.SI, stat.ML\nAbstract: Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.306139",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2007.06559v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2007.06559v2"
        ]
      }
    },
    {
      "id": "1f2ef777",
      "concept": "to combine",
      "definition": "We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance",
      "context": "Paper: Hybrid Quantum-Classical Neural Networks for Downlink Beamforming   Optimization\nAuthors: Juping Zhang, Gan Zheng, Toshiaki Koike-Akino\nCategories: cs.IT, math.IT\nAbstract: This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.410491",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.04747v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2408.04747v1"
        ]
      }
    },
    {
      "id": "5fc03a18",
      "concept": "can be",
      "definition": "The classical neural network can be jointly trained with the quantum neural network or pre-trained leading to a fine-tuning transfer learning method",
      "context": "Paper: Hybrid Quantum-Classical Neural Networks for Downlink Beamforming   Optimization\nAuthors: Juping Zhang, Gan Zheng, Toshiaki Koike-Akino\nCategories: cs.IT, math.IT\nAbstract: This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.410654",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.04747v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2408.04747v1"
        ]
      }
    },
    {
      "id": "586724cc",
      "concept": "one proposes",
      "definition": "The first one proposes a quantum neural network employing parameterized quantum circuits that follows a classical convolutional neural network",
      "context": "Paper: Hybrid Quantum-Classical Neural Networks for Downlink Beamforming   Optimization\nAuthors: Juping Zhang, Gan Zheng, Toshiaki Koike-Akino\nCategories: cs.IT, math.IT\nAbstract: This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.410729",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.04747v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2408.04747v1"
        ]
      }
    },
    {
      "id": "5e15e07b",
      "concept": "quantum-classical",
      "definition": "Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system",
      "context": "Paper: Hybrid Quantum-Classical Neural Networks for Downlink Beamforming   Optimization\nAuthors: Juping Zhang, Gan Zheng, Toshiaki Koike-Akino\nCategories: cs.IT, math.IT\nAbstract: This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.410789",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.04747v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2408.04747v1"
        ]
      }
    },
    {
      "id": "125914b3",
      "concept": "multiuser multiple",
      "definition": "This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system",
      "context": "Paper: Hybrid Quantum-Classical Neural Networks for Downlink Beamforming   Optimization\nAuthors: Juping Zhang, Gan Zheng, Toshiaki Koike-Akino\nCategories: cs.IT, math.IT\nAbstract: This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.410848",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.04747v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2408.04747v1"
        ]
      }
    },
    {
      "id": "50c0f97d",
      "concept": "hardware",
      "definition": "The robustness of the proposed methods is verified using both software simulators and hardware emulators considering noisy intermediate-scale quantum devices",
      "context": "Paper: Hybrid Quantum-Classical Neural Networks for Downlink Beamforming   Optimization\nAuthors: Juping Zhang, Gan Zheng, Toshiaki Koike-Akino\nCategories: cs.IT, math.IT\nAbstract: This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.410909",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.04747v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2408.04747v1"
        ]
      }
    },
    {
      "id": "a59d8ab1",
      "concept": "circuits that",
      "definition": "The first one proposes a quantum neural network employing parameterized quantum circuits that follows a classical convolutional neural network",
      "context": "Paper: Hybrid Quantum-Classical Neural Networks for Downlink Beamforming   Optimization\nAuthors: Juping Zhang, Gan Zheng, Toshiaki Koike-Akino\nCategories: cs.IT, math.IT\nAbstract: This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.410965",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.04747v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2408.04747v1"
        ]
      }
    },
    {
      "id": "2cff742a",
      "concept": "especially",
      "definition": "Our results demonstrate the feasibility of the proposed hybrid neural networks, and reveal that the first method can achieve similar sum rate performance compared to a benchmark classical neural network with significantly less training parameters; while the second method can achieve higher sum rate especially in presence of many users still with less training parameters",
      "context": "Paper: Hybrid Quantum-Classical Neural Networks for Downlink Beamforming   Optimization\nAuthors: Juping Zhang, Gan Zheng, Toshiaki Koike-Akino\nCategories: cs.IT, math.IT\nAbstract: This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.411024",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.04747v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2408.04747v1"
        ]
      }
    },
    {
      "id": "7ae8fdfd",
      "concept": "classical convolutional",
      "definition": "The first one proposes a quantum neural network employing parameterized quantum circuits that follows a classical convolutional neural network",
      "context": "Paper: Hybrid Quantum-Classical Neural Networks for Downlink Beamforming   Optimization\nAuthors: Juping Zhang, Gan Zheng, Toshiaki Koike-Akino\nCategories: cs.IT, math.IT\nAbstract: This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.411081",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.04747v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2408.04747v1"
        ]
      }
    },
    {
      "id": "8384241a",
      "concept": "sum rate",
      "definition": "Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system",
      "context": "Paper: Hybrid Quantum-Classical Neural Networks for Downlink Beamforming   Optimization\nAuthors: Juping Zhang, Gan Zheng, Toshiaki Koike-Akino\nCategories: cs.IT, math.IT\nAbstract: This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.411137",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.04747v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2408.04747v1"
        ]
      }
    },
    {
      "id": "e537fce3",
      "concept": "furthermore",
      "definition": "Furthermore, it is also shown that the accuracy of a poorly trained neural network model can be improved to the same level as that of an adequately trained neural network model, potentially decreasing the training cost and required data to train a neural network",
      "context": "Paper: A Novel Neural Filter to Improve Accuracy of Neural Network Models of   Dynamic Systems\nAuthors: Parham Oveissi, Turibius Rozario, Ankit Goel\nCategories: cs.LG, math.DS\nAbstract: The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the ne...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.521335",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2409.13654v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2409.13654v2"
        ]
      }
    },
    {
      "id": "13af69a1",
      "concept": "systems",
      "definition": "The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions",
      "context": "Paper: A Novel Neural Filter to Improve Accuracy of Neural Network Models of   Dynamic Systems\nAuthors: Parham Oveissi, Turibius Rozario, Ankit Goel\nCategories: cs.LG, math.DS\nAbstract: The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the ne...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.521463",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2409.13654v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2409.13654v2"
        ]
      }
    },
    {
      "id": "0d2a69b6",
      "concept": "required",
      "definition": "Furthermore, it is also shown that the accuracy of a poorly trained neural network model can be improved to the same level as that of an adequately trained neural network model, potentially decreasing the training cost and required data to train a neural network",
      "context": "Paper: A Novel Neural Filter to Improve Accuracy of Neural Network Models of   Dynamic Systems\nAuthors: Parham Oveissi, Turibius Rozario, Ankit Goel\nCategories: cs.LG, math.DS\nAbstract: The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the ne...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.521510",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2409.13654v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2409.13654v2"
        ]
      }
    },
    {
      "id": "8e1ef5ba",
      "concept": "the application",
      "definition": "The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions",
      "context": "Paper: A Novel Neural Filter to Improve Accuracy of Neural Network Models of   Dynamic Systems\nAuthors: Parham Oveissi, Turibius Rozario, Ankit Goel\nCategories: cs.LG, math.DS\nAbstract: The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the ne...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.521553",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2409.13654v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2409.13654v2"
        ]
      }
    },
    {
      "id": "41699431",
      "concept": "long-term",
      "definition": "Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy",
      "context": "Paper: A Novel Neural Filter to Improve Accuracy of Neural Network Models of   Dynamic Systems\nAuthors: Parham Oveissi, Turibius Rozario, Ankit Goel\nCategories: cs.LG, math.DS\nAbstract: The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the ne...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.521596",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2409.13654v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2409.13654v2"
        ]
      }
    },
    {
      "id": "dbe4c983",
      "concept": "over",
      "definition": "Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy",
      "context": "Paper: A Novel Neural Filter to Improve Accuracy of Neural Network Models of   Dynamic Systems\nAuthors: Parham Oveissi, Turibius Rozario, Ankit Goel\nCategories: cs.LG, math.DS\nAbstract: The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the ne...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.521642",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2409.13654v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2409.13654v2"
        ]
      }
    },
    {
      "id": "48f22c40",
      "concept": "systems has",
      "definition": "The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions",
      "context": "Paper: A Novel Neural Filter to Improve Accuracy of Neural Network Models of   Dynamic Systems\nAuthors: Parham Oveissi, Turibius Rozario, Ankit Goel\nCategories: cs.LG, math.DS\nAbstract: The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the ne...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.521682",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2409.13654v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2409.13654v2"
        ]
      }
    },
    {
      "id": "ae572c52",
      "concept": "long",
      "definition": "Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy",
      "context": "Paper: A Novel Neural Filter to Improve Accuracy of Neural Network Models of   Dynamic Systems\nAuthors: Parham Oveissi, Turibius Rozario, Ankit Goel\nCategories: cs.LG, math.DS\nAbstract: The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the ne...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.521728",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2409.13654v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2409.13654v2"
        ]
      }
    },
    {
      "id": "3aefe0bd",
      "concept": "application",
      "definition": "The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions",
      "context": "Paper: A Novel Neural Filter to Improve Accuracy of Neural Network Models of   Dynamic Systems\nAuthors: Parham Oveissi, Turibius Rozario, Ankit Goel\nCategories: cs.LG, math.DS\nAbstract: The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the ne...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.521770",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2409.13654v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2409.13654v2"
        ]
      }
    },
    {
      "id": "4897fa83",
      "concept": "improve accuracy",
      "definition": "The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions",
      "context": "Paper: A Novel Neural Filter to Improve Accuracy of Neural Network Models of   Dynamic Systems\nAuthors: Parham Oveissi, Turibius Rozario, Ankit Goel\nCategories: cs.LG, math.DS\nAbstract: The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the ne...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.521825",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2409.13654v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2409.13654v2"
        ]
      }
    },
    {
      "id": "952ff51a",
      "concept": "reach",
      "definition": "Our experiments proved its ability on the Cortex Neural Network can reach accuracy by 98",
      "context": "Paper: Cortex Neural Network: learning with Neural Network groups\nAuthors: Liyao Gao\nCategories: cs.NE, cs.CV, cs.LG\nAbstract: Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.624889",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1804.03313v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1804.03313v1"
        ]
      }
    },
    {
      "id": "4c5f2b0d",
      "concept": "higher accuracy",
      "definition": "In our implementation, the Cortex Neural Network is able to process different cognitive tasks and perform reflection to get a higher accuracy",
      "context": "Paper: Cortex Neural Network: learning with Neural Network groups\nAuthors: Liyao Gao\nCategories: cs.NE, cs.CV, cs.LG\nAbstract: Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.624960",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1804.03313v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1804.03313v1"
        ]
      }
    },
    {
      "id": "1c093cd5",
      "concept": "the current",
      "definition": "However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together",
      "context": "Paper: Cortex Neural Network: learning with Neural Network groups\nAuthors: Liyao Gao\nCategories: cs.NE, cs.CV, cs.LG\nAbstract: Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.624994",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1804.03313v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1804.03313v1"
        ]
      }
    },
    {
      "id": "d3eb2346",
      "concept": "recognition",
      "definition": "Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation",
      "context": "Paper: Cortex Neural Network: learning with Neural Network groups\nAuthors: Liyao Gao\nCategories: cs.NE, cs.CV, cs.LG\nAbstract: Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.625026",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1804.03313v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1804.03313v1"
        ]
      }
    },
    {
      "id": "3ed922b6",
      "concept": "machine",
      "definition": "Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation",
      "context": "Paper: Cortex Neural Network: learning with Neural Network groups\nAuthors: Liyao Gao\nCategories: cs.NE, cs.CV, cs.LG\nAbstract: Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.625057",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1804.03313v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1804.03313v1"
        ]
      }
    },
    {
      "id": "ff76aa33",
      "concept": "provide",
      "definition": "We provide a series of experiments to examine the capability of the cortex architecture on traditional neural networks",
      "context": "Paper: Cortex Neural Network: learning with Neural Network groups\nAuthors: Liyao Gao\nCategories: cs.NE, cs.CV, cs.LG\nAbstract: Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.625094",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1804.03313v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1804.03313v1"
        ]
      }
    },
    {
      "id": "1f534770",
      "concept": "important architecture",
      "definition": "Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task",
      "context": "Paper: Cortex Neural Network: learning with Neural Network groups\nAuthors: Liyao Gao\nCategories: cs.NE, cs.CV, cs.LG\nAbstract: Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.625129",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1804.03313v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1804.03313v1"
        ]
      }
    },
    {
      "id": "e18480f4",
      "concept": "handle different",
      "definition": "The Cortex Neural Network is an upper architecture of neural networks which motivated from cerebral cortex in the brain to handle different tasks in the same learning system",
      "context": "Paper: Cortex Neural Network: learning with Neural Network groups\nAuthors: Liyao Gao\nCategories: cs.NE, cs.CV, cs.LG\nAbstract: Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.625187",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1804.03313v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1804.03313v1"
        ]
      }
    },
    {
      "id": "658da9c0",
      "concept": "tasks",
      "definition": "However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together",
      "context": "Paper: Cortex Neural Network: learning with Neural Network groups\nAuthors: Liyao Gao\nCategories: cs.NE, cs.CV, cs.LG\nAbstract: Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.625230",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1804.03313v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1804.03313v1"
        ]
      }
    },
    {
      "id": "3e03a29c",
      "concept": "animals",
      "definition": "Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task",
      "context": "Paper: Cortex Neural Network: learning with Neural Network groups\nAuthors: Liyao Gao\nCategories: cs.NE, cs.CV, cs.LG\nAbstract: Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.625265",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1804.03313v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1804.03313v1"
        ]
      }
    },
    {
      "id": "1a15c2a9",
      "concept": "optical neural",
      "definition": "A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given",
      "context": "Paper: Parametrical Neural Networks and Some Other Similar Architectures\nAuthors: Leonid B. Litinskii\nCategories: cs.CV, cs.NE\nAbstract: A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.726270",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0608073v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0608073v1"
        ]
      }
    },
    {
      "id": "a4814b0e",
      "concept": "pnn have",
      "definition": "For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation)",
      "context": "Paper: Parametrical Neural Networks and Some Other Similar Architectures\nAuthors: Leonid B. Litinskii\nCategories: cs.CV, cs.NE\nAbstract: A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.726335",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0608073v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0608073v1"
        ]
      }
    },
    {
      "id": "83839ed4",
      "concept": "basic ideas",
      "definition": "Presentation of basic ideas and principles is accentuated",
      "context": "Paper: Parametrical Neural Networks and Some Other Similar Architectures\nAuthors: Leonid B. Litinskii\nCategories: cs.CV, cs.NE\nAbstract: A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.726413",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0608073v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0608073v1"
        ]
      }
    },
    {
      "id": "e78695e3",
      "concept": "parametrical",
      "definition": "The presentation is based on description of parametrical neural networks (PNN)",
      "context": "Paper: Parametrical Neural Networks and Some Other Similar Architectures\nAuthors: Leonid B. Litinskii\nCategories: cs.CV, cs.NE\nAbstract: A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.726454",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0608073v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/0608073v1"
        ]
      }
    },
    {
      "id": "373fcf47",
      "concept": "architectures",
      "definition": "A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given",
      "context": "Paper: Parametrical Neural Networks and Some Other Similar Architectures\nAuthors: Leonid B. Litinskii\nCategories: cs.CV, cs.NE\nAbstract: A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.726497",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0608073v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/0608073v1"
        ]
      }
    },
    {
      "id": "2961d877",
      "concept": "given",
      "definition": "A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given",
      "context": "Paper: Parametrical Neural Networks and Some Other Similar Architectures\nAuthors: Leonid B. Litinskii\nCategories: cs.CV, cs.NE\nAbstract: A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.726531",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0608073v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0608073v1"
        ]
      }
    },
    {
      "id": "bbc469da",
      "concept": "on description",
      "definition": "The presentation is based on description of parametrical neural networks (PNN)",
      "context": "Paper: Parametrical Neural Networks and Some Other Similar Architectures\nAuthors: Leonid B. Litinskii\nCategories: cs.CV, cs.NE\nAbstract: A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.726564",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0608073v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0608073v1"
        ]
      }
    },
    {
      "id": "b4896b1c",
      "concept": "recognizing",
      "definition": "For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation)",
      "context": "Paper: Parametrical Neural Networks and Some Other Similar Architectures\nAuthors: Leonid B. Litinskii\nCategories: cs.CV, cs.NE\nAbstract: A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.726599",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0608073v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0608073v1"
        ]
      }
    },
    {
      "id": "a3766507",
      "concept": "immunity",
      "definition": "For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation)",
      "context": "Paper: Parametrical Neural Networks and Some Other Similar Architectures\nAuthors: Leonid B. Litinskii\nCategories: cs.CV, cs.NE\nAbstract: A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.726632",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0608073v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0608073v1"
        ]
      }
    },
    {
      "id": "3417650e",
      "concept": "today",
      "definition": "For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation)",
      "context": "Paper: Parametrical Neural Networks and Some Other Similar Architectures\nAuthors: Leonid B. Litinskii\nCategories: cs.CV, cs.NE\nAbstract: A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.726666",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/0608073v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/0608073v1"
        ]
      }
    },
    {
      "id": "096c6475",
      "concept": "aiq was",
      "definition": "The LeNet-5 network with the highest aIQ was 2",
      "context": "Paper: Assessing Intelligence in Artificial Neural Networks\nAuthors: Nicholas J. Schaub, Nathan Hotaling\nCategories: cs.LG, cs.AI, cs.CV\nAbstract: The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.829859",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.02909v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2006.02909v1"
        ]
      }
    },
    {
      "id": "74c3c087",
      "concept": "layer utilization",
      "definition": "To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency",
      "context": "Paper: Assessing Intelligence in Artificial Neural Networks\nAuthors: Nicholas J. Schaub, Nathan Hotaling\nCategories: cs.LG, cs.AI, cs.CV\nAbstract: The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.829961",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.02909v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2006.02909v1"
        ]
      }
    },
    {
      "id": "b2cfceb5",
      "concept": "network architectures",
      "definition": "The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance",
      "context": "Paper: Assessing Intelligence in Artificial Neural Networks\nAuthors: Nicholas J. Schaub, Nathan Hotaling\nCategories: cs.LG, cs.AI, cs.CV\nAbstract: The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.829998",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.02909v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2006.02909v1"
        ]
      }
    },
    {
      "id": "b6e98338",
      "concept": "than",
      "definition": "32% less accurate but contained 30,912 times fewer parameters than the highest accuracy network",
      "context": "Paper: Assessing Intelligence in Artificial Neural Networks\nAuthors: Nicholas J. Schaub, Nathan Hotaling\nCategories: cs.LG, cs.AI, cs.CV\nAbstract: The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.830039",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.02909v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2006.02909v1"
        ]
      }
    },
    {
      "id": "4dd8e026",
      "concept": "of the",
      "definition": "51% even when 75% of the class labels are randomized",
      "context": "Paper: Assessing Intelligence in Artificial Neural Networks\nAuthors: Nicholas J. Schaub, Nathan Hotaling\nCategories: cs.LG, cs.AI, cs.CV\nAbstract: The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.830078",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.02909v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2006.02909v1"
        ]
      }
    },
    {
      "id": "0a0ab5f8",
      "concept": "and overtraining",
      "definition": "Finally, high aIQ networks are shown to be memorization and overtraining resistant, capable of learning proper digit classification with an accuracy of 92",
      "context": "Paper: Assessing Intelligence in Artificial Neural Networks\nAuthors: Nicholas J. Schaub, Nathan Hotaling\nCategories: cs.LG, cs.AI, cs.CV\nAbstract: The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.830116",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.02909v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2006.02909v1"
        ]
      }
    },
    {
      "id": "58459f53",
      "concept": "even",
      "definition": "51% even when 75% of the class labels are randomized",
      "context": "Paper: Assessing Intelligence in Artificial Neural Networks\nAuthors: Nicholas J. Schaub, Nathan Hotaling\nCategories: cs.LG, cs.AI, cs.CV\nAbstract: The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.830154",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.02909v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2006.02909v1"
        ]
      }
    },
    {
      "id": "0366f433",
      "concept": "quotient",
      "definition": "To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency",
      "context": "Paper: Assessing Intelligence in Artificial Neural Networks\nAuthors: Nicholas J. Schaub, Nathan Hotaling\nCategories: cs.LG, cs.AI, cs.CV\nAbstract: The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.830194",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.02909v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2006.02909v1"
        ]
      }
    },
    {
      "id": "9d6e4d3f",
      "concept": "are randomized",
      "definition": "51% even when 75% of the class labels are randomized",
      "context": "Paper: Assessing Intelligence in Artificial Neural Networks\nAuthors: Nicholas J. Schaub, Nathan Hotaling\nCategories: cs.LG, cs.AI, cs.CV\nAbstract: The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.830235",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.02909v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2006.02909v1"
        ]
      }
    },
    {
      "id": "1e6d4f36",
      "concept": "metric",
      "definition": "The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance",
      "context": "Paper: Assessing Intelligence in Artificial Neural Networks\nAuthors: Nicholas J. Schaub, Nathan Hotaling\nCategories: cs.LG, cs.AI, cs.CV\nAbstract: The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.830273",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.02909v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2006.02909v1"
        ]
      }
    },
    {
      "id": "ba35e905",
      "concept": "they use",
      "definition": "However, one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning tasks",
      "context": "Paper: Rational Neural Network Controllers\nAuthors: Matthew Newton, Antonis Papachristodoulou\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properl...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.935278",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.06287v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2307.06287v1"
        ]
      }
    },
    {
      "id": "66b20bf7",
      "concept": "systems",
      "definition": "Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller",
      "context": "Paper: Rational Neural Network Controllers\nAuthors: Matthew Newton, Antonis Papachristodoulou\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properl...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.935455",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.06287v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2307.06287v1"
        ]
      }
    },
    {
      "id": "19623c84",
      "concept": "activation functions",
      "definition": "This paper considers rational neural networks and presents novel rational activation functions, which can be used effectively in robustness problems for neural feedback loops",
      "context": "Paper: Rational Neural Network Controllers\nAuthors: Matthew Newton, Antonis Papachristodoulou\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properl...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.935513",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.06287v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2307.06287v1"
        ]
      }
    },
    {
      "id": "be96116c",
      "concept": "controllers",
      "definition": "This means that, unless they are designed properly, they are not an ideal candidate for controllers due to issues with robustness and uncertainty, which are pivotal aspects of control systems",
      "context": "Paper: Rational Neural Network Controllers\nAuthors: Matthew Newton, Antonis Papachristodoulou\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properl...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.935569",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.06287v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2307.06287v1"
        ]
      }
    },
    {
      "id": "f9c3b942",
      "concept": "network architectures",
      "definition": "However, one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning tasks",
      "context": "Paper: Rational Neural Network Controllers\nAuthors: Matthew Newton, Antonis Papachristodoulou\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properl...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.935613",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.06287v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2307.06287v1"
        ]
      }
    },
    {
      "id": "73dac9ce",
      "concept": "machine",
      "definition": "Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators",
      "context": "Paper: Rational Neural Network Controllers\nAuthors: Matthew Newton, Antonis Papachristodoulou\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properl...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.935654",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.06287v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2307.06287v1"
        ]
      }
    },
    {
      "id": "c9b62fd4",
      "concept": "this method",
      "definition": "Numerical examples show that this method can successfully recover stabilising rational neural network controllers for neural feedback loops with non-linear plants with noise and parametric uncertainty",
      "context": "Paper: Rational Neural Network Controllers\nAuthors: Matthew Newton, Antonis Papachristodoulou\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properl...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.935698",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.06287v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2307.06287v1"
        ]
      }
    },
    {
      "id": "9f87a6a7",
      "concept": "they",
      "definition": "This means that, unless they are designed properly, they are not an ideal candidate for controllers due to issues with robustness and uncertainty, which are pivotal aspects of control systems",
      "context": "Paper: Rational Neural Network Controllers\nAuthors: Matthew Newton, Antonis Papachristodoulou\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properl...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.935739",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.06287v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2307.06287v1"
        ]
      }
    },
    {
      "id": "051c5196",
      "concept": "recent",
      "definition": "Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller",
      "context": "Paper: Rational Neural Network Controllers\nAuthors: Matthew Newton, Antonis Papachristodoulou\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properl...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.935778",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.06287v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2307.06287v1"
        ]
      }
    },
    {
      "id": "d9fd83f5",
      "concept": "related tasks",
      "definition": "Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators",
      "context": "Paper: Rational Neural Network Controllers\nAuthors: Matthew Newton, Antonis Papachristodoulou\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properl...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:40.935823",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.06287v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2307.06287v1"
        ]
      }
    },
    {
      "id": "185b7e8f",
      "concept": "parameter space",
      "definition": "We use sieve method to constrain parameter space",
      "context": "Paper: Asymptotic Theory of Expectile Neural Networks\nAuthors: Jinghang Lin, Xiaoxi Shen, Qing Lu\nCategories: math.ST, stat.TH\nAbstract: Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.036942",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2011.01218v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2011.01218v1"
        ]
      }
    },
    {
      "id": "a7d81de2",
      "concept": "and we",
      "definition": "And we prove its consistency and normality under nonparametric regression framework",
      "context": "Paper: Asymptotic Theory of Expectile Neural Networks\nAuthors: Jinghang Lin, Xiaoxi Shen, Qing Lu\nCategories: math.ST, stat.TH\nAbstract: Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.037043",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2011.01218v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2011.01218v1"
        ]
      }
    },
    {
      "id": "e8eaf6e0",
      "concept": "applications",
      "definition": "Neural networks are becoming an increasingly important tool in applications",
      "context": "Paper: Asymptotic Theory of Expectile Neural Networks\nAuthors: Jinghang Lin, Xiaoxi Shen, Qing Lu\nCategories: math.ST, stat.TH\nAbstract: Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.037087",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2011.01218v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2011.01218v1"
        ]
      }
    },
    {
      "id": "7af46ca2",
      "concept": "asymptotic theory",
      "definition": "Neural networks are becoming an increasingly important tool in applications",
      "context": "Paper: Asymptotic Theory of Expectile Neural Networks\nAuthors: Jinghang Lin, Xiaoxi Shen, Qing Lu\nCategories: math.ST, stat.TH\nAbstract: Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.037161",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2011.01218v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2011.01218v1"
        ]
      }
    },
    {
      "id": "10ef2481",
      "concept": "we propose",
      "definition": "In this paper, we propose a new neural networks method called expectile neural networks",
      "context": "Paper: Asymptotic Theory of Expectile Neural Networks\nAuthors: Jinghang Lin, Xiaoxi Shen, Qing Lu\nCategories: math.ST, stat.TH\nAbstract: Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.037203",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2011.01218v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2011.01218v1"
        ]
      }
    },
    {
      "id": "a6ba231f",
      "concept": "in statistical",
      "definition": "However, neural networks are not widely used in statistical genetics",
      "context": "Paper: Asymptotic Theory of Expectile Neural Networks\nAuthors: Jinghang Lin, Xiaoxi Shen, Qing Lu\nCategories: math.ST, stat.TH\nAbstract: Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.037239",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2011.01218v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2011.01218v1"
        ]
      }
    },
    {
      "id": "d941efcd",
      "concept": "size of",
      "definition": "When the size of parameter is too large, the standard maximum likelihood procedures may not work",
      "context": "Paper: Asymptotic Theory of Expectile Neural Networks\nAuthors: Jinghang Lin, Xiaoxi Shen, Qing Lu\nCategories: math.ST, stat.TH\nAbstract: Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.037274",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2011.01218v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2011.01218v1"
        ]
      }
    },
    {
      "id": "077686be",
      "concept": "prove its",
      "definition": "And we prove its consistency and normality under nonparametric regression framework",
      "context": "Paper: Asymptotic Theory of Expectile Neural Networks\nAuthors: Jinghang Lin, Xiaoxi Shen, Qing Lu\nCategories: math.ST, stat.TH\nAbstract: Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.037309",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2011.01218v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2011.01218v1"
        ]
      }
    },
    {
      "id": "209c6844",
      "concept": "propose",
      "definition": "In this paper, we propose a new neural networks method called expectile neural networks",
      "context": "Paper: Asymptotic Theory of Expectile Neural Networks\nAuthors: Jinghang Lin, Xiaoxi Shen, Qing Lu\nCategories: math.ST, stat.TH\nAbstract: Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.037342",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2011.01218v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2011.01218v1"
        ]
      }
    },
    {
      "id": "3aaa000c",
      "concept": "size",
      "definition": "When the size of parameter is too large, the standard maximum likelihood procedures may not work",
      "context": "Paper: Asymptotic Theory of Expectile Neural Networks\nAuthors: Jinghang Lin, Xiaoxi Shen, Qing Lu\nCategories: math.ST, stat.TH\nAbstract: Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.037397",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2011.01218v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/2011.01218v1"
        ]
      }
    },
    {
      "id": "e0f10bbc",
      "concept": "furthermore",
      "definition": "Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization",
      "context": "Paper: Combining Recurrent and Convolutional Neural Networks for Relation   Classification\nAuthors: Ngoc Thang Vu, Heike Adel, Pankaj Gupta\nCategories: cs.CL\nAbstract: This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. F...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.139650",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1605.07333v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1605.07333v1"
        ]
      }
    },
    {
      "id": "b1269843",
      "concept": "introduce",
      "definition": "Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization",
      "context": "Paper: Combining Recurrent and Convolutional Neural Networks for Relation   Classification\nAuthors: Ngoc Thang Vu, Heike Adel, Pankaj Gupta\nCategories: cs.CL\nAbstract: This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. F...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.139759",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1605.07333v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1605.07333v1"
        ]
      }
    },
    {
      "id": "f72db686",
      "concept": "of different",
      "definition": "For both models, we demonstrate the effect of different architectural choices",
      "context": "Paper: Combining Recurrent and Convolutional Neural Networks for Relation   Classification\nAuthors: Ngoc Thang Vu, Heike Adel, Pankaj Gupta\nCategories: cs.CL\nAbstract: This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. F...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.139802",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1605.07333v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1605.07333v1"
        ]
      }
    },
    {
      "id": "d1fe87f1",
      "concept": "investigates two",
      "definition": "This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks",
      "context": "Paper: Combining Recurrent and Convolutional Neural Networks for Relation   Classification\nAuthors: Ngoc Thang Vu, Heike Adel, Pankaj Gupta\nCategories: cs.CL\nAbstract: This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. F...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.139842",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1605.07333v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1605.07333v1"
        ]
      }
    },
    {
      "id": "1e918336",
      "concept": "state-of",
      "definition": "Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task",
      "context": "Paper: Combining Recurrent and Convolutional Neural Networks for Relation   Classification\nAuthors: Ngoc Thang Vu, Heike Adel, Pankaj Gupta\nCategories: cs.CL\nAbstract: This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. F...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.139889",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1605.07333v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1605.07333v1"
        ]
      }
    },
    {
      "id": "63a8d03a",
      "concept": "connectionist",
      "definition": "Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization",
      "context": "Paper: Combining Recurrent and Convolutional Neural Networks for Relation   Classification\nAuthors: Ngoc Thang Vu, Heike Adel, Pankaj Gupta\nCategories: cs.CL\nAbstract: This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. F...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.139927",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1605.07333v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1605.07333v1"
        ]
      }
    },
    {
      "id": "55118728",
      "concept": "relation",
      "definition": "This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks",
      "context": "Paper: Combining Recurrent and Convolutional Neural Networks for Relation   Classification\nAuthors: Ngoc Thang Vu, Heike Adel, Pankaj Gupta\nCategories: cs.CL\nAbstract: This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. F...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.139963",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1605.07333v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1605.07333v1"
        ]
      }
    },
    {
      "id": "f5eb5dfb",
      "concept": "improve",
      "definition": "Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results",
      "context": "Paper: Combining Recurrent and Convolutional Neural Networks for Relation   Classification\nAuthors: Ngoc Thang Vu, Heike Adel, Pankaj Gupta\nCategories: cs.CL\nAbstract: This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. F...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.140038",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1605.07333v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1605.07333v1"
        ]
      }
    },
    {
      "id": "9713be18",
      "concept": "this paper",
      "definition": "This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks",
      "context": "Paper: Combining Recurrent and Convolutional Neural Networks for Relation   Classification\nAuthors: Ngoc Thang Vu, Heike Adel, Pankaj Gupta\nCategories: cs.CL\nAbstract: This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. F...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.140085",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1605.07333v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1605.07333v1"
        ]
      }
    },
    {
      "id": "b2d8bb8d",
      "concept": "for both",
      "definition": "For both models, we demonstrate the effect of different architectural choices",
      "context": "Paper: Combining Recurrent and Convolutional Neural Networks for Relation   Classification\nAuthors: Ngoc Thang Vu, Heike Adel, Pankaj Gupta\nCategories: cs.CL\nAbstract: This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. F...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.140127",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1605.07333v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1605.07333v1"
        ]
      }
    },
    {
      "id": "ff1e0d59",
      "concept": "contributions",
      "definition": "Key contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitioners",
      "context": "Paper: A Comprehensive Review of Spiking Neural Networks: Interpretation,   Optimization, Efficiency, and Best Practices\nAuthors: Kai Malcolm, Josue Casco-Rodriguez\nCategories: cs.NE, cs.LG, eess.IV\nAbstract: Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.241625",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.10780v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2303.10780v2"
        ]
      }
    },
    {
      "id": "9c194a4f",
      "concept": "hardware",
      "definition": "And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings",
      "context": "Paper: A Comprehensive Review of Spiking Neural Networks: Interpretation,   Optimization, Efficiency, and Best Practices\nAuthors: Kai Malcolm, Josue Casco-Rodriguez\nCategories: cs.NE, cs.LG, eess.IV\nAbstract: Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.241782",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.10780v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2303.10780v2"
        ]
      }
    },
    {
      "id": "9e10b0e9",
      "concept": "biological neural",
      "definition": "Biological neural networks continue to inspire breakthroughs in neural network performance",
      "context": "Paper: A Comprehensive Review of Spiking Neural Networks: Interpretation,   Optimization, Efficiency, and Best Practices\nAuthors: Kai Malcolm, Josue Casco-Rodriguez\nCategories: cs.NE, cs.LG, eess.IV\nAbstract: Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.241843",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.10780v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2303.10780v2"
        ]
      }
    },
    {
      "id": "fb5fec64",
      "concept": "discussion",
      "definition": "Key contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitioners",
      "context": "Paper: A Comprehensive Review of Spiking Neural Networks: Interpretation,   Optimization, Efficiency, and Best Practices\nAuthors: Kai Malcolm, Josue Casco-Rodriguez\nCategories: cs.NE, cs.LG, eess.IV\nAbstract: Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.241890",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.10780v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2303.10780v2"
        ]
      }
    },
    {
      "id": "14b11fff",
      "concept": "especially",
      "definition": "And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings",
      "context": "Paper: A Comprehensive Review of Spiking Neural Networks: Interpretation,   Optimization, Efficiency, and Best Practices\nAuthors: Kai Malcolm, Josue Casco-Rodriguez\nCategories: cs.NE, cs.LG, eess.IV\nAbstract: Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.241942",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.10780v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2303.10780v2"
        ]
      }
    },
    {
      "id": "d05313d8",
      "concept": "of spiking",
      "definition": "We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neural networks",
      "context": "Paper: A Comprehensive Review of Spiking Neural Networks: Interpretation,   Optimization, Efficiency, and Best Practices\nAuthors: Kai Malcolm, Josue Casco-Rodriguez\nCategories: cs.NE, cs.LG, eess.IV\nAbstract: Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.241990",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.10780v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2303.10780v2"
        ]
      }
    },
    {
      "id": "28b7f96c",
      "concept": "energy-efficient",
      "definition": "And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings",
      "context": "Paper: A Comprehensive Review of Spiking Neural Networks: Interpretation,   Optimization, Efficiency, and Best Practices\nAuthors: Kai Malcolm, Josue Casco-Rodriguez\nCategories: cs.NE, cs.LG, eess.IV\nAbstract: Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.242040",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.10780v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2303.10780v2"
        ]
      }
    },
    {
      "id": "eb1b1693",
      "concept": "recent",
      "definition": "We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neural networks",
      "context": "Paper: A Comprehensive Review of Spiking Neural Networks: Interpretation,   Optimization, Efficiency, and Best Practices\nAuthors: Kai Malcolm, Josue Casco-Rodriguez\nCategories: cs.NE, cs.LG, eess.IV\nAbstract: Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.242089",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.10780v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2303.10780v2"
        ]
      }
    },
    {
      "id": "c1bf9ad8",
      "concept": "breakthroughs",
      "definition": "Biological neural networks continue to inspire breakthroughs in neural network performance",
      "context": "Paper: A Comprehensive Review of Spiking Neural Networks: Interpretation,   Optimization, Efficiency, and Best Practices\nAuthors: Kai Malcolm, Josue Casco-Rodriguez\nCategories: cs.NE, cs.LG, eess.IV\nAbstract: Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.242134",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.10780v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2303.10780v2"
        ]
      }
    },
    {
      "id": "0c7936e5",
      "concept": "first",
      "definition": "Key contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitioners",
      "context": "Paper: A Comprehensive Review of Spiking Neural Networks: Interpretation,   Optimization, Efficiency, and Best Practices\nAuthors: Kai Malcolm, Josue Casco-Rodriguez\nCategories: cs.NE, cs.LG, eess.IV\nAbstract: Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neura...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.242182",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2303.10780v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2303.10780v2"
        ]
      }
    },
    {
      "id": "96074302",
      "concept": "precision activation",
      "definition": "This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms",
      "context": "Paper: Design and development of opto-neural processors for simulation of   neural networks trained in image detection for potential implementation in   hybrid robotics\nAuthors: Sanjana Shetty\nCategories: cs.ET, cs.AI, cs.LG, cs.NE\nAbstract: Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.344976",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.10289v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.10289v1"
        ]
      }
    },
    {
      "id": "1f4ee799",
      "concept": "over",
      "definition": "Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks",
      "context": "Paper: Design and development of opto-neural processors for simulation of   neural networks trained in image detection for potential implementation in   hybrid robotics\nAuthors: Sanjana Shetty\nCategories: cs.ET, cs.AI, cs.LG, cs.NE\nAbstract: Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.345047",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.10289v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.10289v1"
        ]
      }
    },
    {
      "id": "07ddab86",
      "concept": "comparable to",
      "definition": "This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms",
      "context": "Paper: Design and development of opto-neural processors for simulation of   neural networks trained in image detection for potential implementation in   hybrid robotics\nAuthors: Sanjana Shetty\nCategories: cs.ET, cs.AI, cs.LG, cs.NE\nAbstract: Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.345092",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.10289v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.10289v1"
        ]
      }
    },
    {
      "id": "30b1dd87",
      "concept": "stdp",
      "definition": "This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms",
      "context": "Paper: Design and development of opto-neural processors for simulation of   neural networks trained in image detection for potential implementation in   hybrid robotics\nAuthors: Sanjana Shetty\nCategories: cs.ET, cs.AI, cs.LG, cs.NE\nAbstract: Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.345133",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.10289v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.10289v1"
        ]
      }
    },
    {
      "id": "24895fbc",
      "concept": "precision",
      "definition": "This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms",
      "context": "Paper: Design and development of opto-neural processors for simulation of   neural networks trained in image detection for potential implementation in   hybrid robotics\nAuthors: Sanjana Shetty\nCategories: cs.ET, cs.AI, cs.LG, cs.NE\nAbstract: Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.345190",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.10289v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.10289v1"
        ]
      }
    },
    {
      "id": "8870bde8",
      "concept": "activation",
      "definition": "This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms",
      "context": "Paper: Design and development of opto-neural processors for simulation of   neural networks trained in image detection for potential implementation in   hybrid robotics\nAuthors: Sanjana Shetty\nCategories: cs.ET, cs.AI, cs.LG, cs.NE\nAbstract: Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.345235",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.10289v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.10289v1"
        ]
      }
    },
    {
      "id": "348cd205",
      "concept": "network",
      "definition": "Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others",
      "context": "Paper: Design and development of opto-neural processors for simulation of   neural networks trained in image detection for potential implementation in   hybrid robotics\nAuthors: Sanjana Shetty\nCategories: cs.ET, cs.AI, cs.LG, cs.NE\nAbstract: Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.345274",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.10289v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2401.10289v1"
        ]
      }
    },
    {
      "id": "acfab479",
      "concept": "realism",
      "definition": "Living neural networks offer advantages of lower power consumption, faster processing, and biological realism",
      "context": "Paper: Design and development of opto-neural processors for simulation of   neural networks trained in image detection for potential implementation in   hybrid robotics\nAuthors: Sanjana Shetty\nCategories: cs.ET, cs.AI, cs.LG, cs.NE\nAbstract: Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.345320",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.10289v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.10289v1"
        ]
      }
    },
    {
      "id": "6ff58116",
      "concept": "achieving accuracy",
      "definition": "This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms",
      "context": "Paper: Design and development of opto-neural processors for simulation of   neural networks trained in image detection for potential implementation in   hybrid robotics\nAuthors: Sanjana Shetty\nCategories: cs.ET, cs.AI, cs.LG, cs.NE\nAbstract: Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.345374",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.10289v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.10289v1"
        ]
      }
    },
    {
      "id": "b4b98270",
      "concept": "indirectly",
      "definition": "This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms",
      "context": "Paper: Design and development of opto-neural processors for simulation of   neural networks trained in image detection for potential implementation in   hybrid robotics\nAuthors: Sanjana Shetty\nCategories: cs.ET, cs.AI, cs.LG, cs.NE\nAbstract: Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.345436",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.10289v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.10289v1"
        ]
      }
    },
    {
      "id": "77970e96",
      "concept": "such systems",
      "definition": "It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations",
      "context": "Paper: Convex Formulation of Overparameterized Deep Neural Networks\nAuthors: Cong Fang, Yihong Gu, Weizhong Zhang\nCategories: cs.LG, stat.ML\nAbstract: Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundam...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.449078",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.07626v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1911.07626v1"
        ]
      }
    },
    {
      "id": "d3bf4c39",
      "concept": "systems",
      "definition": "It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations",
      "context": "Paper: Convex Formulation of Overparameterized Deep Neural Networks\nAuthors: Cong Fang, Yihong Gu, Weizhong Zhang\nCategories: cs.LG, stat.ML\nAbstract: Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundam...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.449197",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.07626v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1911.07626v1"
        ]
      }
    },
    {
      "id": "6b8f0f82",
      "concept": "shown that",
      "definition": "It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations",
      "context": "Paper: Convex Formulation of Overparameterized Deep Neural Networks\nAuthors: Cong Fang, Yihong Gu, Weizhong Zhang\nCategories: cs.LG, stat.ML\nAbstract: Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundam...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.449256",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.07626v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1911.07626v1"
        ]
      }
    },
    {
      "id": "45072803",
      "concept": "analyze",
      "definition": "However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice",
      "context": "Paper: Convex Formulation of Overparameterized Deep Neural Networks\nAuthors: Cong Fang, Yihong Gu, Weizhong Zhang\nCategories: cs.LG, stat.ML\nAbstract: Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundam...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.449305",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.07626v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1911.07626v1"
        ]
      }
    },
    {
      "id": "3b7d9709",
      "concept": "of overparameterized",
      "definition": "Analysis of over-parameterized neural networks has drawn significant attention in recentyears",
      "context": "Paper: Convex Formulation of Overparameterized Deep Neural Networks\nAuthors: Cong Fang, Yihong Gu, Weizhong Zhang\nCategories: cs.LG, stat.ML\nAbstract: Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundam...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.449839",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.07626v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1911.07626v1"
        ]
      }
    },
    {
      "id": "5d2c835f",
      "concept": "over",
      "definition": "Analysis of over-parameterized neural networks has drawn significant attention in recentyears",
      "context": "Paper: Convex Formulation of Overparameterized Deep Neural Networks\nAuthors: Cong Fang, Yihong Gu, Weizhong Zhang\nCategories: cs.LG, stat.ML\nAbstract: Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundam...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.449988",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.07626v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1911.07626v1"
        ]
      }
    },
    {
      "id": "7ff0afac",
      "concept": "convex formulation",
      "definition": "Analysis of over-parameterized neural networks has drawn significant attention in recentyears",
      "context": "Paper: Convex Formulation of Overparameterized Deep Neural Networks\nAuthors: Cong Fang, Yihong Gu, Weizhong Zhang\nCategories: cs.LG, stat.ML\nAbstract: Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundam...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.450039",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.07626v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1911.07626v1"
        ]
      }
    },
    {
      "id": "8f208656",
      "concept": "features",
      "definition": "It is shown that under suitable representations, overparameterized deep neural networks are inherently convex, and when optimized, the system can learn effective features suitable for the underlying learning task under mild conditions",
      "context": "Paper: Convex Formulation of Overparameterized Deep Neural Networks\nAuthors: Cong Fang, Yihong Gu, Weizhong Zhang\nCategories: cs.LG, stat.ML\nAbstract: Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundam...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.450083",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.07626v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1911.07626v1"
        ]
      }
    },
    {
      "id": "de82cb77",
      "concept": "this paper",
      "definition": "This paper solves this fundamental problem by investigating such overparameterizeddeep neural networks when fully trained",
      "context": "Paper: Convex Formulation of Overparameterized Deep Neural Networks\nAuthors: Cong Fang, Yihong Gu, Weizhong Zhang\nCategories: cs.LG, stat.ML\nAbstract: Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundam...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.450124",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.07626v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1911.07626v1"
        ]
      }
    },
    {
      "id": "8ce79c68",
      "concept": "encountered inpractice",
      "definition": "However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice",
      "context": "Paper: Convex Formulation of Overparameterized Deep Neural Networks\nAuthors: Cong Fang, Yihong Gu, Weizhong Zhang\nCategories: cs.LG, stat.ML\nAbstract: Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundam...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.450162",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1911.07626v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1911.07626v1"
        ]
      }
    },
    {
      "id": "79fe24d3",
      "concept": "application",
      "definition": "In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks",
      "context": "Paper: Approximate Bisimulation Relations for Neural Networks and Application   to Assured Neural Network Compression\nAuthors: Weiming Xiang, Zhongzhu Shao\nCategories: cs.LG, cs.AI, cs.SY, eess.SY\nAbstract: In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.555456",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.01214v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2202.01214v1"
        ]
      }
    },
    {
      "id": "73599210",
      "concept": "relation",
      "definition": "In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks",
      "context": "Paper: Approximate Bisimulation Relations for Neural Networks and Application   to Assured Neural Network Compression\nAuthors: Weiming Xiang, Zhongzhu Shao\nCategories: cs.LG, cs.AI, cs.SY, eess.SY\nAbstract: In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.555518",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.01214v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2202.01214v1"
        ]
      }
    },
    {
      "id": "17e78bfe",
      "concept": "perform neural",
      "definition": "Then, we apply the approximate bisimulation relation results to perform neural networks model reduction and compute the compression precision, i",
      "context": "Paper: Approximate Bisimulation Relations for Neural Networks and Application   to Assured Neural Network Compression\nAuthors: Weiming Xiang, Zhongzhu Shao\nCategories: cs.LG, cs.AI, cs.SY, eess.SY\nAbstract: In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.555558",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.01214v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.01214v1"
        ]
      }
    },
    {
      "id": "167d3d44",
      "concept": "precision",
      "definition": "Then, we apply the approximate bisimulation relation results to perform neural networks model reduction and compute the compression precision, i",
      "context": "Paper: Approximate Bisimulation Relations for Neural Networks and Application   to Assured Neural Network Compression\nAuthors: Weiming Xiang, Zhongzhu Shao\nCategories: cs.LG, cs.AI, cs.SY, eess.SY\nAbstract: In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.555597",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.01214v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.01214v1"
        ]
      }
    },
    {
      "id": "09cc9d07",
      "concept": "the distance",
      "definition": "The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs",
      "context": "Paper: Approximate Bisimulation Relations for Neural Networks and Application   to Assured Neural Network Compression\nAuthors: Weiming Xiang, Zhongzhu Shao\nCategories: cs.LG, cs.AI, cs.SY, eess.SY\nAbstract: In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.555633",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.01214v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.01214v1"
        ]
      }
    },
    {
      "id": "df97e1c1",
      "concept": "concept",
      "definition": "In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks",
      "context": "Paper: Approximate Bisimulation Relations for Neural Networks and Application   to Assured Neural Network Compression\nAuthors: Weiming Xiang, Zhongzhu Shao\nCategories: cs.LG, cs.AI, cs.SY, eess.SY\nAbstract: In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.555666",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.01214v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.01214v1"
        ]
      }
    },
    {
      "id": "4ea7d7b6",
      "concept": "relation for",
      "definition": "In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks",
      "context": "Paper: Approximate Bisimulation Relations for Neural Networks and Application   to Assured Neural Network Compression\nAuthors: Weiming Xiang, Zhongzhu Shao\nCategories: cs.LG, cs.AI, cs.SY, eess.SY\nAbstract: In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.555699",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.01214v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.01214v1"
        ]
      }
    },
    {
      "id": "313ce712",
      "concept": "network",
      "definition": "In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks",
      "context": "Paper: Approximate Bisimulation Relations for Neural Networks and Application   to Assured Neural Network Compression\nAuthors: Weiming Xiang, Zhongzhu Shao\nCategories: cs.LG, cs.AI, cs.SY, eess.SY\nAbstract: In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.555732",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.01214v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2202.01214v1"
        ]
      }
    },
    {
      "id": "40fb462e",
      "concept": "distance",
      "definition": "The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs",
      "context": "Paper: Approximate Bisimulation Relations for Neural Networks and Application   to Assured Neural Network Compression\nAuthors: Weiming Xiang, Zhongzhu Shao\nCategories: cs.LG, cs.AI, cs.SY, eess.SY\nAbstract: In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.555766",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.01214v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.01214v1"
        ]
      }
    },
    {
      "id": "1e494408",
      "concept": "bisimulation",
      "definition": "In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks",
      "context": "Paper: Approximate Bisimulation Relations for Neural Networks and Application   to Assured Neural Network Compression\nAuthors: Weiming Xiang, Zhongzhu Shao\nCategories: cs.LG, cs.AI, cs.SY, eess.SY\nAbstract: In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.555799",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.01214v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2202.01214v1"
        ]
      }
    },
    {
      "id": "d2db35d2",
      "concept": "convergence",
      "definition": "As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN",
      "context": "Paper: Optimal rates of approximation by shallow ReLU$^k$ neural networks and   applications to nonparametric regression\nAuthors: Yunfei Yang, Ding-Xuan Zhou\nCategories: stat.ML, cs.LG\nAbstract: We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these r...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.657776",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.01561v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.01561v3"
        ]
      }
    },
    {
      "id": "c7aae0ca",
      "concept": "for nonparametric",
      "definition": "As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN",
      "context": "Paper: Optimal rates of approximation by shallow ReLU$^k$ neural networks and   applications to nonparametric regression\nAuthors: Yunfei Yang, Ding-Xuan Zhou\nCategories: stat.ML, cs.LG\nAbstract: We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these r...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.657841",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.01561v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.01561v3"
        ]
      }
    },
    {
      "id": "3e7ec7f2",
      "concept": "spaces corresponding",
      "definition": "We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks",
      "context": "Paper: Optimal rates of approximation by shallow ReLU$^k$ neural networks and   applications to nonparametric regression\nAuthors: Yunfei Yang, Ding-Xuan Zhou\nCategories: stat.ML, cs.LG\nAbstract: We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these r...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.657880",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.01561v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.01561v3"
        ]
      }
    },
    {
      "id": "1c6e6b6f",
      "concept": "shown that",
      "definition": "It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms",
      "context": "Paper: Optimal rates of approximation by shallow ReLU$^k$ neural networks and   applications to nonparametric regression\nAuthors: Yunfei Yang, Ding-Xuan Zhou\nCategories: stat.ML, cs.LG\nAbstract: We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these r...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.657921",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.01561v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.01561v3"
        ]
      }
    },
    {
      "id": "4fb3fa7a",
      "concept": "using these",
      "definition": "Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks",
      "context": "Paper: Optimal rates of approximation by shallow ReLU$^k$ neural networks and   applications to nonparametric regression\nAuthors: Yunfei Yang, Ding-Xuan Zhou\nCategories: stat.ML, cs.LG\nAbstract: We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these r...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.657959",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.01561v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.01561v3"
        ]
      }
    },
    {
      "id": "609a1242",
      "concept": "over",
      "definition": "As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN",
      "context": "Paper: Optimal rates of approximation by shallow ReLU$^k$ neural networks and   applications to nonparametric regression\nAuthors: Yunfei Yang, Ding-Xuan Zhou\nCategories: stat.ML, cs.LG\nAbstract: We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these r...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.658001",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.01561v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.01561v3"
        ]
      }
    },
    {
      "id": "2ca98e94",
      "concept": "three relu",
      "definition": "As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN",
      "context": "Paper: Optimal rates of approximation by shallow ReLU$^k$ neural networks and   applications to nonparametric regression\nAuthors: Yunfei Yang, Ding-Xuan Zhou\nCategories: stat.ML, cs.LG\nAbstract: We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these r...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.658042",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.01561v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.01561v3"
        ]
      }
    },
    {
      "id": "a1e30736",
      "concept": "nonparametric",
      "definition": "As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN",
      "context": "Paper: Optimal rates of approximation by shallow ReLU$^k$ neural networks and   applications to nonparametric regression\nAuthors: Yunfei Yang, Ding-Xuan Zhou\nCategories: stat.ML, cs.LG\nAbstract: We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these r...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.658082",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.01561v3",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2304.01561v3"
        ]
      }
    },
    {
      "id": "b545a808",
      "concept": "established",
      "definition": "For functions with less smoothness, the approximation rates in terms of the variation norm are established",
      "context": "Paper: Optimal rates of approximation by shallow ReLU$^k$ neural networks and   applications to nonparametric regression\nAuthors: Yunfei Yang, Ding-Xuan Zhou\nCategories: stat.ML, cs.LG\nAbstract: We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these r...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.658118",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.01561v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2304.01561v3"
        ]
      }
    },
    {
      "id": "cd150ff3",
      "concept": "shallow",
      "definition": "We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks",
      "context": "Paper: Optimal rates of approximation by shallow ReLU$^k$ neural networks and   applications to nonparametric regression\nAuthors: Yunfei Yang, Ding-Xuan Zhou\nCategories: stat.ML, cs.LG\nAbstract: We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these r...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.658152",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.01561v3",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2304.01561v3"
        ]
      }
    },
    {
      "id": "68bafc1a",
      "concept": "can be",
      "definition": "Precisely, a vector-valued neural network can be obtained by placing restrictions on a real-valued model to consider the intercorrelation between feature channels",
      "context": "Paper: Understanding Vector-Valued Neural Networks and Their Relationship with   Real and Hypercomplex-Valued Neural Networks\nAuthors: Marcos Eduardo Valle\nCategories: cs.LG, cs.NE\nAbstract: Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrela...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.760059",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2309.07716v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2309.07716v2"
        ]
      }
    },
    {
      "id": "e2825bce",
      "concept": "furthermore",
      "definition": "Furthermore, this paper explains the relationship between vector-valued and traditional neural networks",
      "context": "Paper: Understanding Vector-Valued Neural Networks and Their Relationship with   Real and Hypercomplex-Valued Neural Networks\nAuthors: Marcos Eduardo Valle\nCategories: cs.LG, cs.NE\nAbstract: Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrela...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.760197",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2309.07716v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2309.07716v2"
        ]
      }
    },
    {
      "id": "b9ec6244",
      "concept": "explains",
      "definition": "Furthermore, this paper explains the relationship between vector-valued and traditional neural networks",
      "context": "Paper: Understanding Vector-Valued Neural Networks and Their Relationship with   Real and Hypercomplex-Valued Neural Networks\nAuthors: Marcos Eduardo Valle\nCategories: cs.LG, cs.NE\nAbstract: Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrela...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.760258",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2309.07716v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2309.07716v2"
        ]
      }
    },
    {
      "id": "8a1dce17",
      "concept": "intercorrelation",
      "definition": "The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training",
      "context": "Paper: Understanding Vector-Valued Neural Networks and Their Relationship with   Real and Hypercomplex-Valued Neural Networks\nAuthors: Marcos Eduardo Valle\nCategories: cs.LG, cs.NE\nAbstract: Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrela...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.760298",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2309.07716v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2309.07716v2"
        ]
      }
    },
    {
      "id": "01ec9078",
      "concept": "than",
      "definition": "Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks",
      "context": "Paper: Understanding Vector-Valued Neural Networks and Their Relationship with   Real and Hypercomplex-Valued Neural Networks\nAuthors: Marcos Eduardo Valle\nCategories: cs.LG, cs.NE\nAbstract: Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrela...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.760337",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2309.07716v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2309.07716v2"
        ]
      }
    },
    {
      "id": "d0b6a1bf",
      "concept": "they",
      "definition": "Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks",
      "context": "Paper: Understanding Vector-Valued Neural Networks and Their Relationship with   Real and Hypercomplex-Valued Neural Networks\nAuthors: Marcos Eduardo Valle\nCategories: cs.LG, cs.NE\nAbstract: Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrela...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.760411",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2309.07716v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2309.07716v2"
        ]
      }
    },
    {
      "id": "e32c0560",
      "concept": "numerous",
      "definition": "The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training",
      "context": "Paper: Understanding Vector-Valued Neural Networks and Their Relationship with   Real and Hypercomplex-Valued Neural Networks\nAuthors: Marcos Eduardo Valle\nCategories: cs.LG, cs.NE\nAbstract: Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrela...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.760452",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2309.07716v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2309.07716v2"
        ]
      }
    },
    {
      "id": "65ad67aa",
      "concept": "referred",
      "definition": "This paper aims to present a broad framework for vector-valued neural networks, referred to as V-nets",
      "context": "Paper: Understanding Vector-Valued Neural Networks and Their Relationship with   Real and Hypercomplex-Valued Neural Networks\nAuthors: Marcos Eduardo Valle\nCategories: cs.LG, cs.NE\nAbstract: Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrela...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.760490",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2309.07716v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2309.07716v2"
        ]
      }
    },
    {
      "id": "5fb240e9",
      "concept": "more",
      "definition": "Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks",
      "context": "Paper: Understanding Vector-Valued Neural Networks and Their Relationship with   Real and Hypercomplex-Valued Neural Networks\nAuthors: Marcos Eduardo Valle\nCategories: cs.LG, cs.NE\nAbstract: Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrela...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.760530",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2309.07716v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2309.07716v2"
        ]
      }
    },
    {
      "id": "b82ab873",
      "concept": "this paper",
      "definition": "This paper aims to present a broad framework for vector-valued neural networks, referred to as V-nets",
      "context": "Paper: Understanding Vector-Valued Neural Networks and Their Relationship with   Real and Hypercomplex-Valued Neural Networks\nAuthors: Marcos Eduardo Valle\nCategories: cs.LG, cs.NE\nAbstract: Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrela...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.760573",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2309.07716v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2309.07716v2"
        ]
      }
    },
    {
      "id": "f6b643d7",
      "concept": "one weird",
      "definition": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs",
      "context": "Paper: One weird trick for parallelizing convolutional neural networks\nAuthors: Alex Krizhevsky\nCategories: cs.NE, cs.DC, cs.LG\nAbstract: I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.861230",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1404.5997v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1404.5997v2"
        ]
      }
    },
    {
      "id": "a0269c02",
      "concept": "to modern",
      "definition": "The method scales significantly better than all alternatives when applied to modern convolutional neural networks",
      "context": "Paper: One weird trick for parallelizing convolutional neural networks\nAuthors: Alex Krizhevsky\nCategories: cs.NE, cs.DC, cs.LG\nAbstract: I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.861297",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1404.5997v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1404.5997v2"
        ]
      }
    },
    {
      "id": "a92ed5a9",
      "concept": "multiple",
      "definition": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs",
      "context": "Paper: One weird trick for parallelizing convolutional neural networks\nAuthors: Alex Krizhevsky\nCategories: cs.NE, cs.DC, cs.LG\nAbstract: I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.861320",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1404.5997v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1404.5997v2"
        ]
      }
    },
    {
      "id": "987bc975",
      "concept": "when applied",
      "definition": "The method scales significantly better than all alternatives when applied to modern convolutional neural networks",
      "context": "Paper: One weird trick for parallelizing convolutional neural networks\nAuthors: Alex Krizhevsky\nCategories: cs.NE, cs.DC, cs.LG\nAbstract: I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.861342",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1404.5997v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1404.5997v2"
        ]
      }
    },
    {
      "id": "43d95ca0",
      "concept": "than",
      "definition": "The method scales significantly better than all alternatives when applied to modern convolutional neural networks",
      "context": "Paper: One weird trick for parallelizing convolutional neural networks\nAuthors: Alex Krizhevsky\nCategories: cs.NE, cs.DC, cs.LG\nAbstract: I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.861390",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1404.5997v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1404.5997v2"
        ]
      }
    },
    {
      "id": "5c7332bd",
      "concept": "neural",
      "definition": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs",
      "context": "Paper: One weird trick for parallelizing convolutional neural networks\nAuthors: Alex Krizhevsky\nCategories: cs.NE, cs.DC, cs.LG\nAbstract: I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.861414",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1404.5997v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1404.5997v2"
        ]
      }
    },
    {
      "id": "b1e34163",
      "concept": "networks",
      "definition": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs",
      "context": "Paper: One weird trick for parallelizing convolutional neural networks\nAuthors: Alex Krizhevsky\nCategories: cs.NE, cs.DC, cs.LG\nAbstract: I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.861435",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1404.5997v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1404.5997v2"
        ]
      }
    },
    {
      "id": "25068f32",
      "concept": "gpus",
      "definition": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs",
      "context": "Paper: One weird trick for parallelizing convolutional neural networks\nAuthors: Alex Krizhevsky\nCategories: cs.NE, cs.DC, cs.LG\nAbstract: I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.861457",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1404.5997v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1404.5997v2"
        ]
      }
    },
    {
      "id": "4505b518",
      "concept": "new way",
      "definition": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs",
      "context": "Paper: One weird trick for parallelizing convolutional neural networks\nAuthors: Alex Krizhevsky\nCategories: cs.NE, cs.DC, cs.LG\nAbstract: I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.861478",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1404.5997v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1404.5997v2"
        ]
      }
    },
    {
      "id": "405abeec",
      "concept": "scales significantly",
      "definition": "The method scales significantly better than all alternatives when applied to modern convolutional neural networks",
      "context": "Paper: One weird trick for parallelizing convolutional neural networks\nAuthors: Alex Krizhevsky\nCategories: cs.NE, cs.DC, cs.LG\nAbstract: I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:41.861531",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1404.5997v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1404.5997v2"
        ]
      }
    },
    {
      "id": "39d64629",
      "concept": "required",
      "definition": "Hadoop is one of the platforms that can process the large amount of data required for natural language processing",
      "context": "Paper: Natural Language Processing using Hadoop and KOSHIK\nAuthors: Emre Erturk, Hong Shi\nCategories: cs.CL\nAbstract: Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.456409",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1608.04434v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1608.04434v1"
        ]
      }
    },
    {
      "id": "636ec8ea",
      "concept": "research",
      "definition": "Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation",
      "context": "Paper: Natural Language Processing using Hadoop and KOSHIK\nAuthors: Emre Erturk, Hong Shi\nCategories: cs.CL\nAbstract: Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.456533",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1608.04434v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1608.04434v1"
        ]
      }
    },
    {
      "id": "caa92def",
      "concept": "analyze",
      "definition": "This study describes how to build a KOSHIK platform with the relevant tools, and provides the steps to analyze wiki data",
      "context": "Paper: Natural Language Processing using Hadoop and KOSHIK\nAuthors: Emre Erturk, Hong Shi\nCategories: cs.CL\nAbstract: Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.456576",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1608.04434v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1608.04434v1"
        ]
      }
    },
    {
      "id": "f1eba0c7",
      "concept": "recommendations",
      "definition": "Finally, it evaluates and discusses the advantages and disadvantages of the KOSHIK architecture, and gives recommendations on improving the processing performance",
      "context": "Paper: Natural Language Processing using Hadoop and KOSHIK\nAuthors: Emre Erturk, Hong Shi\nCategories: cs.CL\nAbstract: Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.456613",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1608.04434v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1608.04434v1"
        ]
      }
    },
    {
      "id": "04fead55",
      "concept": "processing using",
      "definition": "Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation",
      "context": "Paper: Natural Language Processing using Hadoop and KOSHIK\nAuthors: Emre Erturk, Hong Shi\nCategories: cs.CL\nAbstract: Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.456651",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1608.04434v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1608.04434v1"
        ]
      }
    },
    {
      "id": "a38b0821",
      "concept": "components",
      "definition": "KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains language processing components such as Stanford CoreNLP and OpenNLP",
      "context": "Paper: Natural Language Processing using Hadoop and KOSHIK\nAuthors: Emre Erturk, Hong Shi\nCategories: cs.CL\nAbstract: Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.456686",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1608.04434v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1608.04434v1"
        ]
      }
    },
    {
      "id": "79ff4954",
      "concept": "of the",
      "definition": "Hadoop is one of the platforms that can process the large amount of data required for natural language processing",
      "context": "Paper: Natural Language Processing using Hadoop and KOSHIK\nAuthors: Emre Erturk, Hong Shi\nCategories: cs.CL\nAbstract: Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.456722",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1608.04434v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1608.04434v1"
        ]
      }
    },
    {
      "id": "cec30013",
      "concept": "and opennlp",
      "definition": "KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains language processing components such as Stanford CoreNLP and OpenNLP",
      "context": "Paper: Natural Language Processing using Hadoop and KOSHIK\nAuthors: Emre Erturk, Hong Shi\nCategories: cs.CL\nAbstract: Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.456756",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1608.04434v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1608.04434v1"
        ]
      }
    },
    {
      "id": "5763284e",
      "concept": "language",
      "definition": "Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation",
      "context": "Paper: Natural Language Processing using Hadoop and KOSHIK\nAuthors: Emre Erturk, Hong Shi\nCategories: cs.CL\nAbstract: Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.456789",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1608.04434v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1608.04434v1"
        ]
      }
    },
    {
      "id": "a096586e",
      "concept": "describes how",
      "definition": "This study describes how to build a KOSHIK platform with the relevant tools, and provides the steps to analyze wiki data",
      "context": "Paper: Natural Language Processing using Hadoop and KOSHIK\nAuthors: Emre Erturk, Hong Shi\nCategories: cs.CL\nAbstract: Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.456826",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1608.04434v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1608.04434v1"
        ]
      }
    },
    {
      "id": "404e0887",
      "concept": "are utilized",
      "definition": "Traditional and data-driven methods are utilized in the preprocessing stage",
      "context": "Paper: NLI4DB: A Systematic Review of Natural Language Interfaces for Databases\nAuthors: Mengyi Liu, Jianqiu Xu\nCategories: cs.DB\nAbstract: As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation proce...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.578531",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.02435v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2503.02435v1"
        ]
      }
    },
    {
      "id": "08ba6e08",
      "concept": "rule",
      "definition": "Traditional approaches rely on predefined rules and grammars, and involve techniques such as regular expressions, dependency parsing and named entity recognition",
      "context": "Paper: NLI4DB: A Systematic Review of Natural Language Interfaces for Databases\nAuthors: Mengyi Liu, Jianqiu Xu\nCategories: cs.DB\nAbstract: As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation proce...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.580814",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.02435v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2503.02435v1"
        ]
      }
    },
    {
      "id": "5b87554b",
      "concept": "systems",
      "definition": "Finally, we summarize the classification, development, and enhancement of NLIDB systems, and discuss deep language understanding and database interaction techniques related to NLIDB, including (i) using LLM for Text2SQL tasks, (ii) generating natural language interpretations from SQL, and (iii) transforming speech queries into SQL",
      "context": "Paper: NLI4DB: A Systematic Review of Natural Language Interfaces for Databases\nAuthors: Mengyi Liu, Jianqiu Xu\nCategories: cs.DB\nAbstract: As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation proce...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.581114",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.02435v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2503.02435v1"
        ]
      }
    },
    {
      "id": "ba46228e",
      "concept": "devoted",
      "definition": "As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB)",
      "context": "Paper: NLI4DB: A Systematic Review of Natural Language Interfaces for Databases\nAuthors: Mengyi Liu, Jianqiu Xu\nCategories: cs.DB\nAbstract: As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation proce...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.581288",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.02435v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2503.02435v1"
        ]
      }
    },
    {
      "id": "c0177ce3",
      "concept": "understanding and",
      "definition": "Finally, we summarize the classification, development, and enhancement of NLIDB systems, and discuss deep language understanding and database interaction techniques related to NLIDB, including (i) using LLM for Text2SQL tasks, (ii) generating natural language interpretations from SQL, and (iii) transforming speech queries into SQL",
      "context": "Paper: NLI4DB: A Systematic Review of Natural Language Interfaces for Databases\nAuthors: Mengyi Liu, Jianqiu Xu\nCategories: cs.DB\nAbstract: As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation proce...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.581506",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.02435v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2503.02435v1"
        ]
      }
    },
    {
      "id": "37279354",
      "concept": "entity recognition",
      "definition": "Traditional approaches rely on predefined rules and grammars, and involve techniques such as regular expressions, dependency parsing and named entity recognition",
      "context": "Paper: NLI4DB: A Systematic Review of Natural Language Interfaces for Databases\nAuthors: Mengyi Liu, Jianqiu Xu\nCategories: cs.DB\nAbstract: As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation proce...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.581693",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.02435v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2503.02435v1"
        ]
      }
    },
    {
      "id": "2dff8b9a",
      "concept": "depend on",
      "definition": "Data-driven approaches depend on large-scale data and machine learning models, using techniques including word embedding and pattern linking",
      "context": "Paper: NLI4DB: A Systematic Review of Natural Language Interfaces for Databases\nAuthors: Mengyi Liu, Jianqiu Xu\nCategories: cs.DB\nAbstract: As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation proce...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.581871",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.02435v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2503.02435v1"
        ]
      }
    },
    {
      "id": "b5451537",
      "concept": "recognition",
      "definition": "Traditional approaches rely on predefined rules and grammars, and involve techniques such as regular expressions, dependency parsing and named entity recognition",
      "context": "Paper: NLI4DB: A Systematic Review of Natural Language Interfaces for Databases\nAuthors: Mengyi Liu, Jianqiu Xu\nCategories: cs.DB\nAbstract: As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation proce...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.582036",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.02435v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2503.02435v1"
        ]
      }
    },
    {
      "id": "c77a4337",
      "concept": "over",
      "definition": "We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation process from natural language to executable database language",
      "context": "Paper: NLI4DB: A Systematic Review of Natural Language Interfaces for Databases\nAuthors: Mengyi Liu, Jianqiu Xu\nCategories: cs.DB\nAbstract: As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation proce...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.588402",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.02435v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2503.02435v1"
        ]
      }
    },
    {
      "id": "137bc357",
      "concept": "machine",
      "definition": "Data-driven approaches depend on large-scale data and machine learning models, using techniques including word embedding and pattern linking",
      "context": "Paper: NLI4DB: A Systematic Review of Natural Language Interfaces for Databases\nAuthors: Mengyi Liu, Jianqiu Xu\nCategories: cs.DB\nAbstract: As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation proce...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.588770",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.02435v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2503.02435v1"
        ]
      }
    },
    {
      "id": "67a2ddc2",
      "concept": "preconditions",
      "definition": ", preconditions and effects of action models, and learn from tacit knowledge, e",
      "context": "Paper: Integrating AI Planning with Natural Language Processing: A Combination   of Explicit and Tacit Knowledge\nAuthors: Kebing Jin, Hankz Hankui Zhuo\nCategories: cs.AI, cs.CL\nAbstract: Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.708346",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.07138v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.07138v2"
        ]
      }
    },
    {
      "id": "10f0102e",
      "concept": "introduce",
      "definition": "One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning",
      "context": "Paper: Integrating AI Planning with Natural Language Processing: A Combination   of Explicit and Tacit Knowledge\nAuthors: Kebing Jin, Hankz Hankui Zhuo\nCategories: cs.AI, cs.CL\nAbstract: Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.708566",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.07138v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.07138v2"
        ]
      }
    },
    {
      "id": "cb1eb5ae",
      "concept": "the challenges",
      "definition": "However, the challenges of explainability and complexity come along with the developments of language models",
      "context": "Paper: Integrating AI Planning with Natural Language Processing: A Combination   of Explicit and Tacit Knowledge\nAuthors: Kebing Jin, Hankz Hankui Zhuo\nCategories: cs.AI, cs.CL\nAbstract: Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.708708",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.07138v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.07138v2"
        ]
      }
    },
    {
      "id": "e3df7224",
      "concept": "respectively",
      "definition": ", neural models, respectively",
      "context": "Paper: Integrating AI Planning with Natural Language Processing: A Combination   of Explicit and Tacit Knowledge\nAuthors: Kebing Jin, Hankz Hankui Zhuo\nCategories: cs.AI, cs.CL\nAbstract: Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.709015",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.07138v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.07138v2"
        ]
      }
    },
    {
      "id": "a3344536",
      "concept": "analyzing",
      "definition": "Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data",
      "context": "Paper: Integrating AI Planning with Natural Language Processing: A Combination   of Explicit and Tacit Knowledge\nAuthors: Kebing Jin, Hankz Hankui Zhuo\nCategories: cs.AI, cs.CL\nAbstract: Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.709171",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.07138v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.07138v2"
        ]
      }
    },
    {
      "id": "feb92a0f",
      "concept": "first work",
      "definition": "To the best of our knowledge, this survey is the first work that addresses the deep connections between AI planning and Natural language processing",
      "context": "Paper: Integrating AI Planning with Natural Language Processing: A Combination   of Explicit and Tacit Knowledge\nAuthors: Kebing Jin, Hankz Hankui Zhuo\nCategories: cs.AI, cs.CL\nAbstract: Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.709376",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.07138v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.07138v2"
        ]
      }
    },
    {
      "id": "ac9d3813",
      "concept": "commons and",
      "definition": "This paper outlines the commons and relations between AI planning and natural language processing, argues that each of them can effectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based natural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and (5) applications",
      "context": "Paper: Integrating AI Planning with Natural Language Processing: A Combination   of Explicit and Tacit Knowledge\nAuthors: Kebing Jin, Hankz Hankui Zhuo\nCategories: cs.AI, cs.CL\nAbstract: Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.709552",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.07138v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.07138v2"
        ]
      }
    },
    {
      "id": "2ef27601",
      "concept": "argues that",
      "definition": "This paper outlines the commons and relations between AI planning and natural language processing, argues that each of them can effectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based natural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and (5) applications",
      "context": "Paper: Integrating AI Planning with Natural Language Processing: A Combination   of Explicit and Tacit Knowledge\nAuthors: Kebing Jin, Hankz Hankui Zhuo\nCategories: cs.AI, cs.CL\nAbstract: Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.709708",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.07138v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.07138v2"
        ]
      }
    },
    {
      "id": "9881b0bb",
      "concept": "this paper",
      "definition": "This paper outlines the commons and relations between AI planning and natural language processing, argues that each of them can effectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based natural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and (5) applications",
      "context": "Paper: Integrating AI Planning with Natural Language Processing: A Combination   of Explicit and Tacit Knowledge\nAuthors: Kebing Jin, Hankz Hankui Zhuo\nCategories: cs.AI, cs.CL\nAbstract: Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.709866",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.07138v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2202.07138v2"
        ]
      }
    },
    {
      "id": "895ef5cf",
      "concept": "language",
      "definition": "Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data",
      "context": "Paper: Integrating AI Planning with Natural Language Processing: A Combination   of Explicit and Tacit Knowledge\nAuthors: Kebing Jin, Hankz Hankui Zhuo\nCategories: cs.AI, cs.CL\nAbstract: Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.709994",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2202.07138v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2202.07138v2"
        ]
      }
    },
    {
      "id": "138c7df0",
      "concept": "note describes",
      "definition": "This technical note describes a set of baseline tools for automatic processing of Danish text",
      "context": "Paper: Simple Natural Language Processing Tools for Danish\nAuthors: Leon Derczynski\nCategories: cs.CL\nAbstract: This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.812404",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1906.11608v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1906.11608v2"
        ]
      }
    },
    {
      "id": "50ef48e7",
      "concept": "copenhagen",
      "definition": "They are maintained at ITU Copenhagen and will always be freely available",
      "context": "Paper: Simple Natural Language Processing Tools for Danish\nAuthors: Leon Derczynski\nCategories: cs.CL\nAbstract: This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.812497",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1906.11608v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1906.11608v2"
        ]
      }
    },
    {
      "id": "accc5cc4",
      "concept": "and will",
      "definition": "They are maintained at ITU Copenhagen and will always be freely available",
      "context": "Paper: Simple Natural Language Processing Tools for Danish\nAuthors: Leon Derczynski\nCategories: cs.CL\nAbstract: This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.812543",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1906.11608v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1906.11608v2"
        ]
      }
    },
    {
      "id": "03e66adb",
      "concept": "over",
      "definition": "The tools are machine-learning based, using natural language processing models trained over previously annotated documents",
      "context": "Paper: Simple Natural Language Processing Tools for Danish\nAuthors: Leon Derczynski\nCategories: cs.CL\nAbstract: This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.812584",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1906.11608v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1906.11608v2"
        ]
      }
    },
    {
      "id": "b8691112",
      "concept": "freely available",
      "definition": "They are maintained at ITU Copenhagen and will always be freely available",
      "context": "Paper: Simple Natural Language Processing Tools for Danish\nAuthors: Leon Derczynski\nCategories: cs.CL\nAbstract: This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.812625",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1906.11608v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1906.11608v2"
        ]
      }
    },
    {
      "id": "04b84b05",
      "concept": "machine",
      "definition": "The tools are machine-learning based, using natural language processing models trained over previously annotated documents",
      "context": "Paper: Simple Natural Language Processing Tools for Danish\nAuthors: Leon Derczynski\nCategories: cs.CL\nAbstract: This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.812666",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1906.11608v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1906.11608v2"
        ]
      }
    },
    {
      "id": "5acb2fb8",
      "concept": "set of",
      "definition": "This technical note describes a set of baseline tools for automatic processing of Danish text",
      "context": "Paper: Simple Natural Language Processing Tools for Danish\nAuthors: Leon Derczynski\nCategories: cs.CL\nAbstract: This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.812707",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1906.11608v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1906.11608v2"
        ]
      }
    },
    {
      "id": "5804b202",
      "concept": "text",
      "definition": "This technical note describes a set of baseline tools for automatic processing of Danish text",
      "context": "Paper: Simple Natural Language Processing Tools for Danish\nAuthors: Leon Derczynski\nCategories: cs.CL\nAbstract: This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.812758",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1906.11608v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1906.11608v2"
        ]
      }
    },
    {
      "id": "27f2c100",
      "concept": "they",
      "definition": "They are maintained at ITU Copenhagen and will always be freely available",
      "context": "Paper: Simple Natural Language Processing Tools for Danish\nAuthors: Leon Derczynski\nCategories: cs.CL\nAbstract: This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.812830",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1906.11608v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1906.11608v2"
        ]
      }
    },
    {
      "id": "3f2b5ea5",
      "concept": "language",
      "definition": "The tools are machine-learning based, using natural language processing models trained over previously annotated documents",
      "context": "Paper: Simple Natural Language Processing Tools for Danish\nAuthors: Leon Derczynski\nCategories: cs.CL\nAbstract: This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.812993",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1906.11608v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1906.11608v2"
        ]
      }
    },
    {
      "id": "a1c210b8",
      "concept": "is no",
      "definition": "Some NLG researchers exclude MT from their definition of the field, since there is no content selection involved where the system has to determine what to say",
      "context": "Paper: Natural Language Generation\nAuthors: Emiel van Miltenburg, Chenghua Lin\nCategories: cs.CL\nAbstract: This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Lang...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.914449",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.16728v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2503.16728v2"
        ]
      }
    },
    {
      "id": "e085ea17",
      "concept": "systems",
      "definition": "The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language",
      "context": "Paper: Natural Language Generation\nAuthors: Emiel van Miltenburg, Chenghua Lin\nCategories: cs.CL\nAbstract: This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Lang...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.914566",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.16728v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2503.16728v2"
        ]
      }
    },
    {
      "id": "e6ec73be",
      "concept": "fall",
      "definition": "Conversely, dialog systems do not typically fall under the header of Natural Language Generation since NLG is just one component of dialog systems (the others being Natural Language Understanding and Dialog Management)",
      "context": "Paper: Natural Language Generation\nAuthors: Emiel van Miltenburg, Chenghua Lin\nCategories: cs.CL\nAbstract: This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Lang...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.914621",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.16728v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2503.16728v2"
        ]
      }
    },
    {
      "id": "d81c4c9c",
      "concept": "different subfields",
      "definition": "However, with the rise of Large Language Models (LLMs), different subfields of Natural Language Processing have converged on similar methodologies for the production of natural language and the evaluation of automatically generated text",
      "context": "Paper: Natural Language Generation\nAuthors: Emiel van Miltenburg, Chenghua Lin\nCategories: cs.CL\nAbstract: This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Lang...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.914671",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.16728v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2503.16728v2"
        ]
      }
    },
    {
      "id": "ff6c21e0",
      "concept": "language and",
      "definition": "However, with the rise of Large Language Models (LLMs), different subfields of Natural Language Processing have converged on similar methodologies for the production of natural language and the evaluation of automatically generated text",
      "context": "Paper: Natural Language Generation\nAuthors: Emiel van Miltenburg, Chenghua Lin\nCategories: cs.CL\nAbstract: This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Lang...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.914720",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.16728v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2503.16728v2"
        ]
      }
    },
    {
      "id": "26372c99",
      "concept": "of systems",
      "definition": "The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language",
      "context": "Paper: Natural Language Generation\nAuthors: Emiel van Miltenburg, Chenghua Lin\nCategories: cs.CL\nAbstract: This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Lang...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.914775",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.16728v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2503.16728v2"
        ]
      }
    },
    {
      "id": "e16d5707",
      "concept": "stored",
      "definition": "That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example",
      "context": "Paper: Natural Language Generation\nAuthors: Emiel van Miltenburg, Chenghua Lin\nCategories: cs.CL\nAbstract: This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Lang...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.914821",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.16728v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2503.16728v2"
        ]
      }
    },
    {
      "id": "670054b1",
      "concept": "to say",
      "definition": "Some NLG researchers exclude MT from their definition of the field, since there is no content selection involved where the system has to determine what to say",
      "context": "Paper: Natural Language Generation\nAuthors: Emiel van Miltenburg, Chenghua Lin\nCategories: cs.CL\nAbstract: This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Lang...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.914884",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.16728v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2503.16728v2"
        ]
      }
    },
    {
      "id": "6dc0ca79",
      "concept": "machine",
      "definition": "As a subfield of Natural Language Processing, NLG is closely related to other sub-disciplines such as Machine Translation (MT) and Dialog Systems",
      "context": "Paper: Natural Language Generation\nAuthors: Emiel van Miltenburg, Chenghua Lin\nCategories: cs.CL\nAbstract: This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Lang...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.914930",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.16728v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2503.16728v2"
        ]
      }
    },
    {
      "id": "ba4be1f8",
      "concept": "under the",
      "definition": "Conversely, dialog systems do not typically fall under the header of Natural Language Generation since NLG is just one component of dialog systems (the others being Natural Language Understanding and Dialog Management)",
      "context": "Paper: Natural Language Generation\nAuthors: Emiel van Miltenburg, Chenghua Lin\nCategories: cs.CL\nAbstract: This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Lang...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:43.914983",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2503.16728v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2503.16728v2"
        ]
      }
    },
    {
      "id": "813ef624",
      "concept": "no or",
      "definition": "There is no or little work on natural language processing of Tangkhul language",
      "context": "Paper: Towards the Study of Morphological Processing of the Tangkhul Language\nAuthors: Mirinso Shadang, Navanath Saharia, Thoudam Doren Singh\nCategories: cs.CL\nAbstract: There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.016180",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.16212v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2006.16212v1"
        ]
      }
    },
    {
      "id": "957b1318",
      "concept": "the current",
      "definition": "The current work is a humble beginning of morphological processing of this language using an unsupervised approach",
      "context": "Paper: Towards the Study of Morphological Processing of the Tangkhul Language\nAuthors: Mirinso Shadang, Navanath Saharia, Thoudam Doren Singh\nCategories: cs.CL\nAbstract: There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.016337",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.16212v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2006.16212v1"
        ]
      }
    },
    {
      "id": "0c65aa21",
      "concept": "of the",
      "definition": "There is no or little work on natural language processing of Tangkhul language",
      "context": "Paper: Towards the Study of Morphological Processing of the Tangkhul Language\nAuthors: Mirinso Shadang, Navanath Saharia, Thoudam Doren Singh\nCategories: cs.CL\nAbstract: There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.016430",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.16212v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2006.16212v1"
        ]
      }
    },
    {
      "id": "599b2b7d",
      "concept": "output despite",
      "definition": "Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus",
      "context": "Paper: Towards the Study of Morphological Processing of the Tangkhul Language\nAuthors: Mirinso Shadang, Navanath Saharia, Thoudam Doren Singh\nCategories: cs.CL\nAbstract: There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.016501",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.16212v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2006.16212v1"
        ]
      }
    },
    {
      "id": "9608fb72",
      "concept": "language",
      "definition": "There is no or little work on natural language processing of Tangkhul language",
      "context": "Paper: Towards the Study of Morphological Processing of the Tangkhul Language\nAuthors: Mirinso Shadang, Navanath Saharia, Thoudam Doren Singh\nCategories: cs.CL\nAbstract: There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.016554",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.16212v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2006.16212v1"
        ]
      }
    },
    {
      "id": "67d77336",
      "concept": "different sources",
      "definition": "We use a small corpus collected from different sources of text books, short stories and articles of other topics",
      "context": "Paper: Towards the Study of Morphological Processing of the Tangkhul Language\nAuthors: Mirinso Shadang, Navanath Saharia, Thoudam Doren Singh\nCategories: cs.CL\nAbstract: There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.016612",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.16212v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2006.16212v1"
        ]
      }
    },
    {
      "id": "4521b73e",
      "concept": "study of",
      "definition": "There is no or little work on natural language processing of Tangkhul language",
      "context": "Paper: Towards the Study of Morphological Processing of the Tangkhul Language\nAuthors: Mirinso Shadang, Navanath Saharia, Thoudam Doren Singh\nCategories: cs.CL\nAbstract: There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.016668",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.16212v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2006.16212v1"
        ]
      }
    },
    {
      "id": "e29bd079",
      "concept": "of morphological",
      "definition": "The current work is a humble beginning of morphological processing of this language using an unsupervised approach",
      "context": "Paper: Towards the Study of Morphological Processing of the Tangkhul Language\nAuthors: Mirinso Shadang, Navanath Saharia, Thoudam Doren Singh\nCategories: cs.CL\nAbstract: There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.016722",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.16212v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2006.16212v1"
        ]
      }
    },
    {
      "id": "d974f750",
      "concept": "identification",
      "definition": "Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus",
      "context": "Paper: Towards the Study of Morphological Processing of the Tangkhul Language\nAuthors: Mirinso Shadang, Navanath Saharia, Thoudam Doren Singh\nCategories: cs.CL\nAbstract: There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.016772",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.16212v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2006.16212v1"
        ]
      }
    },
    {
      "id": "39eed378",
      "concept": "despite",
      "definition": "Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus",
      "context": "Paper: Towards the Study of Morphological Processing of the Tangkhul Language\nAuthors: Mirinso Shadang, Navanath Saharia, Thoudam Doren Singh\nCategories: cs.CL\nAbstract: There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.016825",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2006.16212v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2006.16212v1"
        ]
      }
    },
    {
      "id": "8d43542f",
      "concept": "primer on",
      "definition": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing",
      "context": "Paper: A Primer on Neural Network Models for Natural Language Processing\nAuthors: Yoav Goldberg\nCategories: cs.CL\nAbstract: Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.118372",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1510.00726v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1510.00726v1"
        ]
      }
    },
    {
      "id": "53b4e7d6",
      "concept": "bring natural",
      "definition": "This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques",
      "context": "Paper: A Primer on Neural Network Models for Natural Language Processing\nAuthors: Yoav Goldberg\nCategories: cs.CL\nAbstract: Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.118552",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1510.00726v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1510.00726v1"
        ]
      }
    },
    {
      "id": "dea3041a",
      "concept": "research",
      "definition": "This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques",
      "context": "Paper: A Primer on Neural Network Models for Natural Language Processing\nAuthors: Yoav Goldberg\nCategories: cs.CL\nAbstract: Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.118613",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1510.00726v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1510.00726v1"
        ]
      }
    },
    {
      "id": "90b00d04",
      "concept": "over",
      "definition": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing",
      "context": "Paper: A Primer on Neural Network Models for Natural Language Processing\nAuthors: Yoav Goldberg\nCategories: cs.CL\nAbstract: Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.118662",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1510.00726v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1510.00726v1"
        ]
      }
    },
    {
      "id": "3d99813e",
      "concept": "recognition",
      "definition": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing",
      "context": "Paper: A Primer on Neural Network Models for Natural Language Processing\nAuthors: Yoav Goldberg\nCategories: cs.CL\nAbstract: Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.118708",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1510.00726v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1510.00726v1"
        ]
      }
    },
    {
      "id": "495a3476",
      "concept": "state-of",
      "definition": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing",
      "context": "Paper: A Primer on Neural Network Models for Natural Language Processing\nAuthors: Yoav Goldberg\nCategories: cs.CL\nAbstract: Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.118756",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1510.00726v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1510.00726v1"
        ]
      }
    },
    {
      "id": "e3df0a8d",
      "concept": "machine",
      "definition": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing",
      "context": "Paper: A Primer on Neural Network Models for Natural Language Processing\nAuthors: Yoav Goldberg\nCategories: cs.CL\nAbstract: Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.118802",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1510.00726v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1510.00726v1"
        ]
      }
    },
    {
      "id": "e39de6ff",
      "concept": "and speech",
      "definition": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing",
      "context": "Paper: A Primer on Neural Network Models for Natural Language Processing\nAuthors: Yoav Goldberg\nCategories: cs.CL\nAbstract: Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.118839",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1510.00726v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1510.00726v1"
        ]
      }
    },
    {
      "id": "d34d2c69",
      "concept": "textual",
      "definition": "More recently, neural network models started to be applied also to textual natural language signals, again with very promising results",
      "context": "Paper: A Primer on Neural Network Models for Natural Language Processing\nAuthors: Yoav Goldberg\nCategories: cs.CL\nAbstract: Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.118878",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1510.00726v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1510.00726v1"
        ]
      }
    },
    {
      "id": "598bbf5b",
      "concept": "past few",
      "definition": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing",
      "context": "Paper: A Primer on Neural Network Models for Natural Language Processing\nAuthors: Yoav Goldberg\nCategories: cs.CL\nAbstract: Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.118915",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1510.00726v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1510.00726v1"
        ]
      }
    },
    {
      "id": "491b79e6",
      "concept": "fundamental to",
      "definition": "On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding",
      "context": "Paper: Natural Language Understanding with Distributed Representation\nAuthors: Kyunghyun Cho\nCategories: cs.CL, stat.ML\nAbstract: This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are use...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.222628",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1511.07916v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1511.07916v1"
        ]
      }
    },
    {
      "id": "58b9e1a4",
      "concept": "fall",
      "definition": "This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015",
      "context": "Paper: Natural Language Understanding with Distributed Representation\nAuthors: Kyunghyun Cho\nCategories: cs.CL, stat.ML\nAbstract: This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are use...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.222813",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1511.07916v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1511.07916v1"
        ]
      }
    },
    {
      "id": "89dcd735",
      "concept": "personally",
      "definition": "On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding",
      "context": "Paper: Natural Language Understanding with Distributed Representation\nAuthors: Kyunghyun Cho\nCategories: cs.CL, stat.ML\nAbstract: This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are use...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.222888",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1511.07916v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1511.07916v1"
        ]
      }
    },
    {
      "id": "f4fdcb8c",
      "concept": "language front",
      "definition": "On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding",
      "context": "Paper: Natural Language Understanding with Distributed Representation\nAuthors: Kyunghyun Cho\nCategories: cs.CL, stat.ML\nAbstract: This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are use...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.222955",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1511.07916v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1511.07916v1"
        ]
      }
    },
    {
      "id": "f0e8c2ff",
      "concept": "machine",
      "definition": "In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introduced",
      "context": "Paper: Natural Language Understanding with Distributed Representation\nAuthors: Kyunghyun Cho\nCategories: cs.CL, stat.ML\nAbstract: This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are use...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.223014",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1511.07916v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1511.07916v1"
        ]
      }
    },
    {
      "id": "88b73f2a",
      "concept": "they",
      "definition": "In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introduced",
      "context": "Paper: Natural Language Understanding with Distributed Representation\nAuthors: Kyunghyun Cho\nCategories: cs.CL, stat.ML\nAbstract: This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are use...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.223074",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1511.07916v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1511.07916v1"
        ]
      }
    },
    {
      "id": "c46fd598",
      "concept": "language",
      "definition": "This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015",
      "context": "Paper: Natural Language Understanding with Distributed Representation\nAuthors: Kyunghyun Cho\nCategories: cs.CL, stat.ML\nAbstract: This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are use...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.223131",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1511.07916v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1511.07916v1"
        ]
      }
    },
    {
      "id": "2df21faf",
      "concept": "lecture",
      "definition": "This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015",
      "context": "Paper: Natural Language Understanding with Distributed Representation\nAuthors: Kyunghyun Cho\nCategories: cs.CL, stat.ML\nAbstract: This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are use...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.223216",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1511.07916v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1511.07916v1"
        ]
      }
    },
    {
      "id": "c4f8ff60",
      "concept": "language modelling",
      "definition": "On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding",
      "context": "Paper: Natural Language Understanding with Distributed Representation\nAuthors: Kyunghyun Cho\nCategories: cs.CL, stat.ML\nAbstract: This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are use...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.223261",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1511.07916v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1511.07916v1"
        ]
      }
    },
    {
      "id": "fd43ee8a",
      "concept": "most fascinating",
      "definition": "On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding",
      "context": "Paper: Natural Language Understanding with Distributed Representation\nAuthors: Kyunghyun Cho\nCategories: cs.CL, stat.ML\nAbstract: This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are use...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.223318",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1511.07916v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1511.07916v1"
        ]
      }
    },
    {
      "id": "5d88992a",
      "concept": "linguists to",
      "definition": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages",
      "context": "Paper: Deploying Technology to Save Endangered Languages\nAuthors: Hilaria Cruz, Joseph Waring\nCategories: cs.CL\nAbstract: Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.324397",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.08971v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1908.08971v2"
        ]
      }
    },
    {
      "id": "5ec9d5a1",
      "concept": "native speakers",
      "definition": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages",
      "context": "Paper: Deploying Technology to Save Endangered Languages\nAuthors: Hilaria Cruz, Joseph Waring\nCategories: cs.CL\nAbstract: Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.324618",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.08971v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1908.08971v2"
        ]
      }
    },
    {
      "id": "dfbb42de",
      "concept": "technology",
      "definition": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages",
      "context": "Paper: Deploying Technology to Save Endangered Languages\nAuthors: Hilaria Cruz, Joseph Waring\nCategories: cs.CL\nAbstract: Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.324700",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.08971v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1908.08971v2"
        ]
      }
    },
    {
      "id": "c6c60fc5",
      "concept": "and text",
      "definition": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages",
      "context": "Paper: Deploying Technology to Save Endangered Languages\nAuthors: Hilaria Cruz, Joseph Waring\nCategories: cs.CL\nAbstract: Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.324766",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.08971v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1908.08971v2"
        ]
      }
    },
    {
      "id": "bf949688",
      "concept": "computer",
      "definition": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages",
      "context": "Paper: Deploying Technology to Save Endangered Languages\nAuthors: Hilaria Cruz, Joseph Waring\nCategories: cs.CL\nAbstract: Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.324823",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.08971v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1908.08971v2"
        ]
      }
    },
    {
      "id": "f74efb5c",
      "concept": "especially",
      "definition": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages",
      "context": "Paper: Deploying Technology to Save Endangered Languages\nAuthors: Hilaria Cruz, Joseph Waring\nCategories: cs.CL\nAbstract: Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.324880",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.08971v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1908.08971v2"
        ]
      }
    },
    {
      "id": "6a33c0d4",
      "concept": "native",
      "definition": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages",
      "context": "Paper: Deploying Technology to Save Endangered Languages\nAuthors: Hilaria Cruz, Joseph Waring\nCategories: cs.CL\nAbstract: Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.324935",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.08971v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1908.08971v2"
        ]
      }
    },
    {
      "id": "823e913d",
      "concept": "recognition",
      "definition": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages",
      "context": "Paper: Deploying Technology to Save Endangered Languages\nAuthors: Hilaria Cruz, Joseph Waring\nCategories: cs.CL\nAbstract: Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.325004",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.08971v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1908.08971v2"
        ]
      }
    },
    {
      "id": "27dcbbac",
      "concept": "deploying",
      "definition": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages",
      "context": "Paper: Deploying Technology to Save Endangered Languages\nAuthors: Hilaria Cruz, Joseph Waring\nCategories: cs.CL\nAbstract: Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.325069",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.08971v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1908.08971v2"
        ]
      }
    },
    {
      "id": "fba7760b",
      "concept": "text",
      "definition": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages",
      "context": "Paper: Deploying Technology to Save Endangered Languages\nAuthors: Hilaria Cruz, Joseph Waring\nCategories: cs.CL\nAbstract: Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.325177",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1908.08971v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1908.08971v2"
        ]
      }
    },
    {
      "id": "cbdccf2e",
      "concept": "processing downstream",
      "definition": "We suggest several ways of supporting undotted texts with pre-trained models, without additional training, and measure their performance on two Arabic natural-language-processing downstream tasks",
      "context": "Paper: Supporting Undotted Arabic with Pre-trained Language Models\nAuthors: Aviad Rom, Kfir Bar\nCategories: cs.CL, cs.LG\nAbstract: We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.427858",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2111.09791v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2111.09791v1"
        ]
      }
    },
    {
      "id": "9c95a279",
      "concept": "without",
      "definition": "We suggest several ways of supporting undotted texts with pre-trained models, without additional training, and measure their performance on two Arabic natural-language-processing downstream tasks",
      "context": "Paper: Supporting Undotted Arabic with Pre-trained Language Models\nAuthors: Aviad Rom, Kfir Bar\nCategories: cs.CL, cs.LG\nAbstract: We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.428012",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2111.09791v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2111.09791v1"
        ]
      }
    },
    {
      "id": "0303792e",
      "concept": "in one",
      "definition": "The results are encouraging; in one of the tasks our method shows nearly perfect performance",
      "context": "Paper: Supporting Undotted Arabic with Pre-trained Language Models\nAuthors: Aviad Rom, Kfir Bar\nCategories: cs.CL, cs.LG\nAbstract: We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.428081",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2111.09791v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2111.09791v1"
        ]
      }
    },
    {
      "id": "6a7db2ac",
      "concept": "content-classification",
      "definition": "We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms",
      "context": "Paper: Supporting Undotted Arabic with Pre-trained Language Models\nAuthors: Aviad Rom, Kfir Bar\nCategories: cs.CL, cs.LG\nAbstract: We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.428152",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2111.09791v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2111.09791v1"
        ]
      }
    },
    {
      "id": "6840b07d",
      "concept": "recent",
      "definition": "We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms",
      "context": "Paper: Supporting Undotted Arabic with Pre-trained Language Models\nAuthors: Aviad Rom, Kfir Bar\nCategories: cs.CL, cs.LG\nAbstract: We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.428201",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2111.09791v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2111.09791v1"
        ]
      }
    },
    {
      "id": "a3fa6f57",
      "concept": "of the",
      "definition": "The results are encouraging; in one of the tasks our method shows nearly perfect performance",
      "context": "Paper: Supporting Undotted Arabic with Pre-trained Language Models\nAuthors: Aviad Rom, Kfir Bar\nCategories: cs.CL, cs.LG\nAbstract: We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.428373",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2111.09791v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2111.09791v1"
        ]
      }
    },
    {
      "id": "30012412",
      "concept": "fine",
      "definition": "Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications",
      "context": "Paper: Supporting Undotted Arabic with Pre-trained Language Models\nAuthors: Aviad Rom, Kfir Bar\nCategories: cs.CL, cs.LG\nAbstract: We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.428502",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2111.09791v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2111.09791v1"
        ]
      }
    },
    {
      "id": "a3525187",
      "concept": "language",
      "definition": "Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications",
      "context": "Paper: Supporting Undotted Arabic with Pre-trained Language Models\nAuthors: Aviad Rom, Kfir Bar\nCategories: cs.CL, cs.LG\nAbstract: We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.428551",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2111.09791v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2111.09791v1"
        ]
      }
    },
    {
      "id": "46e1cf2a",
      "concept": "arabic natural",
      "definition": "We suggest several ways of supporting undotted texts with pre-trained models, without additional training, and measure their performance on two Arabic natural-language-processing downstream tasks",
      "context": "Paper: Supporting Undotted Arabic with Pre-trained Language Models\nAuthors: Aviad Rom, Kfir Bar\nCategories: cs.CL, cs.LG\nAbstract: We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.428651",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2111.09791v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2111.09791v1"
        ]
      }
    },
    {
      "id": "b1a27f25",
      "concept": "social",
      "definition": "We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms",
      "context": "Paper: Supporting Undotted Arabic with Pre-trained Language Models\nAuthors: Aviad Rom, Kfir Bar\nCategories: cs.CL, cs.LG\nAbstract: We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.428703",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2111.09791v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2111.09791v1"
        ]
      }
    },
    {
      "id": "c8278d88",
      "concept": "research",
      "definition": "As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP)",
      "context": "Paper: Multilingual Text Classification for Dravidian Languages\nAuthors: Xiaotian Lin, Nankai Lin, Kanoksak Wattanachote\nCategories: cs.CL\nAbstract: As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.531748",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2112.01705v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2112.01705v1"
        ]
      }
    },
    {
      "id": "7dabf220",
      "concept": "hand",
      "definition": "On the one hand, the framework used the LaBSE pre-trained model as the base model",
      "context": "Paper: Multilingual Text Classification for Dravidian Languages\nAuthors: Xiaotian Lin, Nankai Lin, Kanoksak Wattanachote\nCategories: cs.CL\nAbstract: As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.532033",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2112.01705v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2112.01705v1"
        ]
      }
    },
    {
      "id": "03f0f7c6",
      "concept": "large number",
      "definition": "Although the Dravidian languages contain a large number of languages, there are relatively few public available resources",
      "context": "Paper: Multilingual Text Classification for Dravidian Languages\nAuthors: Xiaotian Lin, Nankai Lin, Kanoksak Wattanachote\nCategories: cs.CL\nAbstract: As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.532117",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2112.01705v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2112.01705v1"
        ]
      }
    },
    {
      "id": "21c7ff29",
      "concept": "still",
      "definition": "Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing",
      "context": "Paper: Multilingual Text Classification for Dravidian Languages\nAuthors: Xiaotian Lin, Nankai Lin, Kanoksak Wattanachote\nCategories: cs.CL\nAbstract: As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.532169",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2112.01705v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2112.01705v1"
        ]
      }
    },
    {
      "id": "1df3e070",
      "concept": "of the",
      "definition": "On the other hand, in view of the problem that the model cannot well recognize and utilize the correlation among languages, we further proposed a language-specific representation module to enrich semantic information for the model",
      "context": "Paper: Multilingual Text Classification for Dravidian Languages\nAuthors: Xiaotian Lin, Nankai Lin, Kanoksak Wattanachote\nCategories: cs.CL\nAbstract: As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.532272",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2112.01705v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2112.01705v1"
        ]
      }
    },
    {
      "id": "17042795",
      "concept": "correlation among",
      "definition": "On the other hand, in view of the problem that the model cannot well recognize and utilize the correlation among languages, we further proposed a language-specific representation module to enrich semantic information for the model",
      "context": "Paper: Multilingual Text Classification for Dravidian Languages\nAuthors: Xiaotian Lin, Nankai Lin, Kanoksak Wattanachote\nCategories: cs.CL\nAbstract: As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.532678",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2112.01705v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2112.01705v1"
        ]
      }
    },
    {
      "id": "f9cc1b14",
      "concept": "language",
      "definition": "As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP)",
      "context": "Paper: Multilingual Text Classification for Dravidian Languages\nAuthors: Xiaotian Lin, Nankai Lin, Kanoksak Wattanachote\nCategories: cs.CL\nAbstract: As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.532845",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2112.01705v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2112.01705v1"
        ]
      }
    },
    {
      "id": "7a1a5748",
      "concept": "demonstrated",
      "definition": "The experimental results demonstrated that the framework we proposed has a significant performance in multilingual text classification tasks with each strategy achieving certain improvements",
      "context": "Paper: Multilingual Text Classification for Dravidian Languages\nAuthors: Xiaotian Lin, Nankai Lin, Kanoksak Wattanachote\nCategories: cs.CL\nAbstract: As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.532912",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2112.01705v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2112.01705v1"
        ]
      }
    },
    {
      "id": "3df81c63",
      "concept": "perturb",
      "definition": "Aiming at the problem of text information bias in multi-task learning, we propose to use the MLM strategy to select language-specific words, and used adversarial training to perturb them",
      "context": "Paper: Multilingual Text Classification for Dravidian Languages\nAuthors: Xiaotian Lin, Nankai Lin, Kanoksak Wattanachote\nCategories: cs.CL\nAbstract: As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.532958",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2112.01705v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2112.01705v1"
        ]
      }
    },
    {
      "id": "9bed2fed",
      "concept": "tasks",
      "definition": "The experimental results demonstrated that the framework we proposed has a significant performance in multilingual text classification tasks with each strategy achieving certain improvements",
      "context": "Paper: Multilingual Text Classification for Dravidian Languages\nAuthors: Xiaotian Lin, Nankai Lin, Kanoksak Wattanachote\nCategories: cs.CL\nAbstract: As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to a...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.533020",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2112.01705v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2112.01705v1"
        ]
      }
    },
    {
      "id": "1da9cc09",
      "concept": "the wider",
      "definition": "The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition",
      "context": "Paper: A Precis of Language Models are not Models of Language\nAuthors: Csaba Veres\nCategories: cs.CL\nAbstract: Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.634326",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2205.07634v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2205.07634v1"
        ]
      }
    },
    {
      "id": "883b1fcb",
      "concept": "our understanding",
      "definition": "The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition",
      "context": "Paper: A Precis of Language Models are not Models of Language\nAuthors: Csaba Veres\nCategories: cs.CL\nAbstract: Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.634482",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2205.07634v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2205.07634v1"
        ]
      }
    },
    {
      "id": "28b0c149",
      "concept": "artificial neural",
      "definition": "Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks",
      "context": "Paper: A Precis of Language Models are not Models of Language\nAuthors: Csaba Veres\nCategories: cs.CL\nAbstract: Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.634534",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2205.07634v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2205.07634v1"
        ]
      }
    },
    {
      "id": "b5a69d43",
      "concept": "precis of",
      "definition": "Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks",
      "context": "Paper: A Precis of Language Models are not Models of Language\nAuthors: Csaba Veres\nCategories: cs.CL\nAbstract: Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.634591",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2205.07634v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2205.07634v1"
        ]
      }
    },
    {
      "id": "ff54ec01",
      "concept": "spearheaded",
      "definition": "Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks",
      "context": "Paper: A Precis of Language Models are not Models of Language\nAuthors: Csaba Veres\nCategories: cs.CL\nAbstract: Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.634660",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2205.07634v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2205.07634v1"
        ]
      }
    },
    {
      "id": "6289c6f2",
      "concept": "successes at",
      "definition": "We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language",
      "context": "Paper: A Precis of Language Models are not Models of Language\nAuthors: Csaba Veres\nCategories: cs.CL\nAbstract: Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.634737",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2205.07634v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2205.07634v1"
        ]
      }
    },
    {
      "id": "783419a0",
      "concept": "not represent",
      "definition": "The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition",
      "context": "Paper: A Precis of Language Models are not Models of Language\nAuthors: Csaba Veres\nCategories: cs.CL\nAbstract: Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.634805",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2205.07634v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2205.07634v1"
        ]
      }
    },
    {
      "id": "58c55881",
      "concept": "application",
      "definition": "Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks",
      "context": "Paper: A Precis of Language Models are not Models of Language\nAuthors: Csaba Veres\nCategories: cs.CL\nAbstract: Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.634863",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2205.07634v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2205.07634v1"
        ]
      }
    },
    {
      "id": "4d8e67dc",
      "concept": "spearheaded by",
      "definition": "Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks",
      "context": "Paper: A Precis of Language Models are not Models of Language\nAuthors: Csaba Veres\nCategories: cs.CL\nAbstract: Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.634929",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2205.07634v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2205.07634v1"
        ]
      }
    },
    {
      "id": "adbfd692",
      "concept": "performing linguistic",
      "definition": "We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language",
      "context": "Paper: A Precis of Language Models are not Models of Language\nAuthors: Csaba Veres\nCategories: cs.CL\nAbstract: Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.634977",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2205.07634v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2205.07634v1"
        ]
      }
    },
    {
      "id": "2a6036ae",
      "concept": "language",
      "definition": "Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models",
      "context": "Paper: Fence - An Efficient Parser with Ambiguity Support for Model-Driven   Language Specification\nAuthors: Luis Quesada, Fernando Berzal, Francisco J. Cortijo\nCategories: cs.CL\nAbstract: Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser g...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.736559",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1107.4687v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1107.4687v2"
        ]
      }
    },
    {
      "id": "14d28a92",
      "concept": "up parsing",
      "definition": "In this paper, we propose Fence, an efficient bottom-up parsing algorithm with lexical and syntactic ambiguity support that enables the use of model-based language specification in practice",
      "context": "Paper: Fence - An Efficient Parser with Ambiguity Support for Model-Driven   Language Specification\nAuthors: Luis Quesada, Fernando Berzal, Francisco J. Cortijo\nCategories: cs.CL\nAbstract: Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser g...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.736728",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1107.4687v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1107.4687v2"
        ]
      }
    },
    {
      "id": "94b99976",
      "concept": "general parser",
      "definition": "Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguities",
      "context": "Paper: Fence - An Efficient Parser with Ambiguity Support for Model-Driven   Language Specification\nAuthors: Luis Quesada, Fernando Berzal, Francisco J. Cortijo\nCategories: cs.CL\nAbstract: Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser g...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.736792",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1107.4687v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1107.4687v2"
        ]
      }
    },
    {
      "id": "1bfcc978",
      "concept": "generators able",
      "definition": "Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguities",
      "context": "Paper: Fence - An Efficient Parser with Ambiguity Support for Model-Driven   Language Specification\nAuthors: Luis Quesada, Fernando Berzal, Francisco J. Cortijo\nCategories: cs.CL\nAbstract: Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser g...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.736846",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1107.4687v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1107.4687v2"
        ]
      }
    },
    {
      "id": "ffe40b12",
      "concept": "specific languages",
      "definition": "Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models",
      "context": "Paper: Fence - An Efficient Parser with Ambiguity Support for Model-Driven   Language Specification\nAuthors: Luis Quesada, Fernando Berzal, Francisco J. Cortijo\nCategories: cs.CL\nAbstract: Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser g...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.736896",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1107.4687v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1107.4687v2"
        ]
      }
    },
    {
      "id": "91a52382",
      "concept": "use of",
      "definition": "In this paper, we propose Fence, an efficient bottom-up parsing algorithm with lexical and syntactic ambiguity support that enables the use of model-based language specification in practice",
      "context": "Paper: Fence - An Efficient Parser with Ambiguity Support for Model-Driven   Language Specification\nAuthors: Luis Quesada, Fernando Berzal, Francisco J. Cortijo\nCategories: cs.CL\nAbstract: Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser g...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.736953",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1107.4687v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1107.4687v2"
        ]
      }
    },
    {
      "id": "a7966fab",
      "concept": "model-based",
      "definition": "Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models",
      "context": "Paper: Fence - An Efficient Parser with Ambiguity Support for Model-Driven   Language Specification\nAuthors: Luis Quesada, Fernando Berzal, Francisco J. Cortijo\nCategories: cs.CL\nAbstract: Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser g...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.737002",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1107.4687v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1107.4687v2"
        ]
      }
    },
    {
      "id": "a365195b",
      "concept": "kinds",
      "definition": "Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguities",
      "context": "Paper: Fence - An Efficient Parser with Ambiguity Support for Model-Driven   Language Specification\nAuthors: Luis Quesada, Fernando Berzal, Francisco J. Cortijo\nCategories: cs.CL\nAbstract: Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser g...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.737053",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1107.4687v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1107.4687v2"
        ]
      }
    },
    {
      "id": "a0d7cf9f",
      "concept": "which constrain",
      "definition": "Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguities",
      "context": "Paper: Fence - An Efficient Parser with Ambiguity Support for Model-Driven   Language Specification\nAuthors: Luis Quesada, Fernando Berzal, Francisco J. Cortijo\nCategories: cs.CL\nAbstract: Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser g...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.737109",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1107.4687v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1107.4687v2"
        ]
      }
    },
    {
      "id": "f8aac569",
      "concept": "it needs",
      "definition": "Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguities",
      "context": "Paper: Fence - An Efficient Parser with Ambiguity Support for Model-Driven   Language Specification\nAuthors: Luis Quesada, Fernando Berzal, Francisco J. Cortijo\nCategories: cs.CL\nAbstract: Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser g...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.737206",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1107.4687v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1107.4687v2"
        ]
      }
    },
    {
      "id": "4d8b84e7",
      "concept": "to achieve",
      "definition": "Quantum computers offer the best chance to achieve translation fluency in that they are better suited to simulating the natural world and natural phenomenon such as natural speech",
      "context": "Paper: Self-move and Other-move: Quantum Categorical Foundations of Japanese\nAuthors: Ryder Dale Walton\nCategories: cs.CL\nAbstract: The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological princi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.839714",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.04451v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.04451v1"
        ]
      }
    },
    {
      "id": "161aa9c8",
      "concept": "geographic location",
      "definition": "Why is this endeavor important? Hundreds of languages have developed over the course of millennia coinciding with the evolution of human interaction across time and geographic location",
      "context": "Paper: Self-move and Other-move: Quantum Categorical Foundations of Japanese\nAuthors: Ryder Dale Walton\nCategories: cs.CL\nAbstract: The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological princi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.839890",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.04451v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.04451v1"
        ]
      }
    },
    {
      "id": "481971cb",
      "concept": "research",
      "definition": "The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research",
      "context": "Paper: Self-move and Other-move: Quantum Categorical Foundations of Japanese\nAuthors: Ryder Dale Walton\nCategories: cs.CL\nAbstract: The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological princi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.839988",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.04451v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.04451v1"
        ]
      }
    },
    {
      "id": "9c0a1b96",
      "concept": "over",
      "definition": "Why is this endeavor important? Hundreds of languages have developed over the course of millennia coinciding with the evolution of human interaction across time and geographic location",
      "context": "Paper: Self-move and Other-move: Quantum Categorical Foundations of Japanese\nAuthors: Ryder Dale Walton\nCategories: cs.CL\nAbstract: The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological princi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.840063",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.04451v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.04451v1"
        ]
      }
    },
    {
      "id": "55f2bd1f",
      "concept": "the current",
      "definition": "The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research",
      "context": "Paper: Self-move and Other-move: Quantum Categorical Foundations of Japanese\nAuthors: Ryder Dale Walton\nCategories: cs.CL\nAbstract: The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological princi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.840115",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.04451v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.04451v1"
        ]
      }
    },
    {
      "id": "3e01540b",
      "concept": "than",
      "definition": "Tools like Google Translate and DeepL make it easier than ever before to share our experiences with people globally",
      "context": "Paper: Self-move and Other-move: Quantum Categorical Foundations of Japanese\nAuthors: Ryder Dale Walton\nCategories: cs.CL\nAbstract: The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological princi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.840172",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.04451v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.04451v1"
        ]
      }
    },
    {
      "id": "0d989f8a",
      "concept": "further research",
      "definition": "Additionally, topological principles of these diagrams and many potential avenues for further research are proposed",
      "context": "Paper: Self-move and Other-move: Quantum Categorical Foundations of Japanese\nAuthors: Ryder Dale Walton\nCategories: cs.CL\nAbstract: The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological princi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.842572",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.04451v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.04451v1"
        ]
      }
    },
    {
      "id": "3f343af2",
      "concept": "they",
      "definition": "They are also, however, the strongest barrier between people groups",
      "context": "Paper: Self-move and Other-move: Quantum Categorical Foundations of Japanese\nAuthors: Ryder Dale Walton\nCategories: cs.CL\nAbstract: The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological princi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.842869",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.04451v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.04451v1"
        ]
      }
    },
    {
      "id": "c835a751",
      "concept": "still",
      "definition": "Nevertheless, these tools are still inadequate as they fail to convey our ideas across the language barrier fluently, leaving people feeling anxious and embarrassed",
      "context": "Paper: Self-move and Other-move: Quantum Categorical Foundations of Japanese\nAuthors: Ryder Dale Walton\nCategories: cs.CL\nAbstract: The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological princi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.842977",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.04451v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.04451v1"
        ]
      }
    },
    {
      "id": "13b4c3d9",
      "concept": "of the",
      "definition": "This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory",
      "context": "Paper: Self-move and Other-move: Quantum Categorical Foundations of Japanese\nAuthors: Ryder Dale Walton\nCategories: cs.CL\nAbstract: The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological princi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.843040",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.04451v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.04451v1"
        ]
      }
    },
    {
      "id": "cce9fa28",
      "concept": "including signed",
      "definition": "Signed languages are the primary means of communication for many deaf and hard of hearing individuals",
      "context": "Paper: Including Signed Languages in Natural Language Processing\nAuthors: Kayo Yin, Amit Moryossef, Julie Hochgesang\nCategories: cs.CL, cs.AI, cs.LG\nAbstract: Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.944903",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2105.05222v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2105.05222v2"
        ]
      }
    },
    {
      "id": "0cb81937",
      "concept": "research",
      "definition": "However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages",
      "context": "Paper: Including Signed Languages in Natural Language Processing\nAuthors: Kayo Yin, Amit Moryossef, Julie Hochgesang\nCategories: cs.CL, cs.AI, cs.LG\nAbstract: Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.945014",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2105.05222v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2105.05222v2"
        ]
      }
    },
    {
      "id": "0363b32e",
      "concept": "communities",
      "definition": "Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research",
      "context": "Paper: Including Signed Languages in Natural Language Processing\nAuthors: Kayo Yin, Amit Moryossef, Julie Hochgesang\nCategories: cs.CL, cs.AI, cs.LG\nAbstract: Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.945070",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2105.05222v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2105.05222v2"
        ]
      }
    },
    {
      "id": "049c9279",
      "concept": "calls",
      "definition": "This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact",
      "context": "Paper: Including Signed Languages in Natural Language Processing\nAuthors: Kayo Yin, Amit Moryossef, Julie Hochgesang\nCategories: cs.CL, cs.AI, cs.LG\nAbstract: Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.945116",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2105.05222v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2105.05222v2"
        ]
      }
    },
    {
      "id": "944d5cfd",
      "concept": "adoption",
      "definition": "Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research",
      "context": "Paper: Including Signed Languages in Natural Language Processing\nAuthors: Kayo Yin, Amit Moryossef, Julie Hochgesang\nCategories: cs.CL, cs.AI, cs.LG\nAbstract: Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.945178",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2105.05222v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2105.05222v2"
        ]
      }
    },
    {
      "id": "a7c32103",
      "concept": "world signed",
      "definition": "Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research",
      "context": "Paper: Including Signed Languages in Natural Language Processing\nAuthors: Kayo Yin, Amit Moryossef, Julie Hochgesang\nCategories: cs.CL, cs.AI, cs.LG\nAbstract: Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.945234",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2105.05222v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2105.05222v2"
        ]
      }
    },
    {
      "id": "6aa6692e",
      "concept": "of research",
      "definition": "Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research",
      "context": "Paper: Including Signed Languages in Natural Language Processing\nAuthors: Kayo Yin, Amit Moryossef, Julie Hochgesang\nCategories: cs.CL, cs.AI, cs.LG\nAbstract: Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.945281",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2105.05222v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2105.05222v2"
        ]
      }
    },
    {
      "id": "f19650cc",
      "concept": "signed",
      "definition": "Signed languages are the primary means of communication for many deaf and hard of hearing individuals",
      "context": "Paper: Including Signed Languages in Natural Language Processing\nAuthors: Kayo Yin, Amit Moryossef, Julie Hochgesang\nCategories: cs.CL, cs.AI, cs.LG\nAbstract: Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.945324",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2105.05222v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2105.05222v2"
        ]
      }
    },
    {
      "id": "e3738f37",
      "concept": "language",
      "definition": "Signed languages are the primary means of communication for many deaf and hard of hearing individuals",
      "context": "Paper: Including Signed Languages in Natural Language Processing\nAuthors: Kayo Yin, Amit Moryossef, Julie Hochgesang\nCategories: cs.CL, cs.AI, cs.LG\nAbstract: Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.945375",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2105.05222v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2105.05222v2"
        ]
      }
    },
    {
      "id": "d5811d80",
      "concept": "social",
      "definition": "This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact",
      "context": "Paper: Including Signed Languages in Natural Language Processing\nAuthors: Kayo Yin, Amit Moryossef, Julie Hochgesang\nCategories: cs.CL, cs.AI, cs.LG\nAbstract: Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community t...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:44.945425",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2105.05222v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2105.05222v2"
        ]
      }
    },
    {
      "id": "8714cafc",
      "concept": "research",
      "definition": "Evaluation in natural language processing guides and promotes research on models and methods",
      "context": "Paper: Problems and Countermeasures in Natural Language Processing Evaluation\nAuthors: Qingxiu Dong, Zhifang Sui, Weidong Zhan\nCategories: cs.CL\nAbstract: Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.046692",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2104.09712v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2104.09712v1"
        ]
      }
    },
    {
      "id": "b860326a",
      "concept": "machine",
      "definition": "Finally, this article refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability evaluation, and proposes a series of basic principles and implementation ideas for hu-man-like machine language ability evaluation from the three aspects of reliability, difficulty and validity",
      "context": "Paper: Problems and Countermeasures in Natural Language Processing Evaluation\nAuthors: Qingxiu Dong, Zhifang Sui, Weidong Zhan\nCategories: cs.CL\nAbstract: Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.046820",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2104.09712v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2104.09712v1"
        ]
      }
    },
    {
      "id": "bbd09f76",
      "concept": "recent",
      "definition": "In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed",
      "context": "Paper: Problems and Countermeasures in Natural Language Processing Evaluation\nAuthors: Qingxiu Dong, Zhifang Sui, Weidong Zhan\nCategories: cs.CL\nAbstract: Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.046881",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2104.09712v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2104.09712v1"
        ]
      }
    },
    {
      "id": "b4af3b05",
      "concept": "and promotes",
      "definition": "Evaluation in natural language processing guides and promotes research on models and methods",
      "context": "Paper: Problems and Countermeasures in Natural Language Processing Evaluation\nAuthors: Qingxiu Dong, Zhifang Sui, Weidong Zhan\nCategories: cs.CL\nAbstract: Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.046913",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2104.09712v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2104.09712v1"
        ]
      }
    },
    {
      "id": "d650c64a",
      "concept": "language",
      "definition": "Evaluation in natural language processing guides and promotes research on models and methods",
      "context": "Paper: Problems and Countermeasures in Natural Language Processing Evaluation\nAuthors: Qingxiu Dong, Zhifang Sui, Weidong Zhan\nCategories: cs.CL\nAbstract: Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.046938",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2104.09712v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2104.09712v1"
        ]
      }
    },
    {
      "id": "146b755d",
      "concept": "tasks",
      "definition": "In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed",
      "context": "Paper: Problems and Countermeasures in Natural Language Processing Evaluation\nAuthors: Qingxiu Dong, Zhifang Sui, Weidong Zhan\nCategories: cs.CL\nAbstract: Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.046981",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2104.09712v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2104.09712v1"
        ]
      }
    },
    {
      "id": "d7060508",
      "concept": "concept",
      "definition": "Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteristics of mainstream natural language evaluation, and then summarizes the problems and causes of natural language pro-cessing evaluation",
      "context": "Paper: Problems and Countermeasures in Natural Language Processing Evaluation\nAuthors: Qingxiu Dong, Zhifang Sui, Weidong Zhan\nCategories: cs.CL\nAbstract: Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.047008",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2104.09712v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2104.09712v1"
        ]
      }
    },
    {
      "id": "efe838ff",
      "concept": "language ability",
      "definition": "Finally, this article refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability evaluation, and proposes a series of basic principles and implementation ideas for hu-man-like machine language ability evaluation from the three aspects of reliability, difficulty and validity",
      "context": "Paper: Problems and Countermeasures in Natural Language Processing Evaluation\nAuthors: Qingxiu Dong, Zhifang Sui, Weidong Zhan\nCategories: cs.CL\nAbstract: Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.047035",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2104.09712v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2104.09712v1"
        ]
      }
    },
    {
      "id": "0efb8905",
      "concept": "human-like",
      "definition": "Finally, this article refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability evaluation, and proposes a series of basic principles and implementation ideas for hu-man-like machine language ability evaluation from the three aspects of reliability, difficulty and validity",
      "context": "Paper: Problems and Countermeasures in Natural Language Processing Evaluation\nAuthors: Qingxiu Dong, Zhifang Sui, Weidong Zhan\nCategories: cs.CL\nAbstract: Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.047072",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2104.09712v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2104.09712v1"
        ]
      }
    },
    {
      "id": "3746d02b",
      "concept": "puts",
      "definition": "Finally, this article refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability evaluation, and proposes a series of basic principles and implementation ideas for hu-man-like machine language ability evaluation from the three aspects of reliability, difficulty and validity",
      "context": "Paper: Problems and Countermeasures in Natural Language Processing Evaluation\nAuthors: Qingxiu Dong, Zhifang Sui, Weidong Zhan\nCategories: cs.CL\nAbstract: Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.047110",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2104.09712v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2104.09712v1"
        ]
      }
    },
    {
      "id": "daa79d1b",
      "concept": "uses",
      "definition": "Specifications in controlled natural language are automatically translated into Prolog clauses, hence become formal and executable",
      "context": "Paper: Specifying Logic Programs in Controlled Natural Language\nAuthors: Norbert E. Fuchs, Rolf Schwitter\nCategories: cmp-lg, cs.CL\nAbstract: Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-special...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.148428",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/9507009v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/9507009v1"
        ]
      }
    },
    {
      "id": "59c212c3",
      "concept": "the application",
      "definition": "Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development",
      "context": "Paper: Specifying Logic Programs in Controlled Natural Language\nAuthors: Norbert E. Fuchs, Rolf Schwitter\nCategories: cmp-lg, cs.CL\nAbstract: Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-special...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.148495",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/9507009v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/9507009v1"
        ]
      }
    },
    {
      "id": "eccb9458",
      "concept": "prototypical",
      "definition": "We have implemented a prototypical specification system that successfully processes the specification of a simple automated teller machine",
      "context": "Paper: Specifying Logic Programs in Controlled Natural Language\nAuthors: Norbert E. Fuchs, Rolf Schwitter\nCategories: cmp-lg, cs.CL\nAbstract: Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-special...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.148541",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/9507009v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/9507009v1"
        ]
      }
    },
    {
      "id": "aa8957e9",
      "concept": "specifications in",
      "definition": "Specifications in controlled natural language are automatically translated into Prolog clauses, hence become formal and executable",
      "context": "Paper: Specifying Logic Programs in Controlled Natural Language\nAuthors: Norbert E. Fuchs, Rolf Schwitter\nCategories: cmp-lg, cs.CL\nAbstract: Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-special...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.148582",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/9507009v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/9507009v1"
        ]
      }
    },
    {
      "id": "d8dcf7ff",
      "concept": "disparate",
      "definition": "Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development",
      "context": "Paper: Specifying Logic Programs in Controlled Natural Language\nAuthors: Norbert E. Fuchs, Rolf Schwitter\nCategories: cmp-lg, cs.CL\nAbstract: Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-special...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.148620",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/9507009v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/9507009v1"
        ]
      }
    },
    {
      "id": "0532ccfc",
      "concept": "take into",
      "definition": "Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development",
      "context": "Paper: Specifying Logic Programs in Controlled Natural Language\nAuthors: Norbert E. Fuchs, Rolf Schwitter\nCategories: cmp-lg, cs.CL\nAbstract: Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-special...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.148661",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/9507009v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/9507009v1"
        ]
      }
    },
    {
      "id": "d4a6deb5",
      "concept": "logic",
      "definition": "Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development",
      "context": "Paper: Specifying Logic Programs in Controlled Natural Language\nAuthors: Norbert E. Fuchs, Rolf Schwitter\nCategories: cmp-lg, cs.CL\nAbstract: Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-special...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.148702",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/9507009v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/9507009v1"
        ]
      }
    },
    {
      "id": "80e1cd3a",
      "concept": "application",
      "definition": "Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development",
      "context": "Paper: Specifying Logic Programs in Controlled Natural Language\nAuthors: Norbert E. Fuchs, Rolf Schwitter\nCategories: cmp-lg, cs.CL\nAbstract: Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-special...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.148738",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/9507009v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/9507009v1"
        ]
      }
    },
    {
      "id": "f96d5ed1",
      "concept": "worlds",
      "definition": "Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development",
      "context": "Paper: Specifying Logic Programs in Controlled Natural Language\nAuthors: Norbert E. Fuchs, Rolf Schwitter\nCategories: cmp-lg, cs.CL\nAbstract: Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-special...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.148772",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/9507009v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/9507009v1"
        ]
      }
    },
    {
      "id": "0e4328c6",
      "concept": "machine",
      "definition": "We have implemented a prototypical specification system that successfully processes the specification of a simple automated teller machine",
      "context": "Paper: Specifying Logic Programs in Controlled Natural Language\nAuthors: Norbert E. Fuchs, Rolf Schwitter\nCategories: cmp-lg, cs.CL\nAbstract: Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-special...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.148831",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/9507009v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/9507009v1"
        ]
      }
    },
    {
      "id": "cb186307",
      "concept": "can be",
      "definition": "PersianLLaMA marks an important step in the development of Persian natural language processing and can be a valuable resource for the Persian-speaking community",
      "context": "Paper: PersianLLaMA: Towards Building First Persian Large Language Model\nAuthors: Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi\nCategories: cs.CL, cs.AI\nAbstract: Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. Thi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.253983",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.15713v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.15713v1"
        ]
      }
    },
    {
      "id": "107f7f5f",
      "concept": "hardware",
      "definition": "The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources",
      "context": "Paper: PersianLLaMA: Towards Building First Persian Large Language Model\nAuthors: Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi\nCategories: cs.CL, cs.AI\nAbstract: Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. Thi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.254270",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.15713v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.15713v1"
        ]
      }
    },
    {
      "id": "ac4ef7d5",
      "concept": "outperforms its",
      "definition": "The results indicate that PersianLLaMA significantly outperforms its competitors in both understanding and generating Persian text",
      "context": "Paper: PersianLLaMA: Towards Building First Persian Large Language Model\nAuthors: Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi\nCategories: cs.CL, cs.AI\nAbstract: Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. Thi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.254490",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.15713v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.15713v1"
        ]
      }
    },
    {
      "id": "817bb5b8",
      "concept": "especially",
      "definition": "This large language model can be used for various natural language processing tasks, especially text generation like chatbots, question-answering, machine translation, and text summarization",
      "context": "Paper: PersianLLaMA: Towards Building First Persian Large Language Model\nAuthors: Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi\nCategories: cs.CL, cs.AI\nAbstract: Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. Thi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.254695",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.15713v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.15713v1"
        ]
      }
    },
    {
      "id": "a4eaee2a",
      "concept": "been evaluated",
      "definition": "PersianLLaMA has been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, and for natural language understanding tasks based on automated machine metrics",
      "context": "Paper: PersianLLaMA: Towards Building First Persian Large Language Model\nAuthors: Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi\nCategories: cs.CL, cs.AI\nAbstract: Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. Thi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.254868",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.15713v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.15713v1"
        ]
      }
    },
    {
      "id": "17c88d25",
      "concept": "textual",
      "definition": "The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources",
      "context": "Paper: PersianLLaMA: Towards Building First Persian Large Language Model\nAuthors: Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi\nCategories: cs.CL, cs.AI\nAbstract: Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. Thi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.255020",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.15713v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.15713v1"
        ]
      }
    },
    {
      "id": "153ee5a5",
      "concept": "larger language",
      "definition": "PersianLLaMA has been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, and for natural language understanding tasks based on automated machine metrics",
      "context": "Paper: PersianLLaMA: Towards Building First Persian Large Language Model\nAuthors: Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi\nCategories: cs.CL, cs.AI\nAbstract: Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. Thi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.255232",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.15713v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.15713v1"
        ]
      }
    },
    {
      "id": "b012e7d4",
      "concept": "persianllama significantly",
      "definition": "The results indicate that PersianLLaMA significantly outperforms its competitors in both understanding and generating Persian text",
      "context": "Paper: PersianLLaMA: Towards Building First Persian Large Language Model\nAuthors: Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi\nCategories: cs.CL, cs.AI\nAbstract: Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. Thi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.255665",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.15713v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.15713v1"
        ]
      }
    },
    {
      "id": "5abb8e9c",
      "concept": "machine",
      "definition": "PersianLLaMA has been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, and for natural language understanding tasks based on automated machine metrics",
      "context": "Paper: PersianLLaMA: Towards Building First Persian Large Language Model\nAuthors: Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi\nCategories: cs.CL, cs.AI\nAbstract: Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. Thi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.255862",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.15713v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.15713v1"
        ]
      }
    },
    {
      "id": "35d66639",
      "concept": "of the",
      "definition": "Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language",
      "context": "Paper: PersianLLaMA: Towards Building First Persian Large Language Model\nAuthors: Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi\nCategories: cs.CL, cs.AI\nAbstract: Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. Thi...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.256008",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2312.15713v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2312.15713v1"
        ]
      }
    },
    {
      "id": "7f5c75f4",
      "concept": "contributions",
      "definition": "Thus, the contributions of this paper are as follows: First, to our knowledge, it is the first attempt to propose a natural language interface to graph-based bibliographic information retrieval",
      "context": "Paper: A natural language interface to a graph-based bibliographic information   retrieval system\nAuthors: Yongjun Zhu, Erjia Yan, Il-Yeol Song\nCategories: cs.IR, cs.CL\nAbstract: With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.364308",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.03231v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1612.03231v1"
        ]
      }
    },
    {
      "id": "89a6fcca",
      "concept": "systems",
      "definition": "With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively",
      "context": "Paper: A natural language interface to a graph-based bibliographic information   retrieval system\nAuthors: Yongjun Zhu, Erjia Yan, Il-Yeol Song\nCategories: cs.IR, cs.CL\nAbstract: With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.364551",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.03231v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1612.03231v1"
        ]
      }
    },
    {
      "id": "ef73584b",
      "concept": "analyze",
      "definition": "A series of text- and linguistic-based techniques are used to analyze and answer natural language queries, including tokenization, named entity recognition, and syntactic analysis",
      "context": "Paper: A natural language interface to a graph-based bibliographic information   retrieval system\nAuthors: Yongjun Zhu, Erjia Yan, Il-Yeol Song\nCategories: cs.IR, cs.CL\nAbstract: With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.364722",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.03231v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1612.03231v1"
        ]
      }
    },
    {
      "id": "fbe1b1b9",
      "concept": "recognition",
      "definition": "A series of text- and linguistic-based techniques are used to analyze and answer natural language queries, including tokenization, named entity recognition, and syntactic analysis",
      "context": "Paper: A natural language interface to a graph-based bibliographic information   retrieval system\nAuthors: Yongjun Zhu, Erjia Yan, Il-Yeol Song\nCategories: cs.IR, cs.CL\nAbstract: With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.364885",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.03231v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1612.03231v1"
        ]
      }
    },
    {
      "id": "75a77d7a",
      "concept": "integrates algorithms",
      "definition": "Our framework integrates algorithms/heuristics for interpreting and analyzing natural language bibliographic queries",
      "context": "Paper: A natural language interface to a graph-based bibliographic information   retrieval system\nAuthors: Yongjun Zhu, Erjia Yan, Il-Yeol Song\nCategories: cs.IR, cs.CL\nAbstract: With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.365070",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.03231v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1612.03231v1"
        ]
      }
    },
    {
      "id": "53a64e89",
      "concept": "presented",
      "definition": "Our experimental results show that the presented system can correctly answer 39 out of 40 example natural language queries with varying lengths and complexities",
      "context": "Paper: A natural language interface to a graph-based bibliographic information   retrieval system\nAuthors: Yongjun Zhu, Erjia Yan, Il-Yeol Song\nCategories: cs.IR, cs.CL\nAbstract: With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.365256",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.03231v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1612.03231v1"
        ]
      }
    },
    {
      "id": "cbfe6a59",
      "concept": "analyzing",
      "definition": "Our framework integrates algorithms/heuristics for interpreting and analyzing natural language bibliographic queries",
      "context": "Paper: A natural language interface to a graph-based bibliographic information   retrieval system\nAuthors: Yongjun Zhu, Erjia Yan, Il-Yeol Song\nCategories: cs.IR, cs.CL\nAbstract: With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.365459",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.03231v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1612.03231v1"
        ]
      }
    },
    {
      "id": "01bb9a3c",
      "concept": "through",
      "definition": "NLI-GIBIR allows users to search for a variety of bibliographic data through natural language",
      "context": "Paper: A natural language interface to a graph-based bibliographic information   retrieval system\nAuthors: Yongjun Zhu, Erjia Yan, Il-Yeol Song\nCategories: cs.IR, cs.CL\nAbstract: With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.365640",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.03231v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1612.03231v1"
        ]
      }
    },
    {
      "id": "fb13c213",
      "concept": "provide",
      "definition": "Third, we show that the proposed framework and natural language interface provide a practical solution in building real-world natural language interface-based bibliographic information retrieval systems",
      "context": "Paper: A natural language interface to a graph-based bibliographic information   retrieval system\nAuthors: Yongjun Zhu, Erjia Yan, Il-Yeol Song\nCategories: cs.IR, cs.CL\nAbstract: With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.365815",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.03231v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1612.03231v1"
        ]
      }
    },
    {
      "id": "97d491a7",
      "concept": "our experimental",
      "definition": "Our experimental results show that the presented system can correctly answer 39 out of 40 example natural language queries with varying lengths and complexities",
      "context": "Paper: A natural language interface to a graph-based bibliographic information   retrieval system\nAuthors: Yongjun Zhu, Erjia Yan, Il-Yeol Song\nCategories: cs.IR, cs.CL\nAbstract: With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.365997",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.03231v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1612.03231v1"
        ]
      }
    },
    {
      "id": "13df8833",
      "concept": "can be",
      "definition": "We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training",
      "context": "Paper: Continuous multilinguality with language vectors\nAuthors: Robert stling, Jrg Tiedemann\nCategories: cs.CL\nAbstract: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we emp...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.467184",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.07486v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1612.07486v2"
        ]
      }
    },
    {
      "id": "17700baf",
      "concept": "character-based",
      "definition": "We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training",
      "context": "Paper: Continuous multilinguality with language vectors\nAuthors: Robert stling, Jrg Tiedemann\nCategories: cs.CL\nAbstract: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we emp...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.467280",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.07486v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1612.07486v2"
        ]
      }
    },
    {
      "id": "58b8dbb5",
      "concept": "that these",
      "definition": "We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training",
      "context": "Paper: Continuous multilinguality with language vectors\nAuthors: Robert stling, Jrg Tiedemann\nCategories: cs.CL\nAbstract: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we emp...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.467330",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.07486v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1612.07486v2"
        ]
      }
    },
    {
      "id": "7229dcd3",
      "concept": "seen",
      "definition": "We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training",
      "context": "Paper: Continuous multilinguality with language vectors\nAuthors: Robert stling, Jrg Tiedemann\nCategories: cs.CL\nAbstract: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we emp...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.467374",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.07486v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1612.07486v2"
        ]
      }
    },
    {
      "id": "7a647264",
      "concept": "improve",
      "definition": "We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training",
      "context": "Paper: Continuous multilinguality with language vectors\nAuthors: Robert stling, Jrg Tiedemann\nCategories: cs.CL\nAbstract: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we emp...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.467406",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.07486v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1612.07486v2"
        ]
      }
    },
    {
      "id": "bc0aac03",
      "concept": "language",
      "definition": "Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other",
      "context": "Paper: Continuous multilinguality with language vectors\nAuthors: Robert stling, Jrg Tiedemann\nCategories: cs.CL\nAbstract: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we emp...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.467428",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.07486v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1612.07486v2"
        ]
      }
    },
    {
      "id": "1b7d167d",
      "concept": "vectors capture",
      "definition": "In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages",
      "context": "Paper: Continuous multilinguality with language vectors\nAuthors: Robert stling, Jrg Tiedemann\nCategories: cs.CL\nAbstract: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we emp...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.467458",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.07486v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1612.07486v2"
        ]
      }
    },
    {
      "id": "209646ea",
      "concept": "empirically",
      "definition": "In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages",
      "context": "Paper: Continuous multilinguality with language vectors\nAuthors: Robert stling, Jrg Tiedemann\nCategories: cs.CL\nAbstract: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we emp...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.467489",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.07486v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1612.07486v2"
        ]
      }
    },
    {
      "id": "80712b6d",
      "concept": "vector",
      "definition": "In contrast, we propose using continuous vector representations of language",
      "context": "Paper: Continuous multilinguality with language vectors\nAuthors: Robert stling, Jrg Tiedemann\nCategories: cs.CL\nAbstract: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we emp...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.467515",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.07486v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1612.07486v2"
        ]
      }
    },
    {
      "id": "1e056049",
      "concept": "based neural",
      "definition": "We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training",
      "context": "Paper: Continuous multilinguality with language vectors\nAuthors: Robert stling, Jrg Tiedemann\nCategories: cs.CL\nAbstract: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we emp...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:45.467553",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1612.07486v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1612.07486v2"
        ]
      }
    },
    {
      "id": "0ff940f5",
      "concept": "research",
      "definition": "Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces",
      "context": "Paper: Implications of Computer Vision Driven Assistive Technologies Towards   Individuals with Visual Impairment\nAuthors: Linda Wang, Alexander Wong\nCategories: cs.CV, cs.CY\nAbstract: Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potenti...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:46.952814",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1905.07844v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1905.07844v1"
        ]
      }
    },
    {
      "id": "0e4965c4",
      "concept": "visual impairment",
      "definition": "One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment",
      "context": "Paper: Implications of Computer Vision Driven Assistive Technologies Towards   Individuals with Visual Impairment\nAuthors: Linda Wang, Alexander Wong\nCategories: cs.CV, cs.CY\nAbstract: Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potenti...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:46.952985",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1905.07844v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1905.07844v1"
        ]
      }
    },
    {
      "id": "4e417c0a",
      "concept": "and autonomy",
      "definition": "Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potential usefulness arise",
      "context": "Paper: Implications of Computer Vision Driven Assistive Technologies Towards   Individuals with Visual Impairment\nAuthors: Linda Wang, Alexander Wong\nCategories: cs.CV, cs.CY\nAbstract: Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potenti...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:46.953061",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1905.07844v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1905.07844v1"
        ]
      }
    },
    {
      "id": "94cdee28",
      "concept": "over",
      "definition": "Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potential usefulness arise",
      "context": "Paper: Implications of Computer Vision Driven Assistive Technologies Towards   Individuals with Visual Impairment\nAuthors: Linda Wang, Alexander Wong\nCategories: cs.CV, cs.CY\nAbstract: Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potenti...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:46.953127",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1905.07844v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1905.07844v1"
        ]
      }
    },
    {
      "id": "10e1ddc2",
      "concept": "mitigate",
      "definition": "This paper addresses the positive and negative implications computer vision based assistive technologies have on individuals with visual impairment, as well as considerations for computer vision researchers and developers in order to mitigate the amount of negative implications",
      "context": "Paper: Implications of Computer Vision Driven Assistive Technologies Towards   Individuals with Visual Impairment\nAuthors: Linda Wang, Alexander Wong\nCategories: cs.CV, cs.CY\nAbstract: Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potenti...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:46.953191",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1905.07844v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1905.07844v1"
        ]
      }
    },
    {
      "id": "f1f0e02f",
      "concept": "application",
      "definition": "One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment",
      "context": "Paper: Implications of Computer Vision Driven Assistive Technologies Towards   Individuals with Visual Impairment\nAuthors: Linda Wang, Alexander Wong\nCategories: cs.CV, cs.CY\nAbstract: Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potenti...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:46.953251",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1905.07844v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1905.07844v1"
        ]
      }
    },
    {
      "id": "c84562c6",
      "concept": "to mitigate",
      "definition": "This paper addresses the positive and negative implications computer vision based assistive technologies have on individuals with visual impairment, as well as considerations for computer vision researchers and developers in order to mitigate the amount of negative implications",
      "context": "Paper: Implications of Computer Vision Driven Assistive Technologies Towards   Individuals with Visual Impairment\nAuthors: Linda Wang, Alexander Wong\nCategories: cs.CV, cs.CY\nAbstract: Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potenti...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:46.953317",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1905.07844v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1905.07844v1"
        ]
      }
    },
    {
      "id": "d5026261",
      "concept": "seen",
      "definition": "One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment",
      "context": "Paper: Implications of Computer Vision Driven Assistive Technologies Towards   Individuals with Visual Impairment\nAuthors: Linda Wang, Alexander Wong\nCategories: cs.CV, cs.CY\nAbstract: Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potenti...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:46.953398",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1905.07844v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1905.07844v1"
        ]
      }
    },
    {
      "id": "09dc1198",
      "concept": "positive",
      "definition": "This paper addresses the positive and negative implications computer vision based assistive technologies have on individuals with visual impairment, as well as considerations for computer vision researchers and developers in order to mitigate the amount of negative implications",
      "context": "Paper: Implications of Computer Vision Driven Assistive Technologies Towards   Individuals with Visual Impairment\nAuthors: Linda Wang, Alexander Wong\nCategories: cs.CV, cs.CY\nAbstract: Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potenti...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:46.953463",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1905.07844v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1905.07844v1"
        ]
      }
    },
    {
      "id": "959dee39",
      "concept": "provide",
      "definition": "Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces",
      "context": "Paper: Implications of Computer Vision Driven Assistive Technologies Towards   Individuals with Visual Impairment\nAuthors: Linda Wang, Alexander Wong\nCategories: cs.CV, cs.CY\nAbstract: Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potenti...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:46.953519",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1905.07844v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1905.07844v1"
        ]
      }
    },
    {
      "id": "a0f9103c",
      "concept": "the center",
      "definition": "Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb",
      "context": "Paper: Second Croatian Computer Vision Workshop (CCVW 2013)\nAuthors: Sven Lonari, Sinia egvi\nCategories: cs.CV\nAbstract: Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.054538",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1310.0319v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1310.0319v3"
        ]
      }
    },
    {
      "id": "115e6e95",
      "concept": "computer",
      "definition": "Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www",
      "context": "Paper: Second Croatian Computer Vision Workshop (CCVW 2013)\nAuthors: Sven Lonari, Sinia egvi\nCategories: cs.CV\nAbstract: Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.054672",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1310.0319v3",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1310.0319v3"
        ]
      }
    },
    {
      "id": "8b5be2f8",
      "concept": "the second",
      "definition": "Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www",
      "context": "Paper: Second Croatian Computer Vision Workshop (CCVW 2013)\nAuthors: Sven Lonari, Sinia egvi\nCategories: cs.CV\nAbstract: Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.054741",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1310.0319v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1310.0319v3"
        ]
      }
    },
    {
      "id": "4ef744e5",
      "concept": "workshop",
      "definition": "Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www",
      "context": "Paper: Second Croatian Computer Vision Workshop (CCVW 2013)\nAuthors: Sven Lonari, Sinia egvi\nCategories: cs.CV\nAbstract: Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.054804",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1310.0319v3",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1310.0319v3"
        ]
      }
    },
    {
      "id": "369f54fe",
      "concept": "held",
      "definition": "hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia",
      "context": "Paper: Second Croatian Computer Vision Workshop (CCVW 2013)\nAuthors: Sven Lonari, Sinia egvi\nCategories: cs.CV\nAbstract: Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.054859",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1310.0319v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1310.0319v3"
        ]
      }
    },
    {
      "id": "2db17d28",
      "concept": "organized by",
      "definition": "Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb",
      "context": "Paper: Second Croatian Computer Vision Workshop (CCVW 2013)\nAuthors: Sven Lonari, Sinia egvi\nCategories: cs.CV\nAbstract: Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.054914",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1310.0319v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1310.0319v3"
        ]
      }
    },
    {
      "id": "aa960a61",
      "concept": "proceedings",
      "definition": "Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www",
      "context": "Paper: Second Croatian Computer Vision Workshop (CCVW 2013)\nAuthors: Sven Lonari, Sinia egvi\nCategories: cs.CV\nAbstract: Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.054964",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1310.0319v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1310.0319v3"
        ]
      }
    },
    {
      "id": "e839445a",
      "concept": "of excellence",
      "definition": "Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb",
      "context": "Paper: Second Croatian Computer Vision Workshop (CCVW 2013)\nAuthors: Sven Lonari, Sinia egvi\nCategories: cs.CV\nAbstract: Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.055013",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1310.0319v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1310.0319v3"
        ]
      }
    },
    {
      "id": "073ed21a",
      "concept": "http",
      "definition": "Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www",
      "context": "Paper: Second Croatian Computer Vision Workshop (CCVW 2013)\nAuthors: Sven Lonari, Sinia egvi\nCategories: cs.CV\nAbstract: Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.055064",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1310.0319v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1310.0319v3"
        ]
      }
    },
    {
      "id": "b0686c06",
      "concept": "september",
      "definition": "hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia",
      "context": "Paper: Second Croatian Computer Vision Workshop (CCVW 2013)\nAuthors: Sven Lonari, Sinia egvi\nCategories: cs.CV\nAbstract: Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.055111",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1310.0319v3",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1310.0319v3"
        ]
      }
    },
    {
      "id": "e4806c24",
      "concept": "to achieve",
      "definition": "In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput",
      "context": "Paper: Multiband NFC for High-Throughput Wireless Computer Vision Sensor   Network\nAuthors: F. Li, J. Du\nCategories: cs.NI, cs.CV\nAbstract: Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.156018",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.03720v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.03720v1"
        ]
      }
    },
    {
      "id": "8bc059e6",
      "concept": "high-throughput",
      "definition": "Vision sensors lie in the heart of computer vision",
      "context": "Paper: Multiband NFC for High-Throughput Wireless Computer Vision Sensor   Network\nAuthors: F. Li, J. Du\nCategories: cs.NI, cs.CV\nAbstract: Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.156118",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.03720v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1707.03720v1"
        ]
      }
    },
    {
      "id": "058dd09b",
      "concept": "required",
      "definition": "In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms",
      "context": "Paper: Multiband NFC for High-Throughput Wireless Computer Vision Sensor   Network\nAuthors: F. Li, J. Du\nCategories: cs.NI, cs.CV\nAbstract: Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.156166",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.03720v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.03720v1"
        ]
      }
    },
    {
      "id": "881c2414",
      "concept": "with high",
      "definition": "In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms",
      "context": "Paper: Multiband NFC for High-Throughput Wireless Computer Vision Sensor   Network\nAuthors: F. Li, J. Du\nCategories: cs.NI, cs.CV\nAbstract: Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.156220",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.03720v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.03720v1"
        ]
      }
    },
    {
      "id": "43b833a6",
      "concept": "such as",
      "definition": "In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms",
      "context": "Paper: Multiband NFC for High-Throughput Wireless Computer Vision Sensor   Network\nAuthors: F. Li, J. Du\nCategories: cs.NI, cs.CV\nAbstract: Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.156266",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.03720v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.03720v1"
        ]
      }
    },
    {
      "id": "211ba7ff",
      "concept": "computer",
      "definition": "Vision sensors lie in the heart of computer vision",
      "context": "Paper: Multiband NFC for High-Throughput Wireless Computer Vision Sensor   Network\nAuthors: F. Li, J. Du\nCategories: cs.NI, cs.CV\nAbstract: Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.156311",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.03720v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1707.03720v1"
        ]
      }
    },
    {
      "id": "66dfc872",
      "concept": "applications",
      "definition": "In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms",
      "context": "Paper: Multiband NFC for High-Throughput Wireless Computer Vision Sensor   Network\nAuthors: F. Li, J. Du\nCategories: cs.NI, cs.CV\nAbstract: Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.156370",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.03720v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.03720v1"
        ]
      }
    },
    {
      "id": "62f644a4",
      "concept": "multiple",
      "definition": "In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput",
      "context": "Paper: Multiband NFC for High-Throughput Wireless Computer Vision Sensor   Network\nAuthors: F. Li, J. Du\nCategories: cs.NI, cs.CV\nAbstract: Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.156417",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.03720v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.03720v1"
        ]
      }
    },
    {
      "id": "8b8b2629",
      "concept": "to algorithms",
      "definition": "In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms",
      "context": "Paper: Multiband NFC for High-Throughput Wireless Computer Vision Sensor   Network\nAuthors: F. Li, J. Du\nCategories: cs.NI, cs.CV\nAbstract: Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.156458",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.03720v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.03720v1"
        ]
      }
    },
    {
      "id": "a3a464d3",
      "concept": "the heart",
      "definition": "Vision sensors lie in the heart of computer vision",
      "context": "Paper: Multiband NFC for High-Throughput Wireless Computer Vision Sensor   Network\nAuthors: F. Li, J. Du\nCategories: cs.NI, cs.CV\nAbstract: Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.156502",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1707.03720v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1707.03720v1"
        ]
      }
    },
    {
      "id": "6b095595",
      "concept": "can be",
      "definition": "The paper will also explore how the two sides of computer vision can be combined",
      "context": "Paper: Deep Learning vs. Traditional Computer Vision\nAuthors: Niall O' Mahony, Sean Campbell, Anderson Carvalho\nCategories: cs.CV, cs.LG\nAbstract: Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will als...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.257934",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1910.13796v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1910.13796v1"
        ]
      }
    },
    {
      "id": "7b012ced",
      "concept": "discussion",
      "definition": "The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained",
      "context": "Paper: Deep Learning vs. Traditional Computer Vision\nAuthors: Niall O' Mahony, Sean Campbell, Anderson Carvalho\nCategories: cs.CV, cs.LG\nAbstract: Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will als...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.258036",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1910.13796v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1910.13796v1"
        ]
      }
    },
    {
      "id": "f3035305",
      "concept": "recent",
      "definition": "Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning",
      "context": "Paper: Deep Learning vs. Traditional Computer Vision\nAuthors: Niall O' Mahony, Sean Campbell, Anderson Carvalho\nCategories: cs.CV, cs.LG\nAbstract: Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will als...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.258079",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1910.13796v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1910.13796v1"
        ]
      }
    },
    {
      "id": "a038b940",
      "concept": "improve",
      "definition": "Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning",
      "context": "Paper: Deep Learning vs. Traditional Computer Vision\nAuthors: Niall O' Mahony, Sean Campbell, Anderson Carvalho\nCategories: cs.CV, cs.LG\nAbstract: Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will als...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.258118",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1910.13796v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1910.13796v1"
        ]
      }
    },
    {
      "id": "41e06c33",
      "concept": "this paper",
      "definition": "This paper will analyse the benefits and drawbacks of each approach",
      "context": "Paper: Deep Learning vs. Traditional Computer Vision\nAuthors: Niall O' Mahony, Sean Campbell, Anderson Carvalho\nCategories: cs.CV, cs.LG\nAbstract: Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will als...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.258166",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1910.13796v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1910.13796v1"
        ]
      }
    },
    {
      "id": "7d177519",
      "concept": "the aim",
      "definition": "The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained",
      "context": "Paper: Deep Learning vs. Traditional Computer Vision\nAuthors: Niall O' Mahony, Sean Campbell, Anderson Carvalho\nCategories: cs.CV, cs.LG\nAbstract: Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will als...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.258205",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1910.13796v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1910.13796v1"
        ]
      }
    },
    {
      "id": "df372fca",
      "concept": "demonstrated",
      "definition": "Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning",
      "context": "Paper: Deep Learning vs. Traditional Computer Vision\nAuthors: Niall O' Mahony, Sean Campbell, Anderson Carvalho\nCategories: cs.CV, cs.LG\nAbstract: Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will als...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.258241",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1910.13796v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1910.13796v1"
        ]
      }
    },
    {
      "id": "e585acd5",
      "concept": "deep",
      "definition": "Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing",
      "context": "Paper: Deep Learning vs. Traditional Computer Vision\nAuthors: Niall O' Mahony, Sean Campbell, Anderson Carvalho\nCategories: cs.CV, cs.LG\nAbstract: Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will als...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.258271",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1910.13796v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1910.13796v1"
        ]
      }
    },
    {
      "id": "de16f7dc",
      "concept": "of what",
      "definition": "Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing",
      "context": "Paper: Deep Learning vs. Traditional Computer Vision\nAuthors: Niall O' Mahony, Sean Campbell, Anderson Carvalho\nCategories: cs.CV, cs.LG\nAbstract: Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will als...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.258300",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1910.13796v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1910.13796v1"
        ]
      }
    },
    {
      "id": "7cc60f7b",
      "concept": "the benefits",
      "definition": "This paper will analyse the benefits and drawbacks of each approach",
      "context": "Paper: Deep Learning vs. Traditional Computer Vision\nAuthors: Niall O' Mahony, Sean Campbell, Anderson Carvalho\nCategories: cs.CV, cs.LG\nAbstract: Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will als...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.258344",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1910.13796v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1910.13796v1"
        ]
      }
    },
    {
      "id": "defbc6b3",
      "concept": "live monitoring",
      "definition": "$\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus on computer vision as a solution for two common surveillance camera tasks (live monitoring of multiple surveillance cameras and summarizing archived video files)",
      "context": "Paper: Enhancing camera surveillance using computer vision: a research note\nAuthors: Haroon Idrees, Mubarak Shah, Ray Surette\nCategories: cs.CY\nAbstract: $\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.361184",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1808.03998v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1808.03998v1"
        ]
      }
    },
    {
      "id": "530f5861",
      "concept": "vision or",
      "definition": "There is little knowledge of computer vision or its potential in the field",
      "context": "Paper: Enhancing camera surveillance using computer vision: a research note\nAuthors: Haroon Idrees, Mubarak Shah, Ray Surette\nCategories: cs.CY\nAbstract: $\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.361345",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1808.03998v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1808.03998v1"
        ]
      }
    },
    {
      "id": "835d1e43",
      "concept": "research",
      "definition": "An ongoing research project on the application of computer vision within a municipal police department is described",
      "context": "Paper: Enhancing camera surveillance using computer vision: a research note\nAuthors: Haroon Idrees, Mubarak Shah, Ray Surette\nCategories: cs.CY\nAbstract: $\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.361435",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1808.03998v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1808.03998v1"
        ]
      }
    },
    {
      "id": "fb98d47f",
      "concept": "within",
      "definition": "An ongoing research project on the application of computer vision within a municipal police department is described",
      "context": "Paper: Enhancing camera surveillance using computer vision: a research note\nAuthors: Haroon Idrees, Mubarak Shah, Ray Surette\nCategories: cs.CY\nAbstract: $\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.361492",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1808.03998v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1808.03998v1"
        ]
      }
    },
    {
      "id": "938f3369",
      "concept": "systems",
      "definition": "Three unaddressed research questions (can specialized computer vision applications for law enforcement be developed at this time, how will computer vision be utilized within existing public safety camera monitoring rooms, and what are the system-wide impacts of a computer vision capability on local criminal justice systems) are considered",
      "context": "Paper: Enhancing camera surveillance using computer vision: a research note\nAuthors: Haroon Idrees, Mubarak Shah, Ray Surette\nCategories: cs.CY\nAbstract: $\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.361553",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1808.03998v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1808.03998v1"
        ]
      }
    },
    {
      "id": "cde5c869",
      "concept": "application",
      "definition": "An ongoing research project on the application of computer vision within a municipal police department is described",
      "context": "Paper: Enhancing camera surveillance using computer vision: a research note\nAuthors: Haroon Idrees, Mubarak Shah, Ray Surette\nCategories: cs.CY\nAbstract: $\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.361609",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1808.03998v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1808.03998v1"
        ]
      }
    },
    {
      "id": "eb969e14",
      "concept": "research project",
      "definition": "An ongoing research project on the application of computer vision within a municipal police department is described",
      "context": "Paper: Enhancing camera surveillance using computer vision: a research note\nAuthors: Haroon Idrees, Mubarak Shah, Ray Surette\nCategories: cs.CY\nAbstract: $\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.361665",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1808.03998v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1808.03998v1"
        ]
      }
    },
    {
      "id": "2d50dba7",
      "concept": "perspective and",
      "definition": "$\\mathbf{Originality/value}$ - This paper introduces and discusses computer vision from a law enforcement perspective and will be valuable to police personnel tasked with monitoring large camera networks and considering computer vision as a system upgrade",
      "context": "Paper: Enhancing camera surveillance using computer vision: a research note\nAuthors: Haroon Idrees, Mubarak Shah, Ray Surette\nCategories: cs.CY\nAbstract: $\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.361727",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1808.03998v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1808.03998v1"
        ]
      }
    },
    {
      "id": "67fb7906",
      "concept": "for police",
      "definition": "$\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus on computer vision as a solution for two common surveillance camera tasks (live monitoring of multiple surveillance cameras and summarizing archived video files)",
      "context": "Paper: Enhancing camera surveillance using computer vision: a research note\nAuthors: Haroon Idrees, Mubarak Shah, Ray Surette\nCategories: cs.CY\nAbstract: $\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.361778",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1808.03998v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1808.03998v1"
        ]
      }
    },
    {
      "id": "340027bf",
      "concept": "ongoing",
      "definition": "An ongoing research project on the application of computer vision within a municipal police department is described",
      "context": "Paper: Enhancing camera surveillance using computer vision: a research note\nAuthors: Haroon Idrees, Mubarak Shah, Ray Surette\nCategories: cs.CY\nAbstract: $\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.361832",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1808.03998v1",
        "extraction_confidence": 0.75,
        "parent_sources": [
          "https://arxiv.org/abs/1808.03998v1"
        ]
      }
    },
    {
      "id": "60f33b12",
      "concept": "and challenges",
      "definition": "The maritime environment offers its own unique requirements and challenges",
      "context": "Paper: Are object detection assessment criteria ready for maritime computer   vision?\nAuthors: Dilip K. Prasad, Huixu Dong, Deepu Rajan\nCategories: cs.CV\nAbstract: Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime set...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.463544",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.04659v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1809.04659v2"
        ]
      }
    },
    {
      "id": "c91d291d",
      "concept": "detection assessment",
      "definition": "Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection",
      "context": "Paper: Are object detection assessment criteria ready for maritime computer   vision?\nAuthors: Dilip K. Prasad, Huixu Dong, Deepu Rajan\nCategories: cs.CV\nAbstract: Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime set...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.463696",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.04659v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1809.04659v2"
        ]
      }
    },
    {
      "id": "3618c9f3",
      "concept": "application",
      "definition": "However, application of computer vision techniques in maritime domain received attention only recently",
      "context": "Paper: Are object detection assessment criteria ready for maritime computer   vision?\nAuthors: Dilip K. Prasad, Huixu Dong, Deepu Rajan\nCategories: cs.CV\nAbstract: Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime set...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.463767",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.04659v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1809.04659v2"
        ]
      }
    },
    {
      "id": "486f7bc4",
      "concept": "received attention",
      "definition": "However, application of computer vision techniques in maritime domain received attention only recently",
      "context": "Paper: Are object detection assessment criteria ready for maritime computer   vision?\nAuthors: Dilip K. Prasad, Huixu Dong, Deepu Rajan\nCategories: cs.CV\nAbstract: Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime set...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.463827",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.04659v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1809.04659v2"
        ]
      }
    },
    {
      "id": "bf6d7d26",
      "concept": "maritime",
      "definition": "Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection",
      "context": "Paper: Are object detection assessment criteria ready for maritime computer   vision?\nAuthors: Dilip K. Prasad, Huixu Dong, Deepu Rajan\nCategories: cs.CV\nAbstract: Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime set...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.463877",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.04659v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1809.04659v2"
        ]
      }
    },
    {
      "id": "b9b57cfd",
      "concept": "inapplicable",
      "definition": "Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight",
      "context": "Paper: Are object detection assessment criteria ready for maritime computer   vision?\nAuthors: Dilip K. Prasad, Huixu Dong, Deepu Rajan\nCategories: cs.CV\nAbstract: Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime set...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.463967",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.04659v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1809.04659v2"
        ]
      }
    },
    {
      "id": "ed037549",
      "concept": "first",
      "definition": "Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight",
      "context": "Paper: Are object detection assessment criteria ready for maritime computer   vision?\nAuthors: Dilip K. Prasad, Huixu Dong, Deepu Rajan\nCategories: cs.CV\nAbstract: Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime set...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.464027",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.04659v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1809.04659v2"
        ]
      }
    },
    {
      "id": "d26f4438",
      "concept": "work in",
      "definition": "Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight",
      "context": "Paper: Are object detection assessment criteria ready for maritime computer   vision?\nAuthors: Dilip K. Prasad, Huixu Dong, Deepu Rajan\nCategories: cs.CV\nAbstract: Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime set...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.464092",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.04659v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1809.04659v2"
        ]
      }
    },
    {
      "id": "54b1a15e",
      "concept": "maritime computer",
      "definition": "We discuss the problem of defining assessment metrics suitable for maritime computer vision",
      "context": "Paper: Are object detection assessment criteria ready for maritime computer   vision?\nAuthors: Dilip K. Prasad, Huixu Dong, Deepu Rajan\nCategories: cs.CV\nAbstract: Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime set...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.464146",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.04659v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1809.04659v2"
        ]
      }
    },
    {
      "id": "8e846fd9",
      "concept": "at the",
      "definition": "Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight",
      "context": "Paper: Are object detection assessment criteria ready for maritime computer   vision?\nAuthors: Dilip K. Prasad, Huixu Dong, Deepu Rajan\nCategories: cs.CV\nAbstract: Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime set...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.464196",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.04659v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1809.04659v2"
        ]
      }
    },
    {
      "id": "496cffb1",
      "concept": "september",
      "definition": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019",
      "context": "Paper: BMVC 2019: Workshop on Interpretable and Explainable Machine Vision\nAuthors: Alun Preece\nCategories: cs.CV, cs.AI, cs.LG\nAbstract: Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.564814",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.07245v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.07245v1"
        ]
      }
    },
    {
      "id": "01dbdef3",
      "concept": "interpretable and",
      "definition": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019",
      "context": "Paper: BMVC 2019: Workshop on Interpretable and Explainable Machine Vision\nAuthors: Alun Preece\nCategories: cs.CV, cs.AI, cs.LG\nAbstract: Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.564898",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.07245v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1909.07245v1"
        ]
      }
    },
    {
      "id": "9e430b70",
      "concept": "explainable",
      "definition": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019",
      "context": "Paper: BMVC 2019: Workshop on Interpretable and Explainable Machine Vision\nAuthors: Alun Preece\nCategories: cs.CV, cs.AI, cs.LG\nAbstract: Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.564938",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.07245v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1909.07245v1"
        ]
      }
    },
    {
      "id": "e7144f12",
      "concept": "bmvc",
      "definition": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019",
      "context": "Paper: BMVC 2019: Workshop on Interpretable and Explainable Machine Vision\nAuthors: Alun Preece\nCategories: cs.CV, cs.AI, cs.LG\nAbstract: Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.564975",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.07245v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1909.07245v1"
        ]
      }
    },
    {
      "id": "4a06e728",
      "concept": "cardiff",
      "definition": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019",
      "context": "Paper: BMVC 2019: Workshop on Interpretable and Explainable Machine Vision\nAuthors: Alun Preece\nCategories: cs.CV, cs.AI, cs.LG\nAbstract: Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.565013",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.07245v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.07245v1"
        ]
      }
    },
    {
      "id": "a0816546",
      "concept": "workshop",
      "definition": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019",
      "context": "Paper: BMVC 2019: Workshop on Interpretable and Explainable Machine Vision\nAuthors: Alun Preece\nCategories: cs.CV, cs.AI, cs.LG\nAbstract: Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.565050",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.07245v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1909.07245v1"
        ]
      }
    },
    {
      "id": "a487515a",
      "concept": "the bmvc",
      "definition": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019",
      "context": "Paper: BMVC 2019: Workshop on Interpretable and Explainable Machine Vision\nAuthors: Alun Preece\nCategories: cs.CV, cs.AI, cs.LG\nAbstract: Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.565109",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.07245v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.07245v1"
        ]
      }
    },
    {
      "id": "f05d621c",
      "concept": "machine",
      "definition": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019",
      "context": "Paper: BMVC 2019: Workshop on Interpretable and Explainable Machine Vision\nAuthors: Alun Preece\nCategories: cs.CV, cs.AI, cs.LG\nAbstract: Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.565146",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.07245v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1909.07245v1"
        ]
      }
    },
    {
      "id": "9286e22c",
      "concept": "proceedings of",
      "definition": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019",
      "context": "Paper: BMVC 2019: Workshop on Interpretable and Explainable Machine Vision\nAuthors: Alun Preece\nCategories: cs.CV, cs.AI, cs.LG\nAbstract: Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.565182",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.07245v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1909.07245v1"
        ]
      }
    },
    {
      "id": "8b4cb08b",
      "concept": "vision",
      "definition": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019",
      "context": "Paper: BMVC 2019: Workshop on Interpretable and Explainable Machine Vision\nAuthors: Alun Preece\nCategories: cs.CV, cs.AI, cs.LG\nAbstract: Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.565220",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1909.07245v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1909.07245v1"
        ]
      }
    },
    {
      "id": "b8692206",
      "concept": "within",
      "definition": "Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images",
      "context": "Paper: Vision Transformers in Medical Computer Vision -- A Contemplative   Retrospection\nAuthors: Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar\nCategories: eess.IV, cs.CV, cs.LG\nAbstract: Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are imme...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.667195",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2203.15269v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2203.15269v1"
        ]
      }
    },
    {
      "id": "4fdaebff",
      "concept": "segmentation",
      "definition": "We surveyed the application of Vision transformers in different areas of medical computer vision such as image-based disease classification, anatomical structure segmentation, registration, region-based lesion Detection, captioning, report generation, reconstruction using multiple medical imaging modalities that greatly assist in medical diagnosis and hence treatment process",
      "context": "Paper: Vision Transformers in Medical Computer Vision -- A Contemplative   Retrospection\nAuthors: Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar\nCategories: eess.IV, cs.CV, cs.LG\nAbstract: Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are imme...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.667299",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2203.15269v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2203.15269v1"
        ]
      }
    },
    {
      "id": "109f38ef",
      "concept": "the application",
      "definition": "We surveyed the application of Vision transformers in different areas of medical computer vision such as image-based disease classification, anatomical structure segmentation, registration, region-based lesion Detection, captioning, report generation, reconstruction using multiple medical imaging modalities that greatly assist in medical diagnosis and hence treatment process",
      "context": "Paper: Vision Transformers in Medical Computer Vision -- A Contemplative   Retrospection\nAuthors: Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar\nCategories: eess.IV, cs.CV, cs.LG\nAbstract: Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are imme...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.667376",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2203.15269v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2203.15269v1"
        ]
      }
    },
    {
      "id": "5383ed50",
      "concept": "potential to",
      "definition": "Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images",
      "context": "Paper: Vision Transformers in Medical Computer Vision -- A Contemplative   Retrospection\nAuthors: Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar\nCategories: eess.IV, cs.CV, cs.LG\nAbstract: Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are imme...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.667432",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2203.15269v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2203.15269v1"
        ]
      }
    },
    {
      "id": "8f35967b",
      "concept": "discussion",
      "definition": "Conclusively, we also put some light on available data sets, adopted methodology, their performance measures, challenges and their solutions in form of discussion",
      "context": "Paper: Vision Transformers in Medical Computer Vision -- A Contemplative   Retrospection\nAuthors: Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar\nCategories: eess.IV, cs.CV, cs.LG\nAbstract: Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are imme...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.667484",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2203.15269v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2203.15269v1"
        ]
      }
    },
    {
      "id": "9c333356",
      "concept": "in form",
      "definition": "Conclusively, we also put some light on available data sets, adopted methodology, their performance measures, challenges and their solutions in form of discussion",
      "context": "Paper: Vision Transformers in Medical Computer Vision -- A Contemplative   Retrospection\nAuthors: Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar\nCategories: eess.IV, cs.CV, cs.LG\nAbstract: Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are imme...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.667549",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2203.15269v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2203.15269v1"
        ]
      }
    },
    {
      "id": "e1bf3449",
      "concept": "is also",
      "definition": "Moreover, to get more insight and deeper understanding, self-attention mechanism of transformers is also explained briefly",
      "context": "Paper: Vision Transformers in Medical Computer Vision -- A Contemplative   Retrospection\nAuthors: Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar\nCategories: eess.IV, cs.CV, cs.LG\nAbstract: Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are imme...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.667605",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2203.15269v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2203.15269v1"
        ]
      }
    },
    {
      "id": "2b426f10",
      "concept": "investigate the",
      "definition": "Here, in this article we investigate the intersection of Vision Transformers and Medical images and proffered an overview of various ViTs based frameworks that are being used by different researchers in order to decipher the obstacles in Medical Computer Vision",
      "context": "Paper: Vision Transformers in Medical Computer Vision -- A Contemplative   Retrospection\nAuthors: Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar\nCategories: eess.IV, cs.CV, cs.LG\nAbstract: Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are imme...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.667653",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2203.15269v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2203.15269v1"
        ]
      }
    },
    {
      "id": "91bd4f14",
      "concept": "demystify several",
      "definition": "Along with this, we also demystify several imaging modalities used in Medical Computer Vision",
      "context": "Paper: Vision Transformers in Medical Computer Vision -- A Contemplative   Retrospection\nAuthors: Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar\nCategories: eess.IV, cs.CV, cs.LG\nAbstract: Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are imme...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.667699",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2203.15269v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2203.15269v1"
        ]
      }
    },
    {
      "id": "c645ae53",
      "concept": "application",
      "definition": "We surveyed the application of Vision transformers in different areas of medical computer vision such as image-based disease classification, anatomical structure segmentation, registration, region-based lesion Detection, captioning, report generation, reconstruction using multiple medical imaging modalities that greatly assist in medical diagnosis and hence treatment process",
      "context": "Paper: Vision Transformers in Medical Computer Vision -- A Contemplative   Retrospection\nAuthors: Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar\nCategories: eess.IV, cs.CV, cs.LG\nAbstract: Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are imme...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.667743",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2203.15269v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2203.15269v1"
        ]
      }
    },
    {
      "id": "f535cddd",
      "concept": "corresponding contextual",
      "definition": "They fail to convey the fundamental multi-vision sensor information from the dataset and the corresponding contextual knowledge properly",
      "context": "Paper: SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for   Large-scale Vision-Language Models\nAuthors: Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee\nCategories: cs.CV\nAbstract: Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.774106",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.12114v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2408.12114v3"
        ]
      }
    },
    {
      "id": "a25b7f7c",
      "concept": "across different",
      "definition": "We generated 6,248 vision-language test samples to investigate multi-vision sensory perception and multi-vision sensory reasoning on physical sensor knowledge proficiency across different formats, covering different types of sensor-related questions",
      "context": "Paper: SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for   Large-scale Vision-Language Models\nAuthors: Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee\nCategories: cs.CV\nAbstract: Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.774444",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.12114v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2408.12114v3"
        ]
      }
    },
    {
      "id": "10410773",
      "concept": "without",
      "definition": "However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteristics of multi-vision sensors",
      "context": "Paper: SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for   Large-scale Vision-Language Models\nAuthors: Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee\nCategories: cs.CV\nAbstract: Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.774665",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.12114v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2408.12114v3"
        ]
      }
    },
    {
      "id": "51085d16",
      "concept": "extents",
      "definition": "The results showed that most models displayed deficiencies in multi-vision sensory reasoning to varying extents",
      "context": "Paper: SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for   Large-scale Vision-Language Models\nAuthors: Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee\nCategories: cs.CV\nAbstract: Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.774835",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.12114v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2408.12114v3"
        ]
      }
    },
    {
      "id": "06f608c3",
      "concept": "lvlms view",
      "definition": "However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteristics of multi-vision sensors",
      "context": "Paper: SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for   Large-scale Vision-Language Models\nAuthors: Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee\nCategories: cs.CV\nAbstract: Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.774976",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.12114v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2408.12114v3"
        ]
      }
    },
    {
      "id": "6561bee7",
      "concept": "they",
      "definition": "They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs",
      "context": "Paper: SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for   Large-scale Vision-Language Models\nAuthors: Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee\nCategories: cs.CV\nAbstract: Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.775107",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.12114v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2408.12114v3"
        ]
      }
    },
    {
      "id": "bafe38d5",
      "concept": "endeavors",
      "definition": "There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images",
      "context": "Paper: SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for   Large-scale Vision-Language Models\nAuthors: Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee\nCategories: cs.CV\nAbstract: Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.775250",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.12114v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2408.12114v3"
        ]
      }
    },
    {
      "id": "eedb5711",
      "concept": "language",
      "definition": "Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs",
      "context": "Paper: SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for   Large-scale Vision-Language Models\nAuthors: Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee\nCategories: cs.CV\nAbstract: Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.775421",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.12114v3",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2408.12114v3"
        ]
      }
    },
    {
      "id": "425abf85",
      "concept": "spark",
      "definition": "In this paper, we aim to establish a multi-vision Sensor Perception And Reasoning benchmarK called SPARK that can reduce the fundamental multi-vision sensor information gap between images and multi-vision sensors",
      "context": "Paper: SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for   Large-scale Vision-Language Models\nAuthors: Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee\nCategories: cs.CV\nAbstract: Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.775587",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.12114v3",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2408.12114v3"
        ]
      }
    },
    {
      "id": "ce5a83e2",
      "concept": "tasks",
      "definition": "They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs",
      "context": "Paper: SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for   Large-scale Vision-Language Models\nAuthors: Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee\nCategories: cs.CV\nAbstract: Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteri...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.775732",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2408.12114v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2408.12114v3"
        ]
      }
    },
    {
      "id": "72598145",
      "concept": "projection commonly",
      "definition": "For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size",
      "context": "Paper: Adapting Computer Vision Algorithms for Omnidirectional Video\nAuthors: Hannes Fassold\nCategories: cs.CV\nAbstract: Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.876704",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.09233v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1907.09233v1"
        ]
      }
    },
    {
      "id": "3762ca53",
      "concept": "level overview",
      "definition": "In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video",
      "context": "Paper: Adapting Computer Vision Algorithms for Omnidirectional Video\nAuthors: Hannes Fassold\nCategories: cs.CV\nAbstract: Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.876793",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.09233v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1907.09233v1"
        ]
      }
    },
    {
      "id": "068a260b",
      "concept": "popular",
      "definition": "Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience",
      "context": "Paper: Adapting Computer Vision Algorithms for Omnidirectional Video\nAuthors: Hannes Fassold\nCategories: cs.CV\nAbstract: Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.876834",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.09233v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1907.09233v1"
        ]
      }
    },
    {
      "id": "4be70d71",
      "concept": "popular because",
      "definition": "Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience",
      "context": "Paper: Adapting Computer Vision Algorithms for Omnidirectional Video\nAuthors: Hannes Fassold\nCategories: cs.CV\nAbstract: Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.876910",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.09233v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1907.09233v1"
        ]
      }
    },
    {
      "id": "b3f1add8",
      "concept": "specifics of",
      "definition": "In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video",
      "context": "Paper: Adapting Computer Vision Algorithms for Omnidirectional Video\nAuthors: Hannes Fassold\nCategories: cs.CV\nAbstract: Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.876954",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.09233v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1907.09233v1"
        ]
      }
    },
    {
      "id": "c57c529a",
      "concept": "highly",
      "definition": "Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience",
      "context": "Paper: Adapting Computer Vision Algorithms for Omnidirectional Video\nAuthors: Hannes Fassold\nCategories: cs.CV\nAbstract: Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.876994",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.09233v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1907.09233v1"
        ]
      }
    },
    {
      "id": "afad4f0e",
      "concept": "several",
      "definition": "For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size",
      "context": "Paper: Adapting Computer Vision Algorithms for Omnidirectional Video\nAuthors: Hannes Fassold\nCategories: cs.CV\nAbstract: Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.877033",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.09233v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1907.09233v1"
        ]
      }
    },
    {
      "id": "61df2fe7",
      "concept": "computer",
      "definition": "For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size",
      "context": "Paper: Adapting Computer Vision Algorithms for Omnidirectional Video\nAuthors: Hannes Fassold\nCategories: cs.CV\nAbstract: Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.877072",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.09233v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1907.09233v1"
        ]
      }
    },
    {
      "id": "6fee8f4b",
      "concept": "size",
      "definition": "For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size",
      "context": "Paper: Adapting Computer Vision Algorithms for Omnidirectional Video\nAuthors: Hannes Fassold\nCategories: cs.CV\nAbstract: Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.877114",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.09233v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1907.09233v1"
        ]
      }
    },
    {
      "id": "b268e189",
      "concept": "of these",
      "definition": "In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video",
      "context": "Paper: Adapting Computer Vision Algorithms for Omnidirectional Video\nAuthors: Hannes Fassold\nCategories: cs.CV\nAbstract: Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:47.877156",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1907.09233v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1907.09233v1"
        ]
      }
    },
    {
      "id": "934fb1ff",
      "concept": "systems",
      "definition": "A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime",
      "context": "Paper: Some Insights into Lifelong Reinforcement Learning Systems\nAuthors: Changjian Li\nCategories: cs.LG, stat.ML\nAbstract: A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.622456",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2001.09608v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2001.09608v1"
        ]
      }
    },
    {
      "id": "6d1a4dbd",
      "concept": "through trail",
      "definition": "A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime",
      "context": "Paper: Some Insights into Lifelong Reinforcement Learning Systems\nAuthors: Changjian Li\nCategories: cs.LG, stat.ML\nAbstract: A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.622574",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2001.09608v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2001.09608v1"
        ]
      }
    },
    {
      "id": "653f474c",
      "concept": "reinforcement",
      "definition": "A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime",
      "context": "Paper: Some Insights into Lifelong Reinforcement Learning Systems\nAuthors: Changjian Li\nCategories: cs.LG, stat.ML\nAbstract: A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.622617",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2001.09608v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2001.09608v1"
        ]
      }
    },
    {
      "id": "a532e475",
      "concept": "into lifelong",
      "definition": "Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system",
      "context": "Paper: Some Insights into Lifelong Reinforcement Learning Systems\nAuthors: Changjian Li\nCategories: cs.LG, stat.ML\nAbstract: A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.622666",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2001.09608v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2001.09608v1"
        ]
      }
    },
    {
      "id": "35f915fd",
      "concept": "the traditional",
      "definition": "In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system",
      "context": "Paper: Some Insights into Lifelong Reinforcement Learning Systems\nAuthors: Changjian Li\nCategories: cs.LG, stat.ML\nAbstract: A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.622721",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2001.09608v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2001.09608v1"
        ]
      }
    },
    {
      "id": "d7ed6f65",
      "concept": "over",
      "definition": "A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime",
      "context": "Paper: Some Insights into Lifelong Reinforcement Learning Systems\nAuthors: Changjian Li\nCategories: cs.LG, stat.ML\nAbstract: A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.622798",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2001.09608v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2001.09608v1"
        ]
      }
    },
    {
      "id": "7244990d",
      "concept": "learning system",
      "definition": "A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime",
      "context": "Paper: Some Insights into Lifelong Reinforcement Learning Systems\nAuthors: Changjian Li\nCategories: cs.LG, stat.ML\nAbstract: A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.622840",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2001.09608v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2001.09608v1"
        ]
      }
    },
    {
      "id": "0ce57deb",
      "concept": "some insights",
      "definition": "Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system",
      "context": "Paper: Some Insights into Lifelong Reinforcement Learning Systems\nAuthors: Changjian Li\nCategories: cs.LG, stat.ML\nAbstract: A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.622888",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2001.09608v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2001.09608v1"
        ]
      }
    },
    {
      "id": "9f2cc070",
      "concept": "learn",
      "definition": "A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime",
      "context": "Paper: Some Insights into Lifelong Reinforcement Learning Systems\nAuthors: Changjian Li\nCategories: cs.LG, stat.ML\nAbstract: A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.622973",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2001.09608v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2001.09608v1"
        ]
      }
    },
    {
      "id": "a53c9272",
      "concept": "prototype",
      "definition": "Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system",
      "context": "Paper: Some Insights into Lifelong Reinforcement Learning Systems\nAuthors: Changjian Li\nCategories: cs.LG, stat.ML\nAbstract: A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.623034",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2001.09608v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2001.09608v1"
        ]
      }
    },
    {
      "id": "0e067f88",
      "concept": "systems",
      "definition": "We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics",
      "context": "Paper: Counterexample-Guided Repair of Reinforcement Learning Systems Using   Safety Critics\nAuthors: David Boetius, Stefan Leue\nCategories: cs.LG, cs.LO\nAbstract: Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.724201",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2405.15430v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2405.15430v1"
        ]
      }
    },
    {
      "id": "636e8b65",
      "concept": "we may",
      "definition": "To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour",
      "context": "Paper: Counterexample-Guided Repair of Reinforcement Learning Systems Using   Safety Critics\nAuthors: David Boetius, Stefan Leue\nCategories: cs.LG, cs.LO\nAbstract: Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.724347",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2405.15430v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2405.15430v1"
        ]
      }
    },
    {
      "id": "4a7edc9a",
      "concept": "reinforcement",
      "definition": "Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints",
      "context": "Paper: Counterexample-Guided Repair of Reinforcement Learning Systems Using   Safety Critics\nAuthors: David Boetius, Stefan Leue\nCategories: cs.LG, cs.LO\nAbstract: Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.724437",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2405.15430v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2405.15430v1"
        ]
      }
    },
    {
      "id": "0ba89b96",
      "concept": "of reinforcement",
      "definition": "Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints",
      "context": "Paper: Counterexample-Guided Repair of Reinforcement Learning Systems Using   Safety Critics\nAuthors: David Boetius, Stefan Leue\nCategories: cs.LG, cs.LO\nAbstract: Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.724491",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2405.15430v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2405.15430v1"
        ]
      }
    },
    {
      "id": "e0612696",
      "concept": "costly",
      "definition": "To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour",
      "context": "Paper: Counterexample-Guided Repair of Reinforcement Learning Systems Using   Safety Critics\nAuthors: David Boetius, Stefan Leue\nCategories: cs.LG, cs.LO\nAbstract: Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.724550",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2405.15430v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2405.15430v1"
        ]
      }
    },
    {
      "id": "6d10c22c",
      "concept": "obviate",
      "definition": "To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour",
      "context": "Paper: Counterexample-Guided Repair of Reinforcement Learning Systems Using   Safety Critics\nAuthors: David Boetius, Stefan Leue\nCategories: cs.LG, cs.LO\nAbstract: Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.724593",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2405.15430v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2405.15430v1"
        ]
      }
    },
    {
      "id": "1bcc35f0",
      "concept": "desire",
      "definition": "To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour",
      "context": "Paper: Counterexample-Guided Repair of Reinforcement Learning Systems Using   Safety Critics\nAuthors: David Boetius, Stefan Leue\nCategories: cs.LG, cs.LO\nAbstract: Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.724638",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2405.15430v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2405.15430v1"
        ]
      }
    },
    {
      "id": "2e6a5e8d",
      "concept": "unsafe",
      "definition": "To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour",
      "context": "Paper: Counterexample-Guided Repair of Reinforcement Learning Systems Using   Safety Critics\nAuthors: David Boetius, Stefan Leue\nCategories: cs.LG, cs.LO\nAbstract: Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.724683",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2405.15430v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2405.15430v1"
        ]
      }
    },
    {
      "id": "f71ebe09",
      "concept": "guided repair",
      "definition": "We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics",
      "context": "Paper: Counterexample-Guided Repair of Reinforcement Learning Systems Using   Safety Critics\nAuthors: David Boetius, Stefan Leue\nCategories: cs.LG, cs.LO\nAbstract: Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.724767",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2405.15430v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2405.15430v1"
        ]
      }
    },
    {
      "id": "4ea812f7",
      "concept": "previously trained",
      "definition": "To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour",
      "context": "Paper: Counterexample-Guided Repair of Reinforcement Learning Systems Using   Safety Critics\nAuthors: David Boetius, Stefan Leue\nCategories: cs.LG, cs.LO\nAbstract: Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.724830",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2405.15430v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2405.15430v1"
        ]
      }
    },
    {
      "id": "9a7c347e",
      "concept": "start",
      "definition": "We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.826611",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "8506ed54",
      "concept": "research",
      "definition": "In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.826659",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "c23e83b9",
      "concept": "and state",
      "definition": "In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.826684",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "c17ce4f2",
      "concept": "segmentation",
      "definition": "(i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.826710",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "b267b716",
      "concept": "available datasets",
      "definition": "Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.826737",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "f652d1e1",
      "concept": "research directions",
      "definition": "Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.826765",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "487d0ccf",
      "concept": "state-of",
      "definition": "In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.826789",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "380c30c6",
      "concept": "image",
      "definition": "(i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.826814",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "0b4df765",
      "concept": "recent",
      "definition": "Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.826836",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "1b1a836e",
      "concept": "and deep",
      "definition": "We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning",
      "context": "Paper: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\nAuthors: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki\nCategories: cs.CV, cs.AI\nAbstract: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehendin...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.826859",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.11510v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2108.11510v1"
        ]
      }
    },
    {
      "id": "47a6a8d7",
      "concept": "they may",
      "definition": "They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge",
      "context": "Paper: Causal Reinforcement Learning: A Survey\nAuthors: Zhihong Deng, Jing Jiang, Guodong Long\nCategories: cs.LG, cs.AI\nAbstract: Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.928561",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.01452v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2307.01452v2"
        ]
      }
    },
    {
      "id": "d1086a98",
      "concept": "acquired knowledge",
      "definition": "They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge",
      "context": "Paper: Causal Reinforcement Learning: A Survey\nAuthors: Zhihong Deng, Jing Jiang, Guodong Long\nCategories: cs.LG, cs.AI\nAbstract: Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.928663",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.01452v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2307.01452v2"
        ]
      }
    },
    {
      "id": "2235e1a0",
      "concept": "essential paradigm",
      "definition": "Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty",
      "context": "Paper: Causal Reinforcement Learning: A Survey\nAuthors: Zhihong Deng, Jing Jiang, Guodong Long\nCategories: cs.LG, cs.AI\nAbstract: Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.928711",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.01452v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2307.01452v2"
        ]
      }
    },
    {
      "id": "c2718cce",
      "concept": "sequential decision",
      "definition": "Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty",
      "context": "Paper: Causal Reinforcement Learning: A Survey\nAuthors: Zhihong Deng, Jing Jiang, Guodong Long\nCategories: cs.LG, cs.AI\nAbstract: Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.928757",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.01452v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2307.01452v2"
        ]
      }
    },
    {
      "id": "044e9e36",
      "concept": "introduce",
      "definition": "We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learning",
      "context": "Paper: Causal Reinforcement Learning: A Survey\nAuthors: Zhihong Deng, Jing Jiang, Guodong Long\nCategories: cs.LG, cs.AI\nAbstract: Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.928806",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.01452v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2307.01452v2"
        ]
      }
    },
    {
      "id": "db8987a0",
      "concept": "they",
      "definition": "They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge",
      "context": "Paper: Causal Reinforcement Learning: A Survey\nAuthors: Zhihong Deng, Jing Jiang, Guodong Long\nCategories: cs.LG, cs.AI\nAbstract: Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.928850",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.01452v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2307.01452v2"
        ]
      }
    },
    {
      "id": "4a22de62",
      "concept": "recent",
      "definition": "Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging",
      "context": "Paper: Causal Reinforcement Learning: A Survey\nAuthors: Zhihong Deng, Jing Jiang, Guodong Long\nCategories: cs.LG, cs.AI\nAbstract: Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.928893",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.01452v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2307.01452v2"
        ]
      }
    },
    {
      "id": "35681036",
      "concept": "the main",
      "definition": "One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions",
      "context": "Paper: Causal Reinforcement Learning: A Survey\nAuthors: Zhihong Deng, Jing Jiang, Guodong Long\nCategories: cs.LG, cs.AI\nAbstract: Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.928936",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.01452v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2307.01452v2"
        ]
      }
    },
    {
      "id": "204b2b12",
      "concept": "obstacles is",
      "definition": "One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions",
      "context": "Paper: Causal Reinforcement Learning: A Survey\nAuthors: Zhihong Deng, Jing Jiang, Guodong Long\nCategories: cs.LG, cs.AI\nAbstract: Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.928980",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.01452v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2307.01452v2"
        ]
      }
    },
    {
      "id": "2511a83b",
      "concept": "real world",
      "definition": "Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging",
      "context": "Paper: Causal Reinforcement Learning: A Survey\nAuthors: Zhihong Deng, Jing Jiang, Guodong Long\nCategories: cs.LG, cs.AI\nAbstract: Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for th...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:49.929023",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2307.01452v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2307.01452v2"
        ]
      }
    },
    {
      "id": "3d033625",
      "concept": "to achieve",
      "definition": "In this paper, we conclude the state of this exciting field, by comparing the classical distributed deep reinforcement learning methods, and studying important components to achieve efficient distributed learning, covering single player single agent distributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learning",
      "context": "Paper: Distributed Deep Reinforcement Learning: A Survey and A Multi-Player   Multi-Agent Learning Toolbox\nAuthors: Qiyue Yin, Tongtong Yu, Shengqi Shen\nCategories: cs.LG, cs.AI, cs.MA\nAbstract: With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, d...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.032738",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2212.00253v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2212.00253v1"
        ]
      }
    },
    {
      "id": "77a3b01b",
      "concept": "furthermore",
      "definition": "Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modifications of their non-distributed versions",
      "context": "Paper: Distributed Deep Reinforcement Learning: A Survey and A Multi-Player   Multi-Agent Learning Toolbox\nAuthors: Qiyue Yin, Tongtong Yu, Shengqi Shen\nCategories: cs.LG, cs.AI, cs.MA\nAbstract: With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, d...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.032886",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2212.00253v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2212.00253v1"
        ]
      }
    },
    {
      "id": "a59fde98",
      "concept": "and weaknesses",
      "definition": "By analyzing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex games",
      "context": "Paper: Distributed Deep Reinforcement Learning: A Survey and A Multi-Player   Multi-Agent Learning Toolbox\nAuthors: Qiyue Yin, Tongtong Yu, Shengqi Shen\nCategories: cs.LG, cs.AI, cs.MA\nAbstract: With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, d...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.032960",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2212.00253v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2212.00253v1"
        ]
      }
    },
    {
      "id": "26c0e830",
      "concept": "without",
      "definition": "Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modifications of their non-distributed versions",
      "context": "Paper: Distributed Deep Reinforcement Learning: A Survey and A Multi-Player   Multi-Agent Learning Toolbox\nAuthors: Qiyue Yin, Tongtong Yu, Shengqi Shen\nCategories: cs.LG, cs.AI, cs.MA\nAbstract: With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, d...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.033017",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2212.00253v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2212.00253v1"
        ]
      }
    },
    {
      "id": "520f3969",
      "concept": "sequential decision",
      "definition": "With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems",
      "context": "Paper: Distributed Deep Reinforcement Learning: A Survey and A Multi-Player   Multi-Agent Learning Toolbox\nAuthors: Qiyue Yin, Tongtong Yu, Shengqi Shen\nCategories: cs.LG, cs.AI, cs.MA\nAbstract: With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, d...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.033071",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2212.00253v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2212.00253v1"
        ]
      }
    },
    {
      "id": "a74273cf",
      "concept": "their strengths",
      "definition": "By analyzing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex games",
      "context": "Paper: Distributed Deep Reinforcement Learning: A Survey and A Multi-Player   Multi-Agent Learning Toolbox\nAuthors: Qiyue Yin, Tongtong Yu, Shengqi Shen\nCategories: cs.LG, cs.AI, cs.MA\nAbstract: With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, d...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.033128",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2212.00253v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2212.00253v1"
        ]
      }
    },
    {
      "id": "c2e48647",
      "concept": "in various",
      "definition": "Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, distributed deep reinforcement learning has shown its potential in various applications, such as human-computer gaming, and intelligent transportation",
      "context": "Paper: Distributed Deep Reinforcement Learning: A Survey and A Multi-Player   Multi-Agent Learning Toolbox\nAuthors: Qiyue Yin, Tongtong Yu, Shengqi Shen\nCategories: cs.LG, cs.AI, cs.MA\nAbstract: With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, d...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.033182",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2212.00253v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2212.00253v1"
        ]
      }
    },
    {
      "id": "6c144108",
      "concept": "components",
      "definition": "In this paper, we conclude the state of this exciting field, by comparing the classical distributed deep reinforcement learning methods, and studying important components to achieve efficient distributed learning, covering single player single agent distributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learning",
      "context": "Paper: Distributed Deep Reinforcement Learning: A Survey and A Multi-Player   Multi-Agent Learning Toolbox\nAuthors: Qiyue Yin, Tongtong Yu, Shengqi Shen\nCategories: cs.LG, cs.AI, cs.MA\nAbstract: With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, d...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.033235",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2212.00253v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2212.00253v1"
        ]
      }
    },
    {
      "id": "a3854511",
      "concept": "analyzing",
      "definition": "By analyzing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex games",
      "context": "Paper: Distributed Deep Reinforcement Learning: A Survey and A Multi-Player   Multi-Agent Learning Toolbox\nAuthors: Qiyue Yin, Tongtong Yu, Shengqi Shen\nCategories: cs.LG, cs.AI, cs.MA\nAbstract: With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, d...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.033291",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2212.00253v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2212.00253v1"
        ]
      }
    },
    {
      "id": "09f97d11",
      "concept": "of the",
      "definition": "Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modifications of their non-distributed versions",
      "context": "Paper: Distributed Deep Reinforcement Learning: A Survey and A Multi-Player   Multi-Agent Learning Toolbox\nAuthors: Qiyue Yin, Tongtong Yu, Shengqi Shen\nCategories: cs.LG, cs.AI, cs.MA\nAbstract: With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, d...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.033344",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2212.00253v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2212.00253v1"
        ]
      }
    },
    {
      "id": "f640e598",
      "concept": "research",
      "definition": "We also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progress",
      "context": "Paper: Transfer Learning in Deep Reinforcement Learning: A Survey\nAuthors: Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the effic...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.136698",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2009.07888v7",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2009.07888v7"
        ]
      }
    },
    {
      "id": "38dcaf41",
      "concept": "research progress",
      "definition": "We also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progress",
      "context": "Paper: Transfer Learning in Deep Reinforcement Learning: A Survey\nAuthors: Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the effic...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.136778",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2009.07888v7",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2009.07888v7"
        ]
      }
    },
    {
      "id": "3baa7af7",
      "concept": "analyze",
      "definition": "Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications",
      "context": "Paper: Transfer Learning in Deep Reinforcement Learning: A Survey\nAuthors: Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the effic...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.136812",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2009.07888v7",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2009.07888v7"
        ]
      }
    },
    {
      "id": "12409f77",
      "concept": "sequential decision",
      "definition": "Reinforcement learning is a learning paradigm for solving sequential decision-making problems",
      "context": "Paper: Transfer Learning in Deep Reinforcement Learning: A Survey\nAuthors: Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the effic...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.136843",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2009.07888v7",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2009.07888v7"
        ]
      }
    },
    {
      "id": "cad7f0a6",
      "concept": "prospects of",
      "definition": "Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process",
      "context": "Paper: Transfer Learning in Deep Reinforcement Learning: A Survey\nAuthors: Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the effic...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.136873",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2009.07888v7",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2009.07888v7"
        ]
      }
    },
    {
      "id": "90fece65",
      "concept": "investigate the",
      "definition": "In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning",
      "context": "Paper: Transfer Learning in Deep Reinforcement Learning: A Survey\nAuthors: Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the effic...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.136903",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2009.07888v7",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2009.07888v7"
        ]
      }
    },
    {
      "id": "803faac8",
      "concept": "state-of",
      "definition": "Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications",
      "context": "Paper: Transfer Learning in Deep Reinforcement Learning: A Survey\nAuthors: Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the effic...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.136932",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2009.07888v7",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2009.07888v7"
        ]
      }
    },
    {
      "id": "e8465b04",
      "concept": "we systematically",
      "definition": "In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning",
      "context": "Paper: Transfer Learning in Deep Reinforcement Learning: A Survey\nAuthors: Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the effic...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.136960",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2009.07888v7",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2009.07888v7"
        ]
      }
    },
    {
      "id": "5c0689a6",
      "concept": "learning paradigm",
      "definition": "Reinforcement learning is a learning paradigm for solving sequential decision-making problems",
      "context": "Paper: Transfer Learning in Deep Reinforcement Learning: A Survey\nAuthors: Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the effic...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.136987",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2009.07888v7",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2009.07888v7"
        ]
      }
    },
    {
      "id": "f0ad6ace",
      "concept": "recent",
      "definition": "Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks",
      "context": "Paper: Transfer Learning in Deep Reinforcement Learning: A Survey\nAuthors: Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the effic...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.137014",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2009.07888v7",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2009.07888v7"
        ]
      }
    },
    {
      "id": "461efd26",
      "concept": "reinforcement",
      "definition": "We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game",
      "context": "Paper: Memory-two strategies forming symmetric mutual reinforcement learning   equilibrium in repeated prisoners' dilemma game\nAuthors: Masahiko Ueda\nCategories: physics.soc-ph, cs.GT\nAbstract: We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria for...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.237915",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.03258v2",
        "extraction_confidence": 0.8999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2108.03258v2"
        ]
      }
    },
    {
      "id": "27b95fbf",
      "concept": "both",
      "definition": "We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game",
      "context": "Paper: Memory-two strategies forming symmetric mutual reinforcement learning   equilibrium in repeated prisoners' dilemma game\nAuthors: Masahiko Ueda\nCategories: physics.soc-ph, cs.GT\nAbstract: We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria for...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.238007",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.03258v2",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2108.03258v2"
        ]
      }
    },
    {
      "id": "5ef9576f",
      "concept": "game",
      "definition": "We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game",
      "context": "Paper: Memory-two strategies forming symmetric mutual reinforcement learning   equilibrium in repeated prisoners' dilemma game\nAuthors: Masahiko Ueda\nCategories: physics.soc-ph, cs.GT\nAbstract: We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria for...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.238054",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.03258v2",
        "extraction_confidence": 0.8999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2108.03258v2"
        ]
      }
    },
    {
      "id": "fb9d5633",
      "concept": "investigate",
      "definition": "We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game",
      "context": "Paper: Memory-two strategies forming symmetric mutual reinforcement learning   equilibrium in repeated prisoners' dilemma game\nAuthors: Masahiko Ueda\nCategories: physics.soc-ph, cs.GT\nAbstract: We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria for...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.238088",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.03258v2",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2108.03258v2"
        ]
      }
    },
    {
      "id": "60e4e415",
      "concept": "learn",
      "definition": "We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game",
      "context": "Paper: Memory-two strategies forming symmetric mutual reinforcement learning   equilibrium in repeated prisoners' dilemma game\nAuthors: Masahiko Ueda\nCategories: physics.soc-ph, cs.GT\nAbstract: We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria for...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.238124",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.03258v2",
        "extraction_confidence": 0.8999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2108.03258v2"
        ]
      }
    },
    {
      "id": "9795611e",
      "concept": "memory-two",
      "definition": "We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game",
      "context": "Paper: Memory-two strategies forming symmetric mutual reinforcement learning   equilibrium in repeated prisoners' dilemma game\nAuthors: Masahiko Ueda\nCategories: physics.soc-ph, cs.GT\nAbstract: We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria for...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.238160",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.03258v2",
        "extraction_confidence": 0.8999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2108.03258v2"
        ]
      }
    },
    {
      "id": "9d687590",
      "concept": "symmetric",
      "definition": "We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game",
      "context": "Paper: Memory-two strategies forming symmetric mutual reinforcement learning   equilibrium in repeated prisoners' dilemma game\nAuthors: Masahiko Ueda\nCategories: physics.soc-ph, cs.GT\nAbstract: We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria for...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.238196",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.03258v2",
        "extraction_confidence": 0.8999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2108.03258v2"
        ]
      }
    },
    {
      "id": "99f8eed4",
      "concept": "repeated",
      "definition": "We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game",
      "context": "Paper: Memory-two strategies forming symmetric mutual reinforcement learning   equilibrium in repeated prisoners' dilemma game\nAuthors: Masahiko Ueda\nCategories: physics.soc-ph, cs.GT\nAbstract: We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria for...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.238219",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.03258v2",
        "extraction_confidence": 0.8999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2108.03258v2"
        ]
      }
    },
    {
      "id": "4195fb8d",
      "concept": "learn the",
      "definition": "We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game",
      "context": "Paper: Memory-two strategies forming symmetric mutual reinforcement learning   equilibrium in repeated prisoners' dilemma game\nAuthors: Masahiko Ueda\nCategories: physics.soc-ph, cs.GT\nAbstract: We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria for...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.238238",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.03258v2",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2108.03258v2"
        ]
      }
    },
    {
      "id": "064771a1",
      "concept": "of mutual",
      "definition": "We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game",
      "context": "Paper: Memory-two strategies forming symmetric mutual reinforcement learning   equilibrium in repeated prisoners' dilemma game\nAuthors: Masahiko Ueda\nCategories: physics.soc-ph, cs.GT\nAbstract: We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria for...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.238269",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2108.03258v2",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2108.03258v2"
        ]
      }
    },
    {
      "id": "1200747d",
      "concept": "three-factor",
      "definition": "The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules",
      "context": "Paper: Implementing Online Reinforcement Learning with Temporal Neural Networks\nAuthors: James E. Smith\nCategories: cs.NE, 68T07, I.2.6\nAbstract: A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.339088",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.05437v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2204.05437v1"
        ]
      }
    },
    {
      "id": "834aa77b",
      "concept": "reinforcement",
      "definition": "A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation",
      "context": "Paper: Implementing Online Reinforcement Learning with Temporal Neural Networks\nAuthors: James E. Smith\nCategories: cs.NE, 68T07, I.2.6\nAbstract: A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.339178",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.05437v1",
        "extraction_confidence": 0.8999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2204.05437v1"
        ]
      }
    },
    {
      "id": "01a89dcd",
      "concept": "backend",
      "definition": "The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning",
      "context": "Paper: Implementing Online Reinforcement Learning with Temporal Neural Networks\nAuthors: James E. Smith\nCategories: cs.NE, 68T07, I.2.6\nAbstract: A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.339209",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.05437v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2204.05437v1"
        ]
      }
    },
    {
      "id": "6921726d",
      "concept": "learning is",
      "definition": "A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation",
      "context": "Paper: Implementing Online Reinforcement Learning with Temporal Neural Networks\nAuthors: James E. Smith\nCategories: cs.NE, 68T07, I.2.6\nAbstract: A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.339234",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.05437v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2204.05437v1"
        ]
      }
    },
    {
      "id": "3e0f6a81",
      "concept": "balancing an",
      "definition": "As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulation",
      "context": "Paper: Implementing Online Reinforcement Learning with Temporal Neural Networks\nAuthors: James E. Smith\nCategories: cs.NE, 68T07, I.2.6\nAbstract: A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.339262",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.05437v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2204.05437v1"
        ]
      }
    },
    {
      "id": "bcf0fd1d",
      "concept": "pole",
      "definition": "As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulation",
      "context": "Paper: Implementing Online Reinforcement Learning with Temporal Neural Networks\nAuthors: James E. Smith\nCategories: cs.NE, 68T07, I.2.6\nAbstract: A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.339299",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.05437v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2204.05437v1"
        ]
      }
    },
    {
      "id": "8da951ac",
      "concept": "prototype",
      "definition": "As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulation",
      "context": "Paper: Implementing Online Reinforcement Learning with Temporal Neural Networks\nAuthors: James E. Smith\nCategories: cs.NE, 68T07, I.2.6\nAbstract: A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.339331",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.05437v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2204.05437v1"
        ]
      }
    },
    {
      "id": "8876aeeb",
      "concept": "learning system",
      "definition": "The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning",
      "context": "Paper: Implementing Online Reinforcement Learning with Temporal Neural Networks\nAuthors: James E. Smith\nCategories: cs.NE, 68T07, I.2.6\nAbstract: A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.339371",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.05437v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2204.05437v1"
        ]
      }
    },
    {
      "id": "e7576a17",
      "concept": "clustering and",
      "definition": "The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning",
      "context": "Paper: Implementing Online Reinforcement Learning with Temporal Neural Networks\nAuthors: James E. Smith\nCategories: cs.NE, 68T07, I.2.6\nAbstract: A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.339409",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.05437v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2204.05437v1"
        ]
      }
    },
    {
      "id": "dc1c35d9",
      "concept": "cart-pole",
      "definition": "As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulation",
      "context": "Paper: Implementing Online Reinforcement Learning with Temporal Neural Networks\nAuthors: James E. Smith\nCategories: cs.NE, 68T07, I.2.6\nAbstract: A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.339435",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.05437v1",
        "extraction_confidence": 0.7,
        "parent_sources": [
          "https://arxiv.org/abs/2204.05437v1"
        ]
      }
    },
    {
      "id": "9ccc75bc",
      "concept": "systems",
      "definition": "Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world",
      "context": "Paper: Deep Reinforcement Learning for Conversational AI\nAuthors: Mahipal Jadeja, Neelanshi Varia, Agam Shah\nCategories: cs.AI\nAbstract: Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.440651",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1709.05067v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1709.05067v1"
        ]
      }
    },
    {
      "id": "6d79a13c",
      "concept": "artificial intelligence",
      "definition": "Deep reinforcement learning is revolutionizing the artificial intelligence field",
      "context": "Paper: Deep Reinforcement Learning for Conversational AI\nAuthors: Mahipal Jadeja, Neelanshi Varia, Agam Shah\nCategories: cs.AI\nAbstract: Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.440734",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1709.05067v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1709.05067v1"
        ]
      }
    },
    {
      "id": "4afd9db1",
      "concept": "conversational ai",
      "definition": "Key challenges related to the implementation of reinforcement learning in conversational AI domain are identified as well as discussed in detail",
      "context": "Paper: Deep Reinforcement Learning for Conversational AI\nAuthors: Mahipal Jadeja, Neelanshi Varia, Agam Shah\nCategories: cs.AI\nAbstract: Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.440778",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1709.05067v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1709.05067v1"
        ]
      }
    },
    {
      "id": "d9429537",
      "concept": "and supervised",
      "definition": "In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement learning and supervised learning and models for implementation of reinforcement are discussed",
      "context": "Paper: Deep Reinforcement Learning for Conversational AI\nAuthors: Mahipal Jadeja, Neelanshi Varia, Agam Shah\nCategories: cs.AI\nAbstract: Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.440824",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1709.05067v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1709.05067v1"
        ]
      }
    },
    {
      "id": "ae3022ca",
      "concept": "discussed in",
      "definition": "Key challenges related to the implementation of reinforcement learning in conversational AI domain are identified as well as discussed in detail",
      "context": "Paper: Deep Reinforcement Learning for Conversational AI\nAuthors: Mahipal Jadeja, Neelanshi Varia, Agam Shah\nCategories: cs.AI\nAbstract: Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.440853",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1709.05067v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1709.05067v1"
        ]
      }
    },
    {
      "id": "7081a93d",
      "concept": "in summary",
      "definition": "In summary, this paper discusses key aspects of deep reinforcement learning which are crucial for designing an efficient conversational AI",
      "context": "Paper: Deep Reinforcement Learning for Conversational AI\nAuthors: Mahipal Jadeja, Neelanshi Varia, Agam Shah\nCategories: cs.AI\nAbstract: Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.440876",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1709.05067v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1709.05067v1"
        ]
      }
    },
    {
      "id": "8d2b900f",
      "concept": "of the",
      "definition": "Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world",
      "context": "Paper: Deep Reinforcement Learning for Conversational AI\nAuthors: Mahipal Jadeja, Neelanshi Varia, Agam Shah\nCategories: cs.AI\nAbstract: Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.440897",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1709.05067v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1709.05067v1"
        ]
      }
    },
    {
      "id": "46181b2d",
      "concept": "this paper",
      "definition": "In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement learning and supervised learning and models for implementation of reinforcement are discussed",
      "context": "Paper: Deep Reinforcement Learning for Conversational AI\nAuthors: Mahipal Jadeja, Neelanshi Varia, Agam Shah\nCategories: cs.AI\nAbstract: Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.440934",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1709.05067v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1709.05067v1"
        ]
      }
    },
    {
      "id": "5b77bdc5",
      "concept": "deep",
      "definition": "Deep reinforcement learning is revolutionizing the artificial intelligence field",
      "context": "Paper: Deep Reinforcement Learning for Conversational AI\nAuthors: Mahipal Jadeja, Neelanshi Varia, Agam Shah\nCategories: cs.AI\nAbstract: Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.440957",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1709.05067v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1709.05067v1"
        ]
      }
    },
    {
      "id": "deabcb8f",
      "concept": "tasks",
      "definition": "It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games",
      "context": "Paper: Deep Reinforcement Learning for Conversational AI\nAuthors: Mahipal Jadeja, Neelanshi Varia, Agam Shah\nCategories: cs.AI\nAbstract: Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.440978",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1709.05067v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1709.05067v1"
        ]
      }
    },
    {
      "id": "47488521",
      "concept": "within",
      "definition": "Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late",
      "context": "Paper: On the Opportunities and Challenges of Offline Reinforcement Learning   for Recommender Systems\nAuthors: Xiaocong Chen, Siyu Wang, Julian McAuley\nCategories: cs.IR, cs.AI\nAbstract: Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.545044",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2308.11336v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2308.11336v1"
        ]
      }
    },
    {
      "id": "f8cfe176",
      "concept": "furthermore",
      "definition": "Furthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field",
      "context": "Paper: On the Opportunities and Challenges of Offline Reinforcement Learning   for Recommender Systems\nAuthors: Xiaocong Chen, Siyu Wang, Julian McAuley\nCategories: cs.IR, cs.AI\nAbstract: Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.545170",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2308.11336v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2308.11336v1"
        ]
      }
    },
    {
      "id": "8d08586a",
      "concept": "systems",
      "definition": "Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late",
      "context": "Paper: On the Opportunities and Challenges of Offline Reinforcement Learning   for Recommender Systems\nAuthors: Xiaocong Chen, Siyu Wang, Julian McAuley\nCategories: cs.IR, cs.AI\nAbstract: Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.545209",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2308.11336v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2308.11336v1"
        ]
      }
    },
    {
      "id": "693a400f",
      "concept": "research",
      "definition": "Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late",
      "context": "Paper: On the Opportunities and Challenges of Offline Reinforcement Learning   for Recommender Systems\nAuthors: Xiaocong Chen, Siyu Wang, Julian McAuley\nCategories: cs.IR, cs.AI\nAbstract: Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.545246",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2308.11336v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2308.11336v1"
        ]
      }
    },
    {
      "id": "a2875344",
      "concept": "introduce",
      "definition": "This survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domain",
      "context": "Paper: On the Opportunities and Challenges of Offline Reinforcement Learning   for Recommender Systems\nAuthors: Xiaocong Chen, Siyu Wang, Julian McAuley\nCategories: cs.IR, cs.AI\nAbstract: Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.545277",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2308.11336v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2308.11336v1"
        ]
      }
    },
    {
      "id": "59a94040",
      "concept": "given",
      "definition": "Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly",
      "context": "Paper: On the Opportunities and Challenges of Offline Reinforcement Learning   for Recommender Systems\nAuthors: Xiaocong Chen, Siyu Wang, Julian McAuley\nCategories: cs.IR, cs.AI\nAbstract: Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.545308",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2308.11336v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2308.11336v1"
        ]
      }
    },
    {
      "id": "3fec5ae7",
      "concept": "possess extensive",
      "definition": "Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly",
      "context": "Paper: On the Opportunities and Challenges of Offline Reinforcement Learning   for Recommender Systems\nAuthors: Xiaocong Chen, Siyu Wang, Julian McAuley\nCategories: cs.IR, cs.AI\nAbstract: Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.545375",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2308.11336v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2308.11336v1"
        ]
      }
    },
    {
      "id": "c9ace212",
      "concept": "recent",
      "definition": "Recent strides in offline reinforcement learning present a new perspective",
      "context": "Paper: On the Opportunities and Challenges of Offline Reinforcement Learning   for Recommender Systems\nAuthors: Xiaocong Chen, Siyu Wang, Julian McAuley\nCategories: cs.IR, cs.AI\nAbstract: Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.545442",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2308.11336v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2308.11336v1"
        ]
      }
    },
    {
      "id": "81b56020",
      "concept": "aligns seamlessly",
      "definition": "Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly",
      "context": "Paper: On the Opportunities and Challenges of Offline Reinforcement Learning   for Recommender Systems\nAuthors: Xiaocong Chen, Siyu Wang, Julian McAuley\nCategories: cs.IR, cs.AI\nAbstract: Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.545501",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2308.11336v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2308.11336v1"
        ]
      }
    },
    {
      "id": "36519736",
      "concept": "drawback",
      "definition": "However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature",
      "context": "Paper: On the Opportunities and Challenges of Offline Reinforcement Learning   for Recommender Systems\nAuthors: Xiaocong Chen, Siyu Wang, Julian McAuley\nCategories: cs.IR, cs.AI\nAbstract: Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-ba...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.545553",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2308.11336v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2308.11336v1"
        ]
      }
    },
    {
      "id": "32b37c90",
      "concept": "solve problems",
      "definition": "Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.647306",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "29f18fec",
      "concept": "furthermore",
      "definition": "Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policies",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.647391",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "5261b8de",
      "concept": "research",
      "definition": "Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.647436",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "d271790e",
      "concept": "within",
      "definition": "From exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth view",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.647482",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "b1542f2b",
      "concept": "formalize and",
      "definition": "In this paper, we will formalize and analyze generalization in deep reinforcement learning",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.647522",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "bdf65304",
      "concept": "analyze",
      "definition": "In this paper, we will formalize and analyze generalization in deep reinforcement learning",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.647557",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "bece721a",
      "concept": "questions the",
      "definition": "While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.647592",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "e328c618",
      "concept": "still",
      "definition": "While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.647630",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "eef24026",
      "concept": "analyzing",
      "definition": "Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.647716",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "290f9813",
      "concept": "ongoing",
      "definition": "While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies",
      "context": "Paper: A Survey Analyzing Generalization in Deep Reinforcement Learning\nAuthors: Ezgi Korkmaz\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formal...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.647844",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2401.02349v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2401.02349v2"
        ]
      }
    },
    {
      "id": "662d3c41",
      "concept": "process of",
      "definition": "We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algorithm",
      "context": "Paper: Reinforcement Teaching\nAuthors: Calarina Muslimani, Alex Lewandowski, Dale Schuurmans\nCategories: cs.LG\nAbstract: Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algo...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.749753",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.11897v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.11897v3"
        ]
      }
    },
    {
      "id": "695404cb",
      "concept": "improve their",
      "definition": "Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn",
      "context": "Paper: Reinforcement Teaching\nAuthors: Calarina Muslimani, Alex Lewandowski, Dale Schuurmans\nCategories: cs.LG\nAbstract: Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algo...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.749914",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.11897v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.11897v3"
        ]
      }
    },
    {
      "id": "3afcd975",
      "concept": "introduce",
      "definition": "To learn an effective teaching policy, we introduce the parametric-behavior embedder that learns a representation of the student's learnable parameters from its input/output behavior",
      "context": "Paper: Reinforcement Teaching\nAuthors: Calarina Muslimani, Alex Lewandowski, Dale Schuurmans\nCategories: cs.LG\nAbstract: Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algo...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.749952",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.11897v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.11897v3"
        ]
      }
    },
    {
      "id": "d0f779a7",
      "concept": "heuristic reward",
      "definition": "Reinforcement Teaching outperforms previous work using heuristic reward functions and state representations, as well as other parameter representations",
      "context": "Paper: Reinforcement Teaching\nAuthors: Calarina Muslimani, Alex Lewandowski, Dale Schuurmans\nCategories: cs.LG\nAbstract: Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algo...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.749991",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.11897v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.11897v3"
        ]
      }
    },
    {
      "id": "2d0b0b94",
      "concept": "hand",
      "definition": "However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms",
      "context": "Paper: Reinforcement Teaching\nAuthors: Calarina Muslimani, Alex Lewandowski, Dale Schuurmans\nCategories: cs.LG\nAbstract: Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algo...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.750023",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.11897v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.11897v3"
        ]
      }
    },
    {
      "id": "2d622b51",
      "concept": "machine",
      "definition": "Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn",
      "context": "Paper: Reinforcement Teaching\nAuthors: Calarina Muslimani, Alex Lewandowski, Dale Schuurmans\nCategories: cs.LG\nAbstract: Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algo...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.750050",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.11897v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.11897v3"
        ]
      }
    },
    {
      "id": "49d00757",
      "concept": "they",
      "definition": "Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly",
      "context": "Paper: Reinforcement Teaching\nAuthors: Calarina Muslimani, Alex Lewandowski, Dale Schuurmans\nCategories: cs.LG\nAbstract: Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algo...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.750075",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.11897v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.11897v3"
        ]
      }
    },
    {
      "id": "b1168782",
      "concept": "differentiable",
      "definition": "However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms",
      "context": "Paper: Reinforcement Teaching\nAuthors: Calarina Muslimani, Alex Lewandowski, Dale Schuurmans\nCategories: cs.LG\nAbstract: Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algo...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.750105",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.11897v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.11897v3"
        ]
      }
    },
    {
      "id": "f96fe09e",
      "concept": "more",
      "definition": "Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly",
      "context": "Paper: Reinforcement Teaching\nAuthors: Calarina Muslimani, Alex Lewandowski, Dale Schuurmans\nCategories: cs.LG\nAbstract: Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algo...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.750132",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.11897v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.11897v3"
        ]
      }
    },
    {
      "id": "abaafd78",
      "concept": "improve",
      "definition": "Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn",
      "context": "Paper: Reinforcement Teaching\nAuthors: Calarina Muslimani, Alex Lewandowski, Dale Schuurmans\nCategories: cs.LG\nAbstract: Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algo...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.750162",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2204.11897v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2204.11897v3"
        ]
      }
    },
    {
      "id": "db859436",
      "concept": "indirect",
      "definition": "This approach is indirect and can be slow",
      "context": "Paper: Generative Adversarial Imitation Learning\nAuthors: Jonathan Ho, Stefano Ermon\nCategories: cs.LG, cs.AI\nAbstract: Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.851918",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1606.03476v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1606.03476v1"
        ]
      }
    },
    {
      "id": "a96c1b16",
      "concept": "without",
      "definition": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal",
      "context": "Paper: Generative Adversarial Imitation Learning\nAuthors: Jonathan Ho, Stefano Ermon\nCategories: cs.LG, cs.AI\nAbstract: Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.852046",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1606.03476v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1606.03476v1"
        ]
      }
    },
    {
      "id": "af083f8f",
      "concept": "over",
      "definition": "One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning",
      "context": "Paper: Generative Adversarial Imitation Learning\nAuthors: Jonathan Ho, Stefano Ermon\nCategories: cs.LG, cs.AI\nAbstract: Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.852114",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1606.03476v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1606.03476v1"
        ]
      }
    },
    {
      "id": "1dc4cbf4",
      "concept": "following inverse",
      "definition": "We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning",
      "context": "Paper: Generative Adversarial Imitation Learning\nAuthors: Jonathan Ho, Stefano Ermon\nCategories: cs.LG, cs.AI\nAbstract: Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.852170",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1606.03476v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1606.03476v1"
        ]
      }
    },
    {
      "id": "bffef1b2",
      "concept": "framework draws",
      "definition": "We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments",
      "context": "Paper: Generative Adversarial Imitation Learning\nAuthors: Jonathan Ho, Stefano Ermon\nCategories: cs.LG, cs.AI\nAbstract: Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.852220",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1606.03476v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1606.03476v1"
        ]
      }
    },
    {
      "id": "0395ff50",
      "concept": "free imitation",
      "definition": "We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments",
      "context": "Paper: Generative Adversarial Imitation Learning\nAuthors: Jonathan Ho, Stefano Ermon\nCategories: cs.LG, cs.AI\nAbstract: Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.852272",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1606.03476v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1606.03476v1"
        ]
      }
    },
    {
      "id": "49f7859a",
      "concept": "recover",
      "definition": "One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning",
      "context": "Paper: Generative Adversarial Imitation Learning\nAuthors: Jonathan Ho, Stefano Ermon\nCategories: cs.LG, cs.AI\nAbstract: Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.852323",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1606.03476v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1606.03476v1"
        ]
      }
    },
    {
      "id": "6aba22c4",
      "concept": "model-free",
      "definition": "We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments",
      "context": "Paper: Generative Adversarial Imitation Learning\nAuthors: Jonathan Ho, Stefano Ermon\nCategories: cs.LG, cs.AI\nAbstract: Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.852400",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1606.03476v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1606.03476v1"
        ]
      }
    },
    {
      "id": "b5d7bf1e",
      "concept": "from which",
      "definition": "We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments",
      "context": "Paper: Generative Adversarial Imitation Learning\nAuthors: Jonathan Ho, Stefano Ermon\nCategories: cs.LG, cs.AI\nAbstract: Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.852636",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1606.03476v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1606.03476v1"
        ]
      }
    },
    {
      "id": "2f28f38c",
      "concept": "recover the",
      "definition": "One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning",
      "context": "Paper: Generative Adversarial Imitation Learning\nAuthors: Jonathan Ho, Stefano Ermon\nCategories: cs.LG, cs.AI\nAbstract: Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement lea...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.852694",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1606.03476v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1606.03476v1"
        ]
      }
    },
    {
      "id": "4c7a5582",
      "concept": "to combine",
      "definition": "Moreover, the proposed approach provides a general framework that can be used to combine any episodic memory agent with other off-policy reinforcement learning algorithms",
      "context": "Paper: Two-Memory Reinforcement Learning\nAuthors: Zhao Yang, Thomas. M. Moerland, Mike Preuss\nCategories: cs.LG, cs.AI\nAbstract: While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weakness...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.955220",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.10098v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.10098v2"
        ]
      }
    },
    {
      "id": "18dc739e",
      "concept": "part to",
      "definition": "The 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each other",
      "context": "Paper: Two-Memory Reinforcement Learning\nAuthors: Zhao Yang, Thomas. M. Moerland, Mike Preuss\nCategories: cs.LG, cs.AI\nAbstract: While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weakness...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.955382",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.10098v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.10098v2"
        ]
      }
    },
    {
      "id": "c8101603",
      "concept": "systems",
      "definition": "Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them",
      "context": "Paper: Two-Memory Reinforcement Learning\nAuthors: Zhao Yang, Thomas. M. Moerland, Mike Preuss\nCategories: cs.LG, cs.AI\nAbstract: While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weakness...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.955463",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.10098v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.10098v2"
        ]
      }
    },
    {
      "id": "d0ed4d4d",
      "concept": "uses",
      "definition": "Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection",
      "context": "Paper: Two-Memory Reinforcement Learning\nAuthors: Zhao Yang, Thomas. M. Moerland, Mike Preuss\nCategories: cs.LG, cs.AI\nAbstract: While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weakness...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.955495",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.10098v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.10098v2"
        ]
      }
    },
    {
      "id": "514d55f7",
      "concept": "episodic memory",
      "definition": "Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection",
      "context": "Paper: Two-Memory Reinforcement Learning\nAuthors: Zhao Yang, Thomas. M. Moerland, Mike Preuss\nCategories: cs.LG, cs.AI\nAbstract: While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weakness...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.955521",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.10098v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.10098v2"
        ]
      }
    },
    {
      "id": "e8c0f0a4",
      "concept": "and weaknesses",
      "definition": "Episodic memory and reinforcement learning both have their own strengths and weaknesses",
      "context": "Paper: Two-Memory Reinforcement Learning\nAuthors: Zhao Yang, Thomas. M. Moerland, Mike Preuss\nCategories: cs.LG, cs.AI\nAbstract: While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weakness...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.955550",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.10098v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.10098v2"
        ]
      }
    },
    {
      "id": "44262d43",
      "concept": "method called",
      "definition": "In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengths",
      "context": "Paper: Two-Memory Reinforcement Learning\nAuthors: Zhao Yang, Thomas. M. Moerland, Mike Preuss\nCategories: cs.LG, cs.AI\nAbstract: While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weakness...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.955579",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.10098v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.10098v2"
        ]
      }
    },
    {
      "id": "6e0b0826",
      "concept": "state-action",
      "definition": "Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection",
      "context": "Paper: Two-Memory Reinforcement Learning\nAuthors: Zhao Yang, Thomas. M. Moerland, Mike Preuss\nCategories: cs.LG, cs.AI\nAbstract: While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weakness...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.955607",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.10098v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.10098v2"
        ]
      }
    },
    {
      "id": "188ae81c",
      "concept": "state-of",
      "definition": "Our experiments demonstrate that the 2M agent is more data efficient and outperforms both pure episodic memory and pure reinforcement learning, as well as a state-of-the-art memory-augmented RL agent",
      "context": "Paper: Two-Memory Reinforcement Learning\nAuthors: Zhao Yang, Thomas. M. Moerland, Mike Preuss\nCategories: cs.LG, cs.AI\nAbstract: While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weakness...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.955635",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.10098v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.10098v2"
        ]
      }
    },
    {
      "id": "7b7396d3",
      "concept": "hand",
      "definition": "Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection",
      "context": "Paper: Two-Memory Reinforcement Learning\nAuthors: Zhao Yang, Thomas. M. Moerland, Mike Preuss\nCategories: cs.LG, cs.AI\nAbstract: While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weakness...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:50.955660",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2304.10098v2",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/2304.10098v2"
        ]
      }
    },
    {
      "id": "5782d109",
      "concept": "improve their",
      "definition": "At the same time, low-fitness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and improve their adaptability",
      "context": "Paper: Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning\nAuthors: Shuai L, Shuai Han, Wenbo Zhou\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for ev...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.057567",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1912.06310v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1912.06310v1"
        ]
      }
    },
    {
      "id": "33ab9611",
      "concept": "recruit",
      "definition": "In this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning, a scalable framework that combines advantages of the three methods mentioned above",
      "context": "Paper: Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning\nAuthors: Shuai L, Shuai Han, Wenbo Zhou\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for ev...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.057750",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1912.06310v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1912.06310v1"
        ]
      }
    },
    {
      "id": "9d2c6bb9",
      "concept": "instructs",
      "definition": "This agent can recruit high-fitness actors from the population of evolutionary algorithms, which instructs itself to learn from experience replay buffer",
      "context": "Paper: Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning\nAuthors: Shuai L, Shuai Han, Wenbo Zhou\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for ev...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.057819",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1912.06310v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1912.06310v1"
        ]
      }
    },
    {
      "id": "e423fef8",
      "concept": "imitate",
      "definition": "At the same time, low-fitness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and improve their adaptability",
      "context": "Paper: Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning\nAuthors: Shuai L, Shuai Han, Wenbo Zhou\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for ev...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.057875",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1912.06310v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1912.06310v1"
        ]
      }
    },
    {
      "id": "d6cf0336",
      "concept": "patterns",
      "definition": "At the same time, low-fitness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and improve their adaptability",
      "context": "Paper: Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning\nAuthors: Shuai L, Shuai Han, Wenbo Zhou\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for ev...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.057930",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1912.06310v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1912.06310v1"
        ]
      }
    },
    {
      "id": "a943ae81",
      "concept": "than",
      "definition": "The performance of RIM's components is significantly better than components of previous evolutionary reinforcement learning algorithm, and the recruitment using soft update enables reinforcement learning agent to learn faster than that using hard update",
      "context": "Paper: Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning\nAuthors: Shuai L, Shuai Han, Wenbo Zhou\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for ev...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.057991",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1912.06310v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1912.06310v1"
        ]
      }
    },
    {
      "id": "e3545e5b",
      "concept": "buffer",
      "definition": "This agent can recruit high-fitness actors from the population of evolutionary algorithms, which instructs itself to learn from experience replay buffer",
      "context": "Paper: Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning\nAuthors: Shuai L, Shuai Han, Wenbo Zhou\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for ev...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.058058",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1912.06310v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1912.06310v1"
        ]
      }
    },
    {
      "id": "b557c1a4",
      "concept": "components",
      "definition": "The performance of RIM's components is significantly better than components of previous evolutionary reinforcement learning algorithm, and the recruitment using soft update enables reinforcement learning agent to learn faster than that using hard update",
      "context": "Paper: Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning\nAuthors: Shuai L, Shuai Han, Wenbo Zhou\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for ev...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.058117",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1912.06310v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1912.06310v1"
        ]
      }
    },
    {
      "id": "2125cae0",
      "concept": "policy actor",
      "definition": "Reinforcement and imitation learners in this framework can be replaced with any off-policy actor-critic reinforcement learner or data-driven imitation learner",
      "context": "Paper: Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning\nAuthors: Shuai L, Shuai Han, Wenbo Zhou\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for ev...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.058179",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1912.06310v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1912.06310v1"
        ]
      }
    },
    {
      "id": "b976a02b",
      "concept": "of the",
      "definition": "In this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning, a scalable framework that combines advantages of the three methods mentioned above",
      "context": "Paper: Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning\nAuthors: Shuai L, Shuai Han, Wenbo Zhou\nCategories: cs.LG, cs.AI, cs.NE\nAbstract: Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for ev...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.058247",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1912.06310v1",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1912.06310v1"
        ]
      }
    },
    {
      "id": "3cf692c9",
      "concept": "reinforcement",
      "definition": "We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)",
      "context": "Paper: Accelerate Reinforcement Learning with PID Controllers in the Pendulum   Simulations\nAuthors: Liping Bai\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.159236",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.00770v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2210.00770v1"
        ]
      }
    },
    {
      "id": "1525f90d",
      "concept": "accelerate reinforcement",
      "definition": "We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)",
      "context": "Paper: Accelerate Reinforcement Learning with PID Controllers in the Pendulum   Simulations\nAuthors: Liping Bai\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.159433",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.00770v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2210.00770v1"
        ]
      }
    },
    {
      "id": "ebf6c033",
      "concept": "we propose",
      "definition": "We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)",
      "context": "Paper: Accelerate Reinforcement Learning with PID Controllers in the Pendulum   Simulations\nAuthors: Liping Bai\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.159507",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.00770v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.00770v1"
        ]
      }
    },
    {
      "id": "4a36d6f6",
      "concept": "controllers",
      "definition": "We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)",
      "context": "Paper: Accelerate Reinforcement Learning with PID Controllers in the Pendulum   Simulations\nAuthors: Liping Bai\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.159562",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.00770v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2210.00770v1"
        ]
      }
    },
    {
      "id": "3f0c13b7",
      "concept": "propose",
      "definition": "We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)",
      "context": "Paper: Accelerate Reinforcement Learning with PID Controllers in the Pendulum   Simulations\nAuthors: Liping Bai\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.159617",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.00770v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.00770v1"
        ]
      }
    },
    {
      "id": "3519680b",
      "concept": "proportional integral",
      "definition": "We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)",
      "context": "Paper: Accelerate Reinforcement Learning with PID Controllers in the Pendulum   Simulations\nAuthors: Liping Bai\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.159721",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.00770v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.00770v1"
        ]
      }
    },
    {
      "id": "68e97f36",
      "concept": "integral",
      "definition": "We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)",
      "context": "Paper: Accelerate Reinforcement Learning with PID Controllers in the Pendulum   Simulations\nAuthors: Liping Bai\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.159791",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.00770v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.00770v1"
        ]
      }
    },
    {
      "id": "41e06a55",
      "concept": "learning",
      "definition": "We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)",
      "context": "Paper: Accelerate Reinforcement Learning with PID Controllers in the Pendulum   Simulations\nAuthors: Liping Bai\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.159838",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.00770v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2210.00770v1"
        ]
      }
    },
    {
      "id": "ad9eab3c",
      "concept": "pendulum",
      "definition": "We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)",
      "context": "Paper: Accelerate Reinforcement Learning with PID Controllers in the Pendulum   Simulations\nAuthors: Liping Bai\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.159878",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.00770v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2210.00770v1"
        ]
      }
    },
    {
      "id": "6a7102f6",
      "concept": "coaching",
      "definition": "We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)",
      "context": "Paper: Accelerate Reinforcement Learning with PID Controllers in the Pendulum   Simulations\nAuthors: Liping Bai\nCategories: eess.SY, cs.LG, cs.SY\nAbstract: We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL)....",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.159941",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2210.00770v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2210.00770v1"
        ]
      }
    },
    {
      "id": "8c66bce4",
      "concept": "convergence",
      "definition": "This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning",
      "context": "Paper: Augmented Q Imitation Learning (AQIL)\nAuthors: Xiao Lei Zhang, Anish Agarwal\nCategories: cs.LG, cs.AI\nAbstract: The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.261385",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2004.00993v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2004.00993v2"
        ]
      }
    },
    {
      "id": "6756e675",
      "concept": "learns via",
      "definition": "In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback",
      "context": "Paper: Augmented Q Imitation Learning (AQIL)\nAuthors: Xiao Lei Zhang, Anish Agarwal\nCategories: cs.LG, cs.AI\nAbstract: The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.261483",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2004.00993v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2004.00993v2"
        ]
      }
    },
    {
      "id": "e02e8e20",
      "concept": "machine",
      "definition": "In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback",
      "context": "Paper: Augmented Q Imitation Learning (AQIL)\nAuthors: Xiao Lei Zhang, Anish Agarwal\nCategories: cs.LG, cs.AI\nAbstract: The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.261533",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2004.00993v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2004.00993v2"
        ]
      }
    },
    {
      "id": "f4e0f7cd",
      "concept": "aqil",
      "definition": "The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning",
      "context": "Paper: Augmented Q Imitation Learning (AQIL)\nAuthors: Xiao Lei Zhang, Anish Agarwal\nCategories: cs.LG, cs.AI\nAbstract: The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.261599",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2004.00993v2",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/2004.00993v2"
        ]
      }
    },
    {
      "id": "90278f0b",
      "concept": "method by",
      "definition": "This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning",
      "context": "Paper: Augmented Q Imitation Learning (AQIL)\nAuthors: Xiao Lei Zhang, Anish Agarwal\nCategories: cs.LG, cs.AI\nAbstract: The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.261646",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2004.00993v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2004.00993v2"
        ]
      }
    },
    {
      "id": "5dd0c44a",
      "concept": "this paper",
      "definition": "This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning",
      "context": "Paper: Augmented Q Imitation Learning (AQIL)\nAuthors: Xiao Lei Zhang, Anish Agarwal\nCategories: cs.LG, cs.AI\nAbstract: The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.261687",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2004.00993v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2004.00993v2"
        ]
      }
    },
    {
      "id": "18061b59",
      "concept": "optimal policy",
      "definition": "Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy",
      "context": "Paper: Augmented Q Imitation Learning (AQIL)\nAuthors: Xiao Lei Zhang, Anish Agarwal\nCategories: cs.LG, cs.AI\nAbstract: The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.261727",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2004.00993v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2004.00993v2"
        ]
      }
    },
    {
      "id": "f7012081",
      "concept": "to an",
      "definition": "Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy",
      "context": "Paper: Augmented Q Imitation Learning (AQIL)\nAuthors: Xiao Lei Zhang, Anish Agarwal\nCategories: cs.LG, cs.AI\nAbstract: The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.261766",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2004.00993v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2004.00993v2"
        ]
      }
    },
    {
      "id": "49584a8e",
      "concept": "deep",
      "definition": "Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy",
      "context": "Paper: Augmented Q Imitation Learning (AQIL)\nAuthors: Xiao Lei Zhang, Anish Agarwal\nCategories: cs.LG, cs.AI\nAbstract: The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.261806",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2004.00993v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2004.00993v2"
        ]
      }
    },
    {
      "id": "7c01fb37",
      "concept": "significant time",
      "definition": "Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy",
      "context": "Paper: Augmented Q Imitation Learning (AQIL)\nAuthors: Xiao Lei Zhang, Anish Agarwal\nCategories: cs.LG, cs.AI\nAbstract: The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.261860",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/2004.00993v2",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/2004.00993v2"
        ]
      }
    },
    {
      "id": "147e6a31",
      "concept": "has on",
      "definition": "We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention",
      "context": "Paper: Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning\nAuthors: Nick Erickson, Qi Zhao\nCategories: stat.ML, cs.AI, cs.LG\nAbstract: This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.363243",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1706.05749v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1706.05749v1"
        ]
      }
    },
    {
      "id": "68839f1f",
      "concept": "than",
      "definition": "We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments",
      "context": "Paper: Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning\nAuthors: Nick Erickson, Qi Zhao\nCategories: stat.ML, cs.AI, cs.LG\nAbstract: This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.363410",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1706.05749v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1706.05749v1"
        ]
      }
    },
    {
      "id": "ace86cee",
      "concept": "this paper",
      "definition": "This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems",
      "context": "Paper: Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning\nAuthors: Nick Erickson, Qi Zhao\nCategories: stat.ML, cs.AI, cs.LG\nAbstract: This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.363482",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1706.05749v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1706.05749v1"
        ]
      }
    },
    {
      "id": "b7f958d3",
      "concept": "network attention",
      "definition": "We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention",
      "context": "Paper: Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning\nAuthors: Nick Erickson, Qi Zhao\nCategories: stat.ML, cs.AI, cs.LG\nAbstract: This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.363531",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1706.05749v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1706.05749v1"
        ]
      }
    },
    {
      "id": "a960f412",
      "concept": "deep",
      "definition": "This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems",
      "context": "Paper: Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning\nAuthors: Nick Erickson, Qi Zhao\nCategories: stat.ML, cs.AI, cs.LG\nAbstract: This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.363589",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1706.05749v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1706.05749v1"
        ]
      }
    },
    {
      "id": "69e76036",
      "concept": "first",
      "definition": "We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment",
      "context": "Paper: Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning\nAuthors: Nick Erickson, Qi Zhao\nCategories: stat.ML, cs.AI, cs.LG\nAbstract: This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.363651",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1706.05749v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1706.05749v1"
        ]
      }
    },
    {
      "id": "dbc2253a",
      "concept": "network",
      "definition": "We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention",
      "context": "Paper: Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning\nAuthors: Nick Erickson, Qi Zhao\nCategories: stat.ML, cs.AI, cs.LG\nAbstract: This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.363699",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1706.05749v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1706.05749v1"
        ]
      }
    },
    {
      "id": "12ade612",
      "concept": "qualitative",
      "definition": "We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention",
      "context": "Paper: Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning\nAuthors: Nick Erickson, Qi Zhao\nCategories: stat.ML, cs.AI, cs.LG\nAbstract: This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.363745",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1706.05749v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1706.05749v1"
        ]
      }
    },
    {
      "id": "2d0a581b",
      "concept": "analysis of",
      "definition": "We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention",
      "context": "Paper: Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning\nAuthors: Nick Erickson, Qi Zhao\nCategories: stat.ML, cs.AI, cs.LG\nAbstract: This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.363787",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1706.05749v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1706.05749v1"
        ]
      }
    },
    {
      "id": "0aebe6fe",
      "concept": "toolkit",
      "definition": "This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems",
      "context": "Paper: Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning\nAuthors: Nick Erickson, Qi Zhao\nCategories: stat.ML, cs.AI, cs.LG\nAbstract: This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.363833",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1706.05749v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1706.05749v1"
        ]
      }
    },
    {
      "id": "9e7d5638",
      "concept": "improve their",
      "definition": "Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability",
      "context": "Paper: Interpretable Reinforcement Learning with Ensemble Methods\nAuthors: Alexander Brown, Marek Petrik\nCategories: cs.LG, stat.ML\nAbstract: We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.465244",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.06995v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1809.06995v1"
        ]
      }
    },
    {
      "id": "55d03efa",
      "concept": "without",
      "definition": "Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability",
      "context": "Paper: Interpretable Reinforcement Learning with Ensemble Methods\nAuthors: Alexander Brown, Marek Petrik\nCategories: cs.LG, stat.ML\nAbstract: We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.465412",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.06995v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1809.06995v1"
        ]
      }
    },
    {
      "id": "4978076c",
      "concept": "machine",
      "definition": "Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning",
      "context": "Paper: Interpretable Reinforcement Learning with Ensemble Methods\nAuthors: Alexander Brown, Marek Petrik\nCategories: cs.LG, stat.ML\nAbstract: We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.465459",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.06995v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1809.06995v1"
        ]
      }
    },
    {
      "id": "5f7ed4a0",
      "concept": "accuracy without",
      "definition": "Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability",
      "context": "Paper: Interpretable Reinforcement Learning with Ensemble Methods\nAuthors: Alexander Brown, Marek Petrik\nCategories: cs.LG, stat.ML\nAbstract: We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.465492",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.06995v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1809.06995v1"
        ]
      }
    },
    {
      "id": "8c4abe12",
      "concept": "our experimental",
      "definition": "Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods",
      "context": "Paper: Interpretable Reinforcement Learning with Ensemble Methods\nAuthors: Alexander Brown, Marek Petrik\nCategories: cs.LG, stat.ML\nAbstract: We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.465519",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.06995v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1809.06995v1"
        ]
      }
    },
    {
      "id": "5ac36d4a",
      "concept": "ensemble",
      "definition": "We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems",
      "context": "Paper: Interpretable Reinforcement Learning with Ensemble Methods\nAuthors: Alexander Brown, Marek Petrik\nCategories: cs.LG, stat.ML\nAbstract: We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.465547",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.06995v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1809.06995v1"
        ]
      }
    },
    {
      "id": "fdec90f5",
      "concept": "improve",
      "definition": "Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability",
      "context": "Paper: Interpretable Reinforcement Learning with Ensemble Methods\nAuthors: Alexander Brown, Marek Petrik\nCategories: cs.LG, stat.ML\nAbstract: We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.465622",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.06995v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1809.06995v1"
        ]
      }
    },
    {
      "id": "0c3dce8b",
      "concept": "and match",
      "definition": "Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods",
      "context": "Paper: Interpretable Reinforcement Learning with Ensemble Methods\nAuthors: Alexander Brown, Marek Petrik\nCategories: cs.LG, stat.ML\nAbstract: We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.465677",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.06995v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1809.06995v1"
        ]
      }
    },
    {
      "id": "2407a312",
      "concept": "interpretable reinforcement",
      "definition": "Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning",
      "context": "Paper: Interpretable Reinforcement Learning with Ensemble Methods\nAuthors: Alexander Brown, Marek Petrik\nCategories: cs.LG, stat.ML\nAbstract: We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.465704",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.06995v1",
        "extraction_confidence": 0.95,
        "parent_sources": [
          "https://arxiv.org/abs/1809.06995v1"
        ]
      }
    },
    {
      "id": "8be322a5",
      "concept": "significantly reducing",
      "definition": "Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability",
      "context": "Paper: Interpretable Reinforcement Learning with Ensemble Methods\nAuthors: Alexander Brown, Marek Petrik\nCategories: cs.LG, stat.ML\nAbstract: We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees ...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.465732",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1809.06995v1",
        "extraction_confidence": 0.7999999999999999,
        "parent_sources": [
          "https://arxiv.org/abs/1809.06995v1"
        ]
      }
    },
    {
      "id": "5cd3cec6",
      "concept": "can be",
      "definition": "Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners",
      "context": "Paper: Unsupervised Meta-Learning for Reinforcement Learning\nAuthors: Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.568087",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.04640v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1806.04640v3"
        ]
      }
    },
    {
      "id": "ad33428b",
      "concept": "recipe",
      "definition": "We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach",
      "context": "Paper: Unsupervised Meta-Learning for Reinforcement Learning\nAuthors: Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.568220",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.04640v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1806.04640v3"
        ]
      }
    },
    {
      "id": "2e8eba7d",
      "concept": "contributions",
      "definition": "Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners",
      "context": "Paper: Unsupervised Meta-Learning for Reinforcement Learning\nAuthors: Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.568276",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.04640v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1806.04640v3"
        ]
      }
    },
    {
      "id": "1357a5ae",
      "concept": "contributions consist",
      "definition": "Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners",
      "context": "Paper: Unsupervised Meta-Learning for Reinforcement Learning\nAuthors: Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.568325",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.04640v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1806.04640v3"
        ]
      }
    },
    {
      "id": "0507ae7b",
      "concept": "for unsupervised",
      "definition": "We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach",
      "context": "Paper: Unsupervised Meta-Learning for Reinforcement Learning\nAuthors: Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.568385",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.04640v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1806.04640v3"
        ]
      }
    },
    {
      "id": "b35df7bb",
      "concept": "design as",
      "definition": "If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated",
      "context": "Paper: Unsupervised Meta-Learning for Reinforcement Learning\nAuthors: Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.568437",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.04640v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1806.04640v3"
        ]
      }
    },
    {
      "id": "f4cfbcb2",
      "concept": "motivate",
      "definition": "We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach",
      "context": "Paper: Unsupervised Meta-Learning for Reinforcement Learning\nAuthors: Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.568475",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.04640v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1806.04640v3"
        ]
      }
    },
    {
      "id": "cddbcbb5",
      "concept": "and describe",
      "definition": "We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach",
      "context": "Paper: Unsupervised Meta-Learning for Reinforcement Learning\nAuthors: Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.568514",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.04640v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1806.04640v3"
        ]
      }
    },
    {
      "id": "376a003a",
      "concept": "acquires",
      "definition": "Our experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratch",
      "context": "Paper: Unsupervised Meta-Learning for Reinforcement Learning\nAuthors: Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.568552",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.04640v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1806.04640v3"
        ]
      }
    },
    {
      "id": "bd847383",
      "concept": "without",
      "definition": "Our experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratch",
      "context": "Paper: Unsupervised Meta-Learning for Reinforcement Learning\nAuthors: Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn\nCategories: cs.LG, cs.AI, stat.ML\nAbstract: Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta...",
      "provenance": {
        "source_agent": "arxiv_collector_v1",
        "timestamp": "2025-08-15T21:55:51.568591",
        "method": "arxiv_extraction_1.0.0",
        "source_url": "https://arxiv.org/abs/1806.04640v3",
        "extraction_confidence": 0.85,
        "parent_sources": [
          "https://arxiv.org/abs/1806.04640v3"
        ]
      }
    }
  ]
}