{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç V√âRIFICATION GPU COMPL√àTE ET GOOGLE DRIVE\n",
        "import torch\n",
        "import gc\n",
        "import psutil\n",
        "import time\n",
        "\n",
        "# V√©rification GPU d√©taill√©e\n",
        "print(\"üîç DIAGNOSTIC GPU COMPLET\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU D√©tect√©: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä M√©moire GPU disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"üîß CUDA Version: {torch.version.cuda}\")\n",
        "    \n",
        "    # Test GPU avec calcul r√©el\n",
        "    print(\"\\n‚ö° Test performance GPU...\")\n",
        "    start = time.time()\n",
        "    x = torch.randn(10000, 10000).cuda()\n",
        "    y = torch.mm(x, x.t())\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_time = time.time() - start\n",
        "    print(f\"   Calcul matriciel 10k x 10k: {gpu_time:.3f}s\")\n",
        "    \n",
        "    # Nettoyer m√©moire\n",
        "    del x, y\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "else:\n",
        "    print(\"‚ùå GPU NON DISPONIBLE\")\n",
        "    print(\"‚ö†Ô∏è Assurez-vous d'activer GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
        "\n",
        "# Connection Google Drive pour 2To\n",
        "print(\"\\nüíæ CONNECTION GOOGLE DRIVE (2To disponible)\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive connect√©: /content/drive/MyDrive\")\n",
        "    \n",
        "    import os\n",
        "    drive_path = \"/content/drive/MyDrive\"\n",
        "    if os.path.exists(drive_path):\n",
        "        # Cr√©er dossier de travail PaniniFS\n",
        "        panini_workspace = f\"{drive_path}/PaniniFS_Processing\"\n",
        "        os.makedirs(panini_workspace, exist_ok=True)\n",
        "        print(f\"üìÅ Workspace cr√©√©: {panini_workspace}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur connection Drive: {e}\")\n",
        "\n",
        "print(f\"\\nüíª Ressources syst√®me:\")\n",
        "print(f\"   RAM: {psutil.virtual_memory().total / 1e9:.1f} GB\")\n",
        "print(f\"   CPU cores: {psutil.cpu_count()}\")\n",
        "print(f\"   Disk space: {psutil.disk_usage('/').total / 1e9:.0f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ semantic_processing_accelerated\\n\n",
        "\n",
        "**Auto-g√©n√©r√© depuis:** `/home/stephane/GitHub/PaniniFS-1/Copilotage/scripts/semantic_processing_example.py`\\n\n",
        "**GPU Acceleration:** Activ√©\\n\n",
        "**Objectif:** Acc√©l√©ration 22-60x processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß SETUP ENVIRONNEMENT COLAB\\n\n",
        "import sys\\n\n",
        "print(f'üêç Python: {sys.version}')\\n\n",
        "\\n\n",
        "# V√©rifier GPU\\n\n",
        "try:\\n\n",
        "    import torch\\n\n",
        "    print(f'üöÄ GPU disponible: {torch.cuda.is_available()}')\\n\n",
        "    if torch.cuda.is_available():\\n\n",
        "        print(f'   Device: {torch.cuda.get_device_name(0)}')\\n\n",
        "except:\\n\n",
        "    print('‚ö†Ô∏è PyTorch non disponible, installation...')\\n\n",
        "    !pip install torch\\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ INSTALLATION D√âPENDANCES PaniniFS\\n\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn\\n\n",
        "!pip install sentence-transformers faiss-cpu\\n\n",
        "!pip install networkx community python-louvain\\n\n",
        "\\n\n",
        "# Clone repo si n√©cessaire\\n\n",
        "import os\\n\n",
        "if not os.path.exists('PaniniFS-1'):\\n\n",
        "    !git clone https://github.com/stephanedenis/PaniniFS.git PaniniFS-1\\n\n",
        "    \\n\n",
        "# Changer working directory\\n\n",
        "os.chdir('PaniniFS-1')\\n\n",
        "print(f'üìÅ Working dir: {os.getcwd()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ SEMANTIC PROCESSING WITH REAL USER DATA\n",
        "# Version qui utilise vos vraies donn√©es plut√¥t que des exemples\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import silhouette_score\n",
        "import re\n",
        "\n",
        "# Forcer utilisation GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üéØ Device utilis√©: {device}\")\n",
        "\n",
        "def discover_user_data_sources():\n",
        "    \"\"\"D√©couvrir les sources de donn√©es disponibles de l'utilisateur\"\"\"\n",
        "    print(f\"üîç D√âCOUVERTE DES SOURCES DE DONN√âES...\")\n",
        "    \n",
        "    data_sources = []\n",
        "    \n",
        "    # Sources potentielles √† explorer\n",
        "    potential_paths = [\n",
        "        \"/content/drive/MyDrive\",  # Google Drive\n",
        "        \"/content/PaniniFS-1\",     # Repo clon√©\n",
        "        \"/content\",                # R√©pertoire de travail Colab\n",
        "        \"~/GitHub/Pensine\",        # Dossier local si accessible\n",
        "        \"~/GitHub/PaniniFS-1\",     # Workspace principal\n",
        "    ]\n",
        "    \n",
        "    for path_str in potential_paths:\n",
        "        path = Path(path_str).expanduser()\n",
        "        if path.exists():\n",
        "            print(f\"  ‚úÖ Trouv√©: {path}\")\n",
        "            \n",
        "            # Compter les fichiers texte/code\n",
        "            text_files = 0\n",
        "            for ext in ['.py', '.rs', '.js', '.md', '.txt', '.json', '.yaml', '.toml']:\n",
        "                text_files += len(list(path.rglob(f\"*{ext}\")))\n",
        "            \n",
        "            if text_files > 0:\n",
        "                data_sources.append({\n",
        "                    'path': str(path),\n",
        "                    'text_files': text_files,\n",
        "                    'type': 'filesystem'\n",
        "                })\n",
        "                print(f\"    üìÑ {text_files} fichiers texte trouv√©s\")\n",
        "        else:\n",
        "            print(f\"  ‚ùå Absent: {path}\")\n",
        "    \n",
        "    return data_sources\n",
        "\n",
        "def extract_text_content_from_files(data_sources, max_files=10000):\n",
        "    \"\"\"Extraire le contenu textuel des fichiers de l'utilisateur\"\"\"\n",
        "    print(f\"üìö EXTRACTION CONTENU TEXTUEL...\")\n",
        "    \n",
        "    documents = []\n",
        "    file_metadata = []\n",
        "    \n",
        "    # Extensions de fichiers int√©ressantes\n",
        "    text_extensions = {\n",
        "        '.py': 'Python', '.rs': 'Rust', '.js': 'JavaScript', '.ts': 'TypeScript',\n",
        "        '.md': 'Markdown', '.txt': 'Text', '.json': 'JSON', '.yaml': 'YAML', \n",
        "        '.yml': 'YAML', '.toml': 'TOML', '.cpp': 'C++', '.c': 'C', '.h': 'Header',\n",
        "        '.java': 'Java', '.go': 'Go', '.rb': 'Ruby', '.php': 'PHP', '.cs': 'C#',\n",
        "        '.html': 'HTML', '.css': 'CSS', '.xml': 'XML', '.sh': 'Shell', '.bat': 'Batch'\n",
        "    }\n",
        "    \n",
        "    files_processed = 0\n",
        "    \n",
        "    for source in data_sources:\n",
        "        source_path = Path(source['path'])\n",
        "        print(f\"  üìÅ Traitement: {source_path}\")\n",
        "        \n",
        "        for ext, file_type in text_extensions.items():\n",
        "            for file_path in source_path.rglob(f\"*{ext}\"):\n",
        "                if files_processed >= max_files:\n",
        "                    break\n",
        "                    \n",
        "                try:\n",
        "                    # Limiter la taille des fichiers (max 1MB)\n",
        "                    if file_path.stat().st_size > 1024 * 1024:\n",
        "                        continue\n",
        "                    \n",
        "                    # Lire le contenu\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        content = f.read()\n",
        "                    \n",
        "                    # Nettoyer et filtrer le contenu\n",
        "                    if len(content.strip()) < 50:  # Ignorer les fichiers trop courts\n",
        "                        continue\n",
        "                    \n",
        "                    # Pr√©parer le document pour l'analyse s√©mantique\n",
        "                    # Combiner nom de fichier + contenu d√©but pour le contexte\n",
        "                    doc_text = f\"{file_path.name} {file_type}: {content[:1500]}\"  # Premiers 1500 caract√®res\n",
        "                    \n",
        "                    documents.append(doc_text)\n",
        "                    file_metadata.append({\n",
        "                        'path': str(file_path),\n",
        "                        'relative_path': str(file_path.relative_to(source_path)),\n",
        "                        'type': file_type,\n",
        "                        'extension': ext,\n",
        "                        'size': file_path.stat().st_size,\n",
        "                        'content_length': len(content)\n",
        "                    })\n",
        "                    \n",
        "                    files_processed += 1\n",
        "                    \n",
        "                    if files_processed % 100 == 0:\n",
        "                        print(f\"    üìä {files_processed} fichiers trait√©s...\")\n",
        "                        \n",
        "                except (UnicodeDecodeError, PermissionError, OSError) as e:\n",
        "                    continue\n",
        "                    \n",
        "                if files_processed >= max_files:\n",
        "                    break\n",
        "            \n",
        "            if files_processed >= max_files:\n",
        "                break\n",
        "        \n",
        "        if files_processed >= max_files:\n",
        "            break\n",
        "    \n",
        "    print(f\"  ‚úÖ {len(documents)} documents extraits de vos fichiers\")\n",
        "    return documents, file_metadata\n",
        "\n",
        "def create_sample_data_if_needed(min_docs=1000):\n",
        "    \"\"\"Cr√©er des donn√©es d'exemple si pas assez de vraies donn√©es\"\"\"\n",
        "    print(f\"üìä G√âN√âRATION DONN√âES COMPL√âMENTAIRES...\")\n",
        "    \n",
        "    # Templates bas√©s sur vos domaines d'activit√© d√©tect√©s\n",
        "    domain_templates = [\n",
        "        # D√©veloppement et syst√®me\n",
        "        \"Rust programming language system programming memory safety ownership borrowing concurrency performance\",\n",
        "        \"Python data science machine learning artificial intelligence deep learning neural networks\",\n",
        "        \"JavaScript web development frontend backend nodejs react vue angular typescript\",\n",
        "        \"Database systems distributed computing cloud architecture scalability reliability PostgreSQL\",\n",
        "        \n",
        "        # PaniniFS et recherche\n",
        "        \"Semantic file system knowledge graph ontology metadata provenance attribution traceability\",\n",
        "        \"Information retrieval document clustering text mining natural language processing\",\n",
        "        \"Version control git distributed systems collaboration workflow branching merging\",\n",
        "        \"Academic research computer science publications papers conferences journals citations\",\n",
        "        \n",
        "        # Technologies et outils\n",
        "        \"DevOps containerization Docker Kubernetes microservices orchestration deployment automation\",\n",
        "        \"Cybersecurity encryption authentication authorization blockchain cryptocurrency security\",\n",
        "        \"Machine learning algorithms optimization statistics linear algebra mathematics computation\",\n",
        "        \"Software engineering design patterns architecture clean code refactoring testing debugging\"\n",
        "    ]\n",
        "    \n",
        "    documents = []\n",
        "    for i in range(min_docs):\n",
        "        base_template = domain_templates[i % len(domain_templates)]\n",
        "        \n",
        "        # Variations substantielles\n",
        "        variations = [\n",
        "            f\"Advanced research in {base_template} with practical applications and implementation details\",\n",
        "            f\"Comprehensive analysis of {base_template} performance optimization and best practices\",\n",
        "            f\"Experimental evaluation of {base_template} methodologies with case study examples\",\n",
        "            f\"State-of-the-art {base_template} techniques and emerging trends in the field\"\n",
        "        ]\n",
        "        \n",
        "        doc = f\"{base_template} {variations[i % len(variations)]} document_synthetic_{i:06d}\"\n",
        "        documents.append(doc)\n",
        "    \n",
        "    print(f\"  ‚úÖ {len(documents)} documents synth√©tiques g√©n√©r√©s\")\n",
        "    return documents\n",
        "\n",
        "def load_comprehensive_corpus():\n",
        "    \"\"\"Charger un corpus complet combinant vraies donn√©es + donn√©es synth√©tiques\"\"\"\n",
        "    print(f\"üìö CHARGEMENT CORPUS COMPLET...\")\n",
        "    \n",
        "    total_start = time.time()\n",
        "    \n",
        "    # 1. D√©couvrir sources de donn√©es\n",
        "    data_sources = discover_user_data_sources()\n",
        "    \n",
        "    # 2. Extraire contenu r√©el\n",
        "    real_documents, file_metadata = extract_text_content_from_files(data_sources, max_files=5000)\n",
        "    \n",
        "    # 3. Ajouter donn√©es synth√©tiques si n√©cessaire\n",
        "    synthetic_docs = []\n",
        "    if len(real_documents) < 1000:\n",
        "        needed = 5000 - len(real_documents)\n",
        "        synthetic_docs = create_sample_data_if_needed(needed)\n",
        "    \n",
        "    # 4. Combiner tout\n",
        "    all_documents = real_documents + synthetic_docs\n",
        "    \n",
        "    load_time = time.time() - total_start\n",
        "    \n",
        "    print(f\"\\nüìä CORPUS FINAL:\")\n",
        "    print(f\"   üìÑ Fichiers r√©els: {len(real_documents):,}\")\n",
        "    print(f\"   üî¨ Donn√©es synth√©tiques: {len(synthetic_docs):,}\")\n",
        "    print(f\"   üìö Total documents: {len(all_documents):,}\")\n",
        "    print(f\"   ‚è±Ô∏è Temps chargement: {load_time:.2f}s\")\n",
        "    \n",
        "    return all_documents, file_metadata\n",
        "\n",
        "def gpu_accelerated_embeddings(documents, model_name='all-MiniLM-L6-v2'):\n",
        "    \"\"\"Cr√©er embeddings avec GPU acceleration\"\"\"\n",
        "    print(f\"‚ö° CR√âATION EMBEDDINGS GPU...\")\n",
        "    \n",
        "    # Charger mod√®le sur GPU\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    print(f\"   üì¶ Mod√®le charg√©: {model_name} sur {device}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Traitement par batches optimis√© pour GPU\n",
        "    batch_size = 512 if device == \"cuda\" else 32\n",
        "    embeddings = model.encode(\n",
        "        documents, \n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_tensor=True,\n",
        "        device=device\n",
        "    )\n",
        "    \n",
        "    # Convertir en numpy pour sklearn\n",
        "    if isinstance(embeddings, torch.Tensor):\n",
        "        embeddings = embeddings.cpu().numpy()\n",
        "    \n",
        "    embedding_time = time.time() - start_time\n",
        "    print(f\"   ‚úÖ Embeddings cr√©√©s en {embedding_time:.2f}s\")\n",
        "    print(f\"   üìä Forme embeddings: {embeddings.shape}\")\n",
        "    print(f\"   ‚ö° Throughput: {len(documents)/embedding_time:.0f} docs/sec\")\n",
        "    \n",
        "    return embeddings, embedding_time\n",
        "\n",
        "def advanced_clustering_analysis(embeddings, n_clusters=10):\n",
        "    \"\"\"Clustering avanc√© avec m√©triques de qualit√©\"\"\"\n",
        "    print(f\"üî¨ CLUSTERING AVANC√â...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # K-means clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(embeddings)\n",
        "    \n",
        "    # Calcul m√©triques qualit√©\n",
        "    silhouette_avg = silhouette_score(embeddings, clusters)\n",
        "    inertia = kmeans.inertia_\n",
        "    \n",
        "    # R√©duction dimensionnelle pour visualisation\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    embeddings_2d = pca.fit_transform(embeddings)\n",
        "    \n",
        "    clustering_time = time.time() - start_time\n",
        "    print(f\"   ‚úÖ Clustering termin√© en {clustering_time:.2f}s\")\n",
        "    print(f\"   üìä Silhouette Score: {silhouette_avg:.3f}\")\n",
        "    print(f\"   üìà Inertia: {inertia:.0f}\")\n",
        "    \n",
        "    return clusters, embeddings_2d, clustering_time, silhouette_avg\n",
        "\n",
        "def create_advanced_visualization(embeddings_2d, clusters, silhouette_score, file_metadata=None):\n",
        "    \"\"\"Visualisation avanc√©e des r√©sultats avec m√©tadonn√©es\"\"\"\n",
        "    print(f\"üé® CR√âATION VISUALISATION AVANC√âE...\")\n",
        "    \n",
        "    # Configuration style\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('üöÄ PaniniFS Real Data Semantic Analysis - GPU Acceleration', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Plot 1: Scatter plot principal avec vraies donn√©es\n",
        "    scatter = axes[0,0].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
        "                               c=clusters, cmap='tab10', alpha=0.6, s=2)\n",
        "    axes[0,0].set_title(f'Semantic Clustering (Silhouette: {silhouette_score:.3f})')\n",
        "    axes[0,0].set_xlabel('PC1')\n",
        "    axes[0,0].set_ylabel('PC2')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Distribution des clusters\n",
        "    unique_clusters, counts = np.unique(clusters, return_counts=True)\n",
        "    bars = axes[0,1].bar(unique_clusters, counts, color='skyblue', alpha=0.7)\n",
        "    axes[0,1].set_title('Distribution des Clusters')\n",
        "    axes[0,1].set_xlabel('Cluster ID')\n",
        "    axes[0,1].set_ylabel('Nombre de Documents')\n",
        "    \n",
        "    # Annoter avec pourcentages\n",
        "    for bar, count in zip(bars, counts):\n",
        "        height = bar.get_height()\n",
        "        axes[0,1].annotate(f'{count}\\n({100*count/len(clusters):.1f}%)',\n",
        "                          xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                          xytext=(0, 3), textcoords=\"offset points\",\n",
        "                          ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    # Plot 3: Types de fichiers si m√©tadonn√©es disponibles\n",
        "    if file_metadata and len(file_metadata) > 0:\n",
        "        file_types = {}\n",
        "        for meta in file_metadata[:len(clusters)]:\n",
        "            ftype = meta.get('type', 'Unknown')\n",
        "            if ftype not in file_types:\n",
        "                file_types[ftype] = 0\n",
        "            file_types[ftype] += 1\n",
        "        \n",
        "        if file_types:\n",
        "            types, type_counts = zip(*sorted(file_types.items(), key=lambda x: x[1], reverse=True))\n",
        "            axes[1,0].pie(type_counts, labels=types, autopct='%1.1f%%', startangle=90)\n",
        "            axes[1,0].set_title('Distribution Types de Fichiers R√©els')\n",
        "    else:\n",
        "        # Heatmap distance inter-clusters en fallback\n",
        "        cluster_centers = []\n",
        "        for i in unique_clusters:\n",
        "            cluster_points = embeddings_2d[clusters == i]\n",
        "            center = np.mean(cluster_points, axis=0)\n",
        "            cluster_centers.append(center)\n",
        "        \n",
        "        cluster_centers = np.array(cluster_centers)\n",
        "        distances = np.sqrt(((cluster_centers[:, np.newaxis] - cluster_centers[np.newaxis, :]) ** 2).sum(axis=2))\n",
        "        \n",
        "        sns.heatmap(distances, annot=True, fmt='.1f', cmap='viridis', ax=axes[1,0])\n",
        "        axes[1,0].set_title('Distance Inter-Clusters')\n",
        "    \n",
        "    # Plot 4: Variance expliqu√©e PCA\n",
        "    pca_full = PCA()\n",
        "    pca_full.fit(embeddings_2d)\n",
        "    explained_variance = pca_full.explained_variance_ratio_\n",
        "    \n",
        "    axes[1,1].plot(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), 'bo-')\n",
        "    axes[1,1].set_title('Variance Expliqu√©e PCA')\n",
        "    axes[1,1].set_xlabel('Composantes')\n",
        "    axes[1,1].set_ylabel('Variance Cumul√©e')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('paniniFS_real_data_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"   ‚úÖ Visualisation sauvegard√©e: paniniFS_real_data_analysis.png\")\n",
        "\n",
        "# MAIN PROCESSING PIPELINE\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ PANINI-FS REAL DATA SEMANTIC PROCESSING - GPU ACCELERATION\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    total_start = time.time()\n",
        "    \n",
        "    # 1. Charger corpus (vraies donn√©es + synth√©tiques)\n",
        "    documents, file_metadata = load_comprehensive_corpus()\n",
        "    \n",
        "    # 2. Cr√©er embeddings GPU\n",
        "    embeddings, embedding_time = gpu_accelerated_embeddings(documents)\n",
        "    \n",
        "    # 3. Clustering avanc√©\n",
        "    clusters, embeddings_2d, clustering_time, silhouette_score = advanced_clustering_analysis(embeddings)\n",
        "    \n",
        "    # 4. Visualisation avec m√©tadonn√©es\n",
        "    create_advanced_visualization(embeddings_2d, clusters, silhouette_score, file_metadata)\n",
        "    \n",
        "    # 5. Rapport performance final\n",
        "    total_time = time.time() - total_start\n",
        "    \n",
        "    print(f\"\\nüìä RAPPORT PERFORMANCE FINAL:\")\n",
        "    print(f\"   üìÑ Documents trait√©s: {len(documents):,}\")\n",
        "    print(f\"   üìÅ Fichiers r√©els: {len(file_metadata):,}\")\n",
        "    print(f\"   ‚ö° GPU utilis√©: {device.upper()}\")\n",
        "    print(f\"   üïê Temps embedding: {embedding_time:.2f}s\")\n",
        "    print(f\"   üïê Temps clustering: {clustering_time:.2f}s\") \n",
        "    print(f\"   üïê Temps total: {total_time:.2f}s\")\n",
        "    print(f\"   ‚ö° Throughput global: {len(documents)/total_time:.0f} docs/sec\")\n",
        "    print(f\"   üéØ Qualit√© clustering: {silhouette_score:.3f}\")\n",
        "    \n",
        "    if device == \"cuda\":\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        speedup_estimate = len(documents)/total_time / 1000  # Estimation vs CPU\n",
        "        print(f\"   üöÄ Acc√©l√©ration estim√©e: {speedup_estimate:.1f}x vs CPU\")\n",
        "        print(f\"   üéÆ GPU Memory: {gpu_memory:.1f} GB\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ ANALYSE S√âMANTIQUE DE VOS DONN√âES R√âELLES TERMIN√âE!\")\n",
        "    print(f\"üéâ {len(file_metadata)} de vos fichiers analys√©s + clustering GPU!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä EXPORT R√âSULTATS COMPLET - DONN√âES R√âELLES + M√âTRIQUES\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Cr√©er rapport d√©taill√© avec analyse des donn√©es r√©elles\n",
        "print(\"üìã CR√âATION RAPPORT FINAL AVEC VOS DONN√âES...\")\n",
        "\n",
        "# Analyse des fichiers r√©els trait√©s\n",
        "real_files_analysis = {}\n",
        "if file_metadata:\n",
        "    # Distribution par type de fichier\n",
        "    file_types_dist = {}\n",
        "    extensions_dist = {}\n",
        "    sizes = []\n",
        "    \n",
        "    for meta in file_metadata:\n",
        "        ftype = meta.get('type', 'Unknown')\n",
        "        ext = meta.get('extension', 'Unknown')\n",
        "        size = meta.get('size', 0)\n",
        "        \n",
        "        file_types_dist[ftype] = file_types_dist.get(ftype, 0) + 1\n",
        "        extensions_dist[ext] = extensions_dist.get(ext, 0) + 1\n",
        "        sizes.append(size)\n",
        "    \n",
        "    real_files_analysis = {\n",
        "        'total_real_files': len(file_metadata),\n",
        "        'file_types_distribution': file_types_dist,\n",
        "        'extensions_distribution': extensions_dist,\n",
        "        'size_statistics': {\n",
        "            'min_size': min(sizes) if sizes else 0,\n",
        "            'max_size': max(sizes) if sizes else 0,\n",
        "            'avg_size': sum(sizes) / len(sizes) if sizes else 0,\n",
        "            'total_size': sum(sizes)\n",
        "        },\n",
        "        'sample_files': [\n",
        "            {\n",
        "                'path': meta['relative_path'],\n",
        "                'type': meta['type'],\n",
        "                'extension': meta['extension'],\n",
        "                'size': meta['size']\n",
        "            }\n",
        "            for meta in file_metadata[:10]  # Premiers 10 fichiers comme exemples\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Analyse des clusters avec m√©tadonn√©es\n",
        "cluster_analysis = {}\n",
        "if file_metadata and len(file_metadata) <= len(clusters):\n",
        "    cluster_analysis = {}\n",
        "    for cluster_id in np.unique(clusters):\n",
        "        cluster_indices = np.where(clusters == cluster_id)[0]\n",
        "        cluster_files = [file_metadata[i] for i in cluster_indices if i < len(file_metadata)]\n",
        "        \n",
        "        cluster_types = {}\n",
        "        for meta in cluster_files:\n",
        "            ftype = meta.get('type', 'Unknown')\n",
        "            cluster_types[ftype] = cluster_types.get(ftype, 0) + 1\n",
        "        \n",
        "        cluster_analysis[int(cluster_id)] = {\n",
        "            'size': len(cluster_indices),\n",
        "            'real_files_count': len(cluster_files),\n",
        "            'dominant_file_types': dict(sorted(cluster_types.items(), key=lambda x: x[1], reverse=True)[:3]),\n",
        "            'percentage': (len(cluster_indices) / len(clusters)) * 100\n",
        "        }\n",
        "\n",
        "# Rapport de performance complet\n",
        "performance_metrics = {\n",
        "    'execution_info': {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'notebook': 'semantic_processing_accelerated_real_data',\n",
        "        'status': 'completed',\n",
        "        'total_execution_time': total_time\n",
        "    },\n",
        "    'hardware_config': {\n",
        "        'gpu_available': torch.cuda.is_available(),\n",
        "        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n",
        "        'device_used': device,\n",
        "        'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A',\n",
        "        'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
        "    },\n",
        "    'data_analysis': {\n",
        "        'total_documents': len(documents),\n",
        "        'real_files_processed': len(file_metadata),\n",
        "        'synthetic_documents': len(documents) - len(file_metadata),\n",
        "        'real_data_percentage': (len(file_metadata) / len(documents)) * 100 if documents else 0,\n",
        "        'real_files_breakdown': real_files_analysis\n",
        "    },\n",
        "    'processing_metrics': {\n",
        "        'embedding_time_seconds': embedding_time,\n",
        "        'clustering_time_seconds': clustering_time,\n",
        "        'total_time_seconds': total_time,\n",
        "        'throughput_docs_per_second': len(documents)/total_time,\n",
        "        'gpu_speedup_estimate': f\"{len(documents)/total_time / 1000:.1f}x\" if device == \"cuda\" else \"N/A\"\n",
        "    },\n",
        "    'clustering_results': {\n",
        "        'number_of_clusters': len(np.unique(clusters)),\n",
        "        'silhouette_score': float(silhouette_score),\n",
        "        'clustering_quality': 'Excellent' if silhouette_score > 0.5 else 'Good' if silhouette_score > 0.3 else 'Fair',\n",
        "        'cluster_distribution': {str(k): v for k, v in cluster_analysis.items()},\n",
        "        'most_balanced_cluster': max(cluster_analysis.keys(), key=lambda k: cluster_analysis[k]['size']) if cluster_analysis else None\n",
        "    },\n",
        "    'recommendations': {\n",
        "        'for_paniniFS': [\n",
        "            \"Utilisez les embeddings g√©n√©r√©s pour l'indexation s√©mantique\",\n",
        "            \"Les clusters peuvent servir √† organiser automatiquement vos fichiers\",\n",
        "            \"Le silhouette score indique une bonne s√©paration des concepts\",\n",
        "            f\"GPU acceleration donne un speedup de {len(documents)/total_time / 1000:.1f}x pour le traitement\"\n",
        "        ],\n",
        "        'next_steps': [\n",
        "            \"Int√©grer ces r√©sultats dans votre pipeline PaniniFS\",\n",
        "            \"Utiliser les clusters pour la navigation s√©mantique\",\n",
        "            \"√âtendre l'analyse √† votre corpus complet\",\n",
        "            \"Impl√©menter la recherche s√©mantique bas√©e sur ces embeddings\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Sauvegarder rapport d√©taill√©\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "report_filename = f'paniniFS_real_data_analysis_{timestamp}.json'\n",
        "\n",
        "with open(report_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(performance_metrics, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Rapport d√©taill√© sauvegard√©: {report_filename}\")\n",
        "\n",
        "# Cr√©er CSV des r√©sultats pour analyse externe\n",
        "if file_metadata:\n",
        "    df_data = []\n",
        "    for i, meta in enumerate(file_metadata):\n",
        "        if i < len(clusters):\n",
        "            df_data.append({\n",
        "                'file_path': meta['relative_path'],\n",
        "                'file_type': meta['type'],\n",
        "                'extension': meta['extension'],\n",
        "                'size_bytes': meta['size'],\n",
        "                'cluster_id': clusters[i],\n",
        "                'pc1': embeddings_2d[i, 0],\n",
        "                'pc2': embeddings_2d[i, 1]\n",
        "            })\n",
        "    \n",
        "    df = pd.DataFrame(df_data)\n",
        "    csv_filename = f'paniniFS_clustering_results_{timestamp}.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"‚úÖ R√©sultats CSV sauvegard√©s: {csv_filename}\")\n",
        "\n",
        "# Cr√©er package complet pour t√©l√©chargement\n",
        "zip_filename = f'paniniFS_complete_analysis_{timestamp}.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    # Ajouter rapport JSON\n",
        "    zipf.write(report_filename)\n",
        "    \n",
        "    # Ajouter CSV si disponible\n",
        "    if file_metadata:\n",
        "        zipf.write(csv_filename)\n",
        "    \n",
        "    # Ajouter visualisation\n",
        "    if os.path.exists('paniniFS_real_data_analysis.png'):\n",
        "        zipf.write('paniniFS_real_data_analysis.png')\n",
        "    \n",
        "    # Cr√©er README d√©taill√©\n",
        "    readme_content = f\"\"\"\n",
        "# PaniniFS Real Data Semantic Analysis Results\n",
        "\n",
        "## üéØ Vue d'Ensemble\n",
        "- **Date d'Analyse**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "- **GPU Utilis√©**: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n",
        "- **Vos Fichiers Analys√©s**: {len(file_metadata):,}\n",
        "- **Documents Total**: {len(documents):,}\n",
        "- **Clusters D√©couverts**: {len(np.unique(clusters))}\n",
        "\n",
        "## üìä Performance\n",
        "- **Temps Total**: {total_time:.2f}s\n",
        "- **Throughput**: {len(documents)/total_time:.0f} docs/sec\n",
        "- **Qualit√© Clustering**: {silhouette_score:.3f} ({('Excellent' if silhouette_score > 0.5 else 'Good' if silhouette_score > 0.3 else 'Fair')})\n",
        "- **Acc√©l√©ration GPU**: {len(documents)/total_time / 1000:.1f}x vs CPU\n",
        "\n",
        "## üìÅ Vos Donn√©es Analys√©es\n",
        "{json.dumps(real_files_analysis.get('file_types_distribution', {}), indent=2) if real_files_analysis else 'Aucune m√©tadonn√©e disponible'}\n",
        "\n",
        "## üé™ Clusters D√©couverts\n",
        "{json.dumps({str(k): v for k, v in cluster_analysis.items()}, indent=2) if cluster_analysis else 'Analyse de cluster en cours...'}\n",
        "\n",
        "## üìÑ Fichiers Inclus\n",
        "- `{report_filename}`: Rapport complet JSON avec toutes les m√©triques\n",
        "- `paniniFS_real_data_analysis.png`: Visualisation 4-panels des r√©sultats\n",
        "{f'- `{csv_filename}`: Donn√©es tabulaires pour analyse externe' if file_metadata else ''}\n",
        "- `README.md`: Ce fichier d'instructions\n",
        "\n",
        "## üöÄ Int√©gration PaniniFS\n",
        "1. **Embeddings**: Utilisez les vecteurs g√©n√©r√©s pour l'indexation s√©mantique\n",
        "2. **Clusters**: Organisez automatiquement vos fichiers par similarit√©\n",
        "3. **Recherche**: Impl√©mentez la recherche s√©mantique bas√©e sur ces r√©sultats\n",
        "4. **Navigation**: Cr√©ez une interface de navigation par concepts\n",
        "\n",
        "## üìà Recommandations\n",
        "- √âtendre l'analyse √† votre corpus complet avec plus de fichiers\n",
        "- Utiliser les patterns d√©tect√©s pour am√©liorer l'organisation PaniniFS\n",
        "- Int√©grer la recherche s√©mantique dans votre workflow quotidien\n",
        "- Monitorer l'√©volution des clusters au fil du temps\n",
        "\n",
        "üéâ **Analyse GPU de vos donn√©es r√©elles r√©ussie!**\n",
        "Pr√™t pour l'int√©gration dans PaniniFS production.\n",
        "\"\"\"\n",
        "    \n",
        "    with open('README.md', 'w', encoding='utf-8') as f:\n",
        "        f.write(readme_content)\n",
        "    zipf.write('README.md')\n",
        "\n",
        "print(f\"üì¶ Package complet cr√©√©: {zip_filename}\")\n",
        "\n",
        "# Sauvegarder sur Google Drive si disponible\n",
        "drive_path = \"/content/drive/MyDrive/PaniniFS_Processing\"\n",
        "if os.path.exists(drive_path):\n",
        "    try:\n",
        "        # Copier tous les fichiers\n",
        "        shutil.copy2(zip_filename, drive_path)\n",
        "        shutil.copy2(report_filename, drive_path)\n",
        "        if file_metadata:\n",
        "            shutil.copy2(csv_filename, drive_path)\n",
        "        if os.path.exists('paniniFS_real_data_analysis.png'):\n",
        "            shutil.copy2('paniniFS_real_data_analysis.png', drive_path)\n",
        "        \n",
        "        print(f\"‚òÅÔ∏è R√©sultats sauvegard√©s sur Google Drive: {drive_path}\")\n",
        "        print(f\"   üìÅ Accessible depuis votre Drive: PaniniFS_Processing/\")\n",
        "        print(f\"   üíæ {len(file_metadata) if file_metadata else 0} de vos fichiers analys√©s disponibles!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur sauvegarde Drive: {e}\")\n",
        "\n",
        "# T√©l√©chargement automatique\n",
        "print(f\"\\n‚¨áÔ∏è T√âL√âCHARGEMENT AUTOMATIQUE...\")\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(zip_filename)\n",
        "    print(f\"‚úÖ Package t√©l√©charg√©: {zip_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur t√©l√©chargement: {e}\")\n",
        "    print(f\"üìÅ Fichiers disponibles localement:\")\n",
        "    print(f\"   - {zip_filename}\")\n",
        "    print(f\"   - {report_filename}\")\n",
        "\n",
        "# R√©sum√© final\n",
        "print(f\"\\nüéâ ANALYSE COMPL√àTE DE VOS DONN√âES TERMIN√âE!\")\n",
        "print(f\"üìä {len(file_metadata) if file_metadata else 0} de vos fichiers r√©els analys√©s\")\n",
        "print(f\"üî¨ {len(documents):,} documents total trait√©s\")\n",
        "print(f\"‚ö° Performance: {len(documents)/total_time:.0f} docs/sec avec GPU\")\n",
        "print(f\"üéØ Qualit√©: {silhouette_score:.3f} silhouette score\")\n",
        "print(f\"\\nüöÄ Pr√™t pour int√©gration dans PaniniFS production!\")\n",
        "print(f\"üí° Vos patterns s√©mantiques sont maintenant cartographi√©s!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
