{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6178328f",
   "metadata": {},
   "source": [
    "# ğŸš€ PaniniFS - Mode Cloud Autonome\n",
    "\n",
    "**100% Cloud Native** - Clonage automatique des repos GitHub\n",
    "\n",
    "## ğŸ¯ Workflow Autonome\n",
    "1. **Auto-dÃ©tection** : Mode Colab vs Local\n",
    "2. **Clonage repos** : Tous les repos GitHub automatiquement\n",
    "3. **Scan optimisÃ©** : Limites strictes pour performance\n",
    "4. **Embeddings** : Pipeline complet temps rÃ©el\n",
    "5. **Recherche** : Interface interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e609e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ SETUP AUTONOME - DÃ©tection environnement et installation\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# DÃ©tection mode Cloud (Colab/Kaggle/etc)\n",
    "IS_CLOUD = 'google.colab' in sys.modules or '/kaggle/' in os.environ.get('PATH', '') or 'COLAB_GPU' in os.environ\n",
    "print(f\"ğŸŒ Mode dÃ©tectÃ©: {'â˜ï¸ CLOUD' if IS_CLOUD else 'ğŸ–¥ï¸ LOCAL'}\")\n",
    "\n",
    "if IS_CLOUD:\n",
    "    # Installation dÃ©pendances cloud\n",
    "    print(\"ğŸ“¦ Installation dÃ©pendances cloud...\")\n",
    "    !pip install sentence-transformers torch --quiet\n",
    "    BASE_PATH = Path('/content')\n",
    "else:\n",
    "    BASE_PATH = Path('/home/stephane/GitHub')\n",
    "\n",
    "print(f\"ğŸ“ RÃ©pertoire de travail: {BASE_PATH}\")\n",
    "os.chdir(BASE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”„ CLONAGE AUTOMATIQUE DES REPOS\n",
    "def clone_repos_cloud():\n",
    "    \"\"\"Clone tous les repos nÃ©cessaires en mode cloud\"\"\"\n",
    "    \n",
    "    repos_config = {\n",
    "        'PaniniFS-1': 'https://github.com/stephanedenis/PaniniFS.git',\n",
    "        'Pensine': 'https://github.com/stephanedenis/Pensine.git', \n",
    "        'totoro-automation': 'https://github.com/stephanedenis/totoro-automation.git',\n",
    "        'hexagonal-demo': 'https://github.com/stephanedenis/hexagonal-demo.git'\n",
    "    }\n",
    "    \n",
    "    cloned_repos = []\n",
    "    \n",
    "    for repo_name, repo_url in repos_config.items():\n",
    "        repo_path = BASE_PATH / repo_name\n",
    "        \n",
    "        if repo_path.exists():\n",
    "            print(f\"âœ… {repo_name} dÃ©jÃ  prÃ©sent\")\n",
    "            cloned_repos.append(repo_name)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(f\"ğŸ“¥ Clonage {repo_name}...\")\n",
    "            result = subprocess.run(\n",
    "                ['git', 'clone', repo_url, str(repo_path)], \n",
    "                capture_output=True, \n",
    "                text=True, \n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"âœ… {repo_name} clonÃ© avec succÃ¨s\")\n",
    "                cloned_repos.append(repo_name)\n",
    "            else:\n",
    "                print(f\"âš ï¸ Erreur clonage {repo_name}: {result.stderr}\")\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"â±ï¸ Timeout clonage {repo_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur {repo_name}: {e}\")\n",
    "    \n",
    "    return cloned_repos\n",
    "\n",
    "# ExÃ©cution clonage en mode cloud\n",
    "if IS_CLOUD:\n",
    "    start_time = time.time()\n",
    "    available_repos = clone_repos_cloud()\n",
    "    clone_time = time.time() - start_time\n",
    "    print(f\"\\nğŸ¯ Clonage terminÃ© en {clone_time:.2f}s\")\n",
    "    print(f\"ğŸ“¦ {len(available_repos)} repos disponibles: {', '.join(available_repos)}\")\n",
    "else:\n",
    "    # Mode local - utilise les repos existants\n",
    "    available_repos = ['PaniniFS-1', 'Pensine', 'totoro-automation', 'hexagonal-demo']\n",
    "    print(f\"ğŸ“¦ Mode local - {len(available_repos)} repos configurÃ©s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f2c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” SCAN SOURCES CLOUD-OPTIMISÃ‰\n",
    "def scan_sources_cloud_optimized():\n",
    "    \"\"\"Scan optimisÃ© pour mode cloud avec limites strictes\"\"\"\n",
    "    \n",
    "    # Limites cloud-optimisÃ©es\n",
    "    MAX_PY_FILES_PER_REPO = 30  # RÃ©duit pour cloud\n",
    "    MAX_MD_FILES_PER_REPO = 15  # RÃ©duit pour cloud\n",
    "    MAX_FILE_SIZE = 100 * 1024  # 100KB max\n",
    "    \n",
    "    all_sources = []\n",
    "    scan_stats = {'total_files': 0, 'total_size': 0, 'repos_scanned': 0}\n",
    "    \n",
    "    for repo_name in available_repos:\n",
    "        repo_path = BASE_PATH / repo_name\n",
    "        \n",
    "        if not repo_path.exists():\n",
    "            print(f\"âš ï¸ Repo {repo_name} non trouvÃ©\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"ğŸ” Scan {repo_name}...\")\n",
    "        repo_sources = []\n",
    "        py_count, md_count = 0, 0\n",
    "        \n",
    "        try:\n",
    "            # Scan fichiers Python\n",
    "            for py_file in repo_path.rglob('*.py'):\n",
    "                if py_count >= MAX_PY_FILES_PER_REPO:\n",
    "                    break\n",
    "                    \n",
    "                if py_file.stat().st_size > MAX_FILE_SIZE:\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    content = py_file.read_text(encoding='utf-8', errors='replace')\n",
    "                    if len(content.strip()) > 50:  # Filtre fichiers vides\n",
    "                        repo_sources.append({\n",
    "                            'repo': repo_name,\n",
    "                            'path': str(py_file.relative_to(repo_path)),\n",
    "                            'type': 'python',\n",
    "                            'content': content[:5000],  # Limite contenu\n",
    "                            'size': len(content)\n",
    "                        })\n",
    "                        py_count += 1\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            # Scan fichiers Markdown\n",
    "            for md_file in repo_path.rglob('*.md'):\n",
    "                if md_count >= MAX_MD_FILES_PER_REPO:\n",
    "                    break\n",
    "                    \n",
    "                if md_file.stat().st_size > MAX_FILE_SIZE:\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    content = md_file.read_text(encoding='utf-8', errors='replace')\n",
    "                    if len(content.strip()) > 50:\n",
    "                        repo_sources.append({\n",
    "                            'repo': repo_name,\n",
    "                            'path': str(md_file.relative_to(repo_path)),\n",
    "                            'type': 'markdown',\n",
    "                            'content': content[:3000],  # Limite contenu MD\n",
    "                            'size': len(content)\n",
    "                        })\n",
    "                        md_count += 1\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur scan {repo_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        all_sources.extend(repo_sources)\n",
    "        scan_stats['repos_scanned'] += 1\n",
    "        scan_stats['total_files'] += len(repo_sources)\n",
    "        scan_stats['total_size'] += sum(s['size'] for s in repo_sources)\n",
    "        \n",
    "        print(f\"  ğŸ“„ {len(repo_sources)} fichiers ({py_count} .py + {md_count} .md)\")\n",
    "    \n",
    "    return all_sources, scan_stats\n",
    "\n",
    "# ExÃ©cution scan\n",
    "print(\"\\nğŸ“ SCAN SOURCES CLOUD-OPTIMISÃ‰\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "start_time = time.time()\n",
    "sources, stats = scan_sources_cloud_optimized()\n",
    "scan_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâ±ï¸ Scan terminÃ© en {scan_time:.2f}s\")\n",
    "print(f\"ğŸ¯ {stats['total_files']} sources consolidÃ©es\")\n",
    "print(f\"ğŸ“Š {stats['repos_scanned']} repos scannÃ©s\")\n",
    "print(f\"ğŸ’¾ {stats['total_size'] / 1024:.1f}KB total\")\n",
    "\n",
    "if len(sources) == 0:\n",
    "    print(\"\\nâš ï¸ AUCUNE SOURCE TROUVÃ‰E\")\n",
    "    print(\"ğŸ’¡ VÃ©rifiez le clonage des repos\")\n",
    "else:\n",
    "    print(f\"\\nâœ… SOURCES PRÃŠTES POUR EMBEDDINGS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3ae7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  GÃ‰NÃ‰RATION EMBEDDINGS CLOUD-OPTIMISÃ‰E\n",
    "if len(sources) > 0:\n",
    "    print(\"\\nğŸ§  GÃ‰NÃ‰RATION EMBEDDINGS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        import torch\n",
    "        \n",
    "        # Configuration GPU/CPU automatique\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"ğŸ”§ Device: {device}\")\n",
    "        \n",
    "        # ModÃ¨le optimisÃ©\n",
    "        print(\"ğŸ“¥ Chargement modÃ¨le...\")\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "        \n",
    "        # PrÃ©paration documents\n",
    "        documents = []\n",
    "        metadata = []\n",
    "        \n",
    "        for source in sources:\n",
    "            # Formatage document pour embeddings\n",
    "            doc_text = f\"Repo: {source['repo']}\\nFile: {source['path']}\\nType: {source['type']}\\n\\nContent:\\n{source['content']}\"\n",
    "            documents.append(doc_text)\n",
    "            metadata.append({\n",
    "                'repo': source['repo'],\n",
    "                'path': source['path'],\n",
    "                'type': source['type']\n",
    "            })\n",
    "        \n",
    "        # Limitation pour performance cloud\n",
    "        MAX_DOCS = 150  # Limite cloud\n",
    "        if len(documents) > MAX_DOCS:\n",
    "            print(f\"âš¡ Limitation Ã  {MAX_DOCS} docs pour performance cloud\")\n",
    "            documents = documents[:MAX_DOCS]\n",
    "            metadata = metadata[:MAX_DOCS]\n",
    "        \n",
    "        print(f\"ğŸ”„ GÃ©nÃ©ration embeddings pour {len(documents)} documents...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # GÃ©nÃ©ration par batch pour Ã©viter OOM\n",
    "        batch_size = 32 if device == 'cuda' else 16\n",
    "        embeddings = model.encode(documents, batch_size=batch_size, show_progress_bar=True)\n",
    "        \n",
    "        emb_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"âœ… Embeddings gÃ©nÃ©rÃ©s en {emb_time:.2f}s\")\n",
    "        print(f\"ğŸ“Š {len(embeddings)} vecteurs de dimension {embeddings.shape[1]}\")\n",
    "        print(f\"âš¡ Performance: {len(documents)/emb_time:.1f} docs/sec\")\n",
    "        \n",
    "        embeddings_ready = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur embeddings: {e}\")\n",
    "        embeddings_ready = False\n",
    "else:\n",
    "    print(\"âš ï¸ Pas de sources - skip embeddings\")\n",
    "    embeddings_ready = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af985505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” RECHERCHE SÃ‰MANTIQUE INTERACTIVE\n",
    "if embeddings_ready:\n",
    "    import numpy as np\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    def semantic_search_cloud(query, top_k=5):\n",
    "        \"\"\"Recherche sÃ©mantique optimisÃ©e cloud\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # GÃ©nÃ©ration embedding query\n",
    "            query_embedding = model.encode([query])\n",
    "            \n",
    "            # Calcul similaritÃ©\n",
    "            similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "            \n",
    "            # Top rÃ©sultats\n",
    "            top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "            \n",
    "            results = []\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                score = similarities[idx]\n",
    "                meta = metadata[idx]\n",
    "                doc = documents[idx]\n",
    "                \n",
    "                results.append({\n",
    "                    'rank': i + 1,\n",
    "                    'score': float(score),\n",
    "                    'repo': meta['repo'],\n",
    "                    'path': meta['path'],\n",
    "                    'type': meta['type'],\n",
    "                    'content_preview': doc[:300] + '...' if len(doc) > 300 else doc\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur recherche: {e}\")\n",
    "            return []\n",
    "    \n",
    "    # Interface de recherche\n",
    "    print(\"\\nğŸ” RECHERCHE SÃ‰MANTIQUE INTERACTIVE\")\n",
    "    print(\"=\" * 35)\n",
    "    print(\"ğŸ¯ Exemples de requÃªtes:\")\n",
    "    print(\"  - 'filesystem implementation'\")\n",
    "    print(\"  - 'neural network training'\")\n",
    "    print(\"  - 'configuration files'\")\n",
    "    print(\"  - 'error handling'\")\n",
    "    \n",
    "    # Test automatique\n",
    "    test_query = \"filesystem implementation\"\n",
    "    print(f\"\\nğŸ§ª Test automatique: '{test_query}'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = semantic_search_cloud(test_query, top_k=3)\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"âš¡ Recherche en {search_time:.3f}s\")\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nğŸ“Š RÃ‰SULTATS:\")\n",
    "        for result in results:\n",
    "            print(f\"\\n{result['rank']}. ğŸ“ {result['repo']}/{result['path']}\")\n",
    "            print(f\"   ğŸ¯ Score: {result['score']:.3f} | Type: {result['type']}\")\n",
    "            print(f\"   ğŸ“ {result['content_preview'][:150]}...\")\n",
    "    \n",
    "    print(\"\\nâœ… SYSTÃˆME PRÃŠT POUR RECHERCHE INTERACTIVE\")\n",
    "    search_function_ready = True\n",
    "else:\n",
    "    search_function_ready = False\n",
    "    print(\"âš ï¸ Recherche non disponible - problÃ¨me embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407bc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š RAPPORT FINAL CLOUD AUTONOME\n",
    "print(\"\\nğŸ‰ RAPPORT FINAL - MODE CLOUD AUTONOME\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "report = {\n",
    "    'mode': 'CLOUD' if IS_CLOUD else 'LOCAL',\n",
    "    'repos_clones': len(available_repos) if 'available_repos' in locals() else 0,\n",
    "    'sources_scannees': len(sources) if 'sources' in locals() else 0,\n",
    "    'embeddings_generes': len(embeddings) if 'embeddings_ready' else 0,\n",
    "    'recherche_active': search_function_ready if 'search_function_ready' in locals() else False,\n",
    "    'performance': {\n",
    "        'clonage': f\"{clone_time:.2f}s\" if 'clone_time' in locals() else 'N/A',\n",
    "        'scan': f\"{scan_time:.2f}s\" if 'scan_time' in locals() else 'N/A',\n",
    "        'embeddings': f\"{emb_time:.2f}s\" if 'emb_time' in locals() else 'N/A',\n",
    "        'recherche': f\"{search_time:.3f}s\" if 'search_time' in locals() else 'N/A'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"ğŸŒ Mode: {report['mode']}\")\n",
    "print(f\"ğŸ“¦ Repos clonÃ©s: {report['repos_clones']}\")\n",
    "print(f\"ğŸ“„ Sources scannÃ©es: {report['sources_scannees']}\")\n",
    "print(f\"ğŸ§  Embeddings gÃ©nÃ©rÃ©s: {report['embeddings_generes']}\")\n",
    "print(f\"ğŸ” Recherche: {'âœ… ACTIVE' if report['recherche_active'] else 'âŒ INACTIVE'}\")\n",
    "\n",
    "print(\"\\nâš¡ PERFORMANCE:\")\n",
    "for step, time_val in report['performance'].items():\n",
    "    print(f\"  {step.capitalize()}: {time_val}\")\n",
    "\n",
    "# Calcul temps total\n",
    "total_time = 0\n",
    "if 'clone_time' in locals(): total_time += clone_time\n",
    "if 'scan_time' in locals(): total_time += scan_time\n",
    "if 'emb_time' in locals(): total_time += emb_time\n",
    "\n",
    "print(f\"\\nğŸ TEMPS TOTAL: {total_time:.2f}s\")\n",
    "\n",
    "if report['recherche_active']:\n",
    "    print(\"\\nğŸ¯ SYSTÃˆME 100% OPÃ‰RATIONNEL\")\n",
    "    print(\"ğŸ’¡ Utilisez: semantic_search_cloud('votre requÃªte')\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ SYSTÃˆME PARTIELLEMENT OPÃ‰RATIONNEL\")\n",
    "    print(\"ğŸ’¡ VÃ©rifiez les Ã©tapes prÃ©cÃ©dentes\")\n",
    "\n",
    "print(\"\\nğŸš€ MODE CLOUD AUTONOME COMPLÃ‰TÃ‰ !\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c19084",
   "metadata": {},
   "source": [
    "# ğŸ¯ Utilisation Interactive\n",
    "\n",
    "## Recherche PersonnalisÃ©e\n",
    "```python\n",
    "# Exemples de recherches\n",
    "results = semantic_search_cloud(\"neural network training\", top_k=5)\n",
    "results = semantic_search_cloud(\"configuration files\", top_k=3)\n",
    "results = semantic_search_cloud(\"error handling patterns\", top_k=5)\n",
    "```\n",
    "\n",
    "## Exploration des Repos\n",
    "```python\n",
    "# Voir les repos disponibles\n",
    "print(\"Repos disponibles:\", available_repos)\n",
    "\n",
    "# Statistiques par repo\n",
    "repo_stats = {}\n",
    "for source in sources:\n",
    "    repo = source['repo']\n",
    "    if repo not in repo_stats:\n",
    "        repo_stats[repo] = {'python': 0, 'markdown': 0}\n",
    "    repo_stats[repo][source['type']] += 1\n",
    "\n",
    "for repo, stats in repo_stats.items():\n",
    "    print(f\"{repo}: {stats['python']} .py + {stats['markdown']} .md\")\n",
    "```\n",
    "\n",
    "**âœ… SystÃ¨me Cloud Autonome OpÃ©rationnel !**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
